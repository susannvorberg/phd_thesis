# Contact Prior {#contact-prior}

The wealth of successful meta-predictors presented in section \@ref(meta-predictors) highlights the importance to exploit other sources of information apart from coevolution statistics.
Much information about residue interactions is typically contained in single position features that can be predicted from local sequence profiles, such as secondary structure, solvent accessibility or contact number, and in pairwise features such as the contact prediction scores for residue pairs $(i,j)$ from a simple local statistical methods as presented in section \@ref(local-methods). 

For example, predictions of secondary structure elements and solvent accessibility are used by almost all modern machine learning predictors, such as MetaPsicov [@Jones2015a], NeBCon [@He2017], EPSILON-CP [@Stahl2017], PconsC3 [@Skwark2016].
Other frequently used sequence derived features include pairwise contact potentials, sequence separation and conservation measures such as column entropy [@Jones2015a; @He2017; @Ma2015a].

In the following sections I present a random forest classifier that uses sequence derived features to distinguish contacts from non-contacts.
Methods section \@ref(seq-features) lists all features used to train the classifier including the aforementioned standard features as well as some novel features. 

The probabilistic predictions of the random forest model can be introduced directly as prior information into the Bayesian statistical model presented in the last section \@ref(bayesian-approach) to improve the overall prediction accuracy in terms of posterior probabilities.
Furthermore, contact scores from coevolution methods can be added as additional feature to the random forest model in order to elucidate how much the combined information improves prediction accuracy over the single methods. 


## Random Forest Classifiers

Random Forests are supervised machine learning methods that belong to the class of ensemble methods [@Ho1998;@TinKamHo;@Breiman2001].
They are easy to implement, fast to train and can handle large numbers of features due to implicit feature selection [@Menze2009].

Ensemble methods combine the predictions of several independent base estimators with the goal to improve generalizability over a single estimator.
Random forests are ensembles of decision trees where randomness is introduced in two ways:

1. every tree is build on a random sample that is drawn with replacement from the training set and has the same size as the training set (i.e., a bootstrap sample) 
2. every split of a node is evaluated on a random subset of features

A single decision tree, especially when it is grown very deep is highly susceptible to noise in the training set and therefore prone to overfitting which results in poor generalization ability. 
As a consequence of randomness and averaging over many decision trees, the variance of a random forest predictor decreases and therefore the risk of overfitting [@Louppe2014].
It is still advisable to restrict the depth of single trees in a random forest, not only to counteract overfitting but also to reduce model complexity and to speedup the algorithm. 

Random forests are capable of regression and classification tasks.
For classification, predictions for new data are obtained by running each data sample down every tree in the forest and then either apply majority voting over single class votes or averaging the probabilistic class predictions. Probabilistic class predictions of single trees are computed as the fraction of training set samples of the same class in a leaf whereas the single class vote refers to the majority class in a leaf. 
Figure \@ref(fig:rf-intro) visualizes the procedure of classifying a new data sample.

(ref:caption-rf-intro) Classifying new data with random forests. A new data sample is run down every tree in the forest until it ends up in a leaf node. Every leaf node has associated class probabilities $p(c)$ reflecting the fraction of training samples at this leaf node belonging to every class $c$. The color of the leaf nodes reflects the class with highest probability. The predictions from all trees in form of the class probabilties are averaged and yield the final prediction. 

```{r rf-intro, echo = FALSE, out.width='80%', fig.align='center', fig.cap = '(ref:caption-rf-intro)'}
knitr::include_graphics("img/random_forest_contact_prior/intro_random_forest.png")
```


Typically, *Gini impurity*, which is a computationally efficient approximation to the entropy, is used as a split criterion to evaluate the quality of a split.
It measures the degree of purity in a data set regarding class labels as $GI = (1 - \sum_{k=1}^K p_k^2)$, where $p_k$ is the proportion of class $k$ in the data set.
For every feature $f$ in the random subset that is considered for splitting a particular node $N$, the *decrease in Gini impurity* $\Delta GI_f$  will be computed as,
 
$$
\Delta GI_f(N_{\textrm{parent}}) = GI_f(N_{\textrm{parent}}) - p_{\textrm{left}} GI_f(N_{\textrm{left}}) - p_{\textrm{right}} GI_f(N_{\textrm{left}})
$$

where $p_{\textrm{left}}$ and $p_{\textrm{right}}$ refers to the fraction of samples ending up in the left and right child node respectively [@Menze2009].
The feature $f$ with highest $\Delta GI_f$ over the two resulting child node subsets will be used to split the data set at the given node $N$.

Summing the *decrease in Gini impurity* for a feature $f$ over all trees whenever $f$ was used for a split yields the *Gini importance* measure, which can be used as an estimate of general feature relevance.
Random forests therefore are popular methods for feature selection and it is common practice to remove the least important features from a data set to reduce the complexity of the model.
However, feature importance measured with respect to *Gini importance* needs to be interpreted with care.
The random forest model cannot distinguish between correlated features and it will choose any of the correlated features for a split, thereby reducing the importance of the other features and introducing bias.
Furthermore, it has been found that feature selection based on *Gini importance* is biased towards selecting features with more categories as they will be chosen more often for splits and therefore tend to obtain higher scores [@Strobl2007].


## Hyperparameter Optimization for Random Forest

There are several hyperparameters in a random forest model that need to be tuned to achieve best balance between predictive power and runtime.
While more trees in the random forest generally improve performance of the model, they will slow down training and prediction. 
A crucial hyperparamter is the number of features that is randomly selected for a split at each node in a tree [@Bernard2009].
Stochasticity introduced by the random selection of features is a key characteristic of random forests as it reduces correlation between the trees and thus the variance of the predictor. 
Selecting many features typically increases performance as more options can be considered for each split, but at the same time increases risk of overfitting and decreases speed of the algorithm. 
In general, random forests are robust to overfitting, as long as there are enough trees in the ensemble and the selection of features for splitting a node introduces sufficient stochasticity. 
Overfitting can furthermore be prevented by restricting the depth of the trees, which is known as pruning or by enforcing a minimal leaf node size regarding the minimal number of data samples ending in a leaf node. 
Again, a positive side-effect of pruning and requiring minimal leaf node size is a speedup of the algorithm. [@Louppe2014]


In the following, I use 5-fold cross-validation to identify the optimal architecture of the random forest.
Details about the training set and he cross-validation procedure can be found  in method section \@ref(rf-training).
First I assessed performance of models for combinations of the parameter *n_estimators*, defining the number of trees in the forest and the parameter *max_depth* defining the maximum depth of the trees:

- *n_estimators* $\in \{100,500,1000\}$
- *max_depth* $\in \{10, 100, 1000, None\}$

Figure \@ref(fig:rf-gridsearch-nestimators-maxfeatures) shows that the top five parameter combinations perform nearly identical. 
Random forests with 1000 trees perform slightly better than models constituting 500 trees, irrespective of the depth of the trees.
In order to keep model complexity small, I chose `n_estimators=1000` and `max_depth=100` for further analysis.


(ref:caption-rf-gridsearch-nestimators-maxfeatures) Mean precision over 200 proteins against highest scoring contact predictions from random forest models for different settings of *n_estimators* and *max_depth*.  Dashed lines show the performance of models that have been learned on the five different subsets of training data. Solid lines give the mean precision over the five models. Only those models are shown that yielded the five highest mean precision values (given in parantheses in the legend). Random forest models with 1000 trees and maximum depth of trees of either 100, 1000 or unrestricted tree depth perform nearly identical (lines overlap). Random forest models with 500 trees and *max_depth*=10 or *max_depth*=100 perform slightly worse.

```{r rf-gridsearch-nestimators-maxfeatures, echo = FALSE, screenshot.alt="img/random_forest_contact_prior/new_gridsearch/precision_vs_rank_cv_on_test_random_forest_nestimators_maxdepth_top5_notitle.png", out.width = '100%', fig.align='center', fig.cap = '(ref:caption-rf-gridsearch-nestimators-maxfeatures)'}
knitr::include_url("img/random_forest_contact_prior/new_gridsearch/precision_vs_rank_cv_on_test_random_forest_nestimators_maxdepth_top5_notitle.html", height = "600px")
```

Next, I optimized the parameters *min_samples_leaf*, defining the minimum number of samples required at a leaf node and *max_features*, defining the number of randomly selected features considered for each split using the following settings:

- *min_samples_leaf* $\in \{1, 10, 100\}$
- *max_features* $\in \{8, 16, 38, 75 \}$ representing $\sqrt{N}$, $\log2{N}$, $0.15N$ and $0.3N$ respectively with $N=250$ being the number of features listed in method section \@ref(seq-features).

Randomly selecting 30% of features (=75 features) and requiring at least 10 samples per leaf gives highest mean precision as can be seen in Figure \@ref(fig:rf-gridsearch-maxdepth-minsampleleaf).
I chose `max_features=0.30` and `min_samples_leaf=10` for further analysis.
Tuning the hyperparameters in a different order or on a larger dataset gives similar results.


(ref:rf-gridsearch-maxdepth-minsampleleaf) Mean precision over 200 proteins against highest scoring contact predictions from random forest models with different settings of *min_samples_leaf* and *max_features*.  Dashed lines show the performance of models that have been learned on the five different subsets of training data. Solid lines give the mean precision over the five models. Only those models are shown that yielded the five best mean precision values (given in parantheses in the legend).

```{r rf-gridsearch-maxdepth-minsampleleaf, echo = FALSE, screenshot.alt="img/random_forest_contact_prior/new_gridsearch/precision_vs_rank_cv_on_test_random_forest_maxfeatures_minsampleleaf_top5_notitle.png", out.width = '100%', fig.align='center', fig.cap = '(ref:rf-gridsearch-maxdepth-minsampleleaf)'}
knitr::include_url("img/random_forest_contact_prior/new_gridsearch/precision_vs_rank_cv_on_test_random_forest_maxfeatures_minsampleleaf_top5_notitle.html", height = "600px")
```

In a next step I assessed dataset specific settings, such as the window size over which single positions features will be computed, the distance threshold to define non-contacts and the optimal proportions of contacts and non-contacts in the training set.
I used the previously identified settings of random forest hyperparameters (`n_estimators=1000, min_samples_leaf=10, max_depth=100, max_features=0.30`).

- proportion of contacts/non-contacts $\in \{1\!:\!2, 1\!:\!5, 1\!:\!10, 1\!:\!20 \}$ while keeping total dataset size fixed at 300,000 residue pairs
- window size:  $\in \{5, 7, 9, 11\}$
- non-contact threshold $\in \{8, 15, 20\}$

As can be seen in appendix \@ref(rf-window-size) and \@ref(rf-noncontact-threshold), the default choice of using a window size of five positions and the non-contact threshold of $8 \angstrom$ proves to be the optimal setting.
Furthermore, using five-times as many non-contacts as contacts in the training set results in highest mean precision as can be seen in appendix \@ref(rf-ratio-noncontacts). 
These estimates might be biased in a way since the random forest hyperparameters have been optimized on a dataset using exactly these optimal settings. 



## Evaluating Random Forest Model as Contact Predictor

I trained a random forest classifier on the feature set described in methods section \@ref(seq-features) and using the optimal hyperparameters identified with 5-fold cross-validation as described in the last section. 

Figure \@ref(fig:rf-feature-importance) shows the ranking of the ten most important features according to *Gini importance*.
Both local statistical contact scores, *OMES* [@Fodor2004a] and [MI](#abbrev) (mutual information between amino acid counts), constitute the most important features besides the mean pair potentials acording to Miyazawa & Jernigan [@Miyazawa1999a] and Li&Fang[@Li2011].
Further important features include the relative solvent accessibility at both pair positions, the total percentage of gaps at both positions, the correlation between mean isoelectric point property at both positions, sequence separation and the beta-sheet propensity in a window of size five around position i. 

(ref:caption-rf-feature-importance) Top ten features ranked according to *Gini importance*. **OMES+APC**:  [APC](#abbrev) corrected OMES score according to Fodor&Aldrich [@Fodor2004a]. **mean pair potential (Miyasawa & Jernigan)**: average quasi-chemical energy of transfer of amino acids from water to the protein environment [@Miyazawa1999a]. **MI+APC**: [APC](#abbrev) corrected mutual information between amino acid counts (using pseudo-counts). **mean pair potential (Li&Fang)**: average general contact potential by Li & Fang [@Li2011]. **rel. solvent accessibilty i(j)**: RSA score computed with Netsurfp (v1.0) [@Petersen2009a] for position i(j). **pairwise gap%**: percentage of gapped sequences at either position i and j. **correlation mean isoelectric feature**: Pearson correlation between the mean isoelectric point feature (according to Zimmermann et al., 1968) for positions i and j. **sequence separation**: |j-i|. **beta sheet propensity window(i)**: beta-sheet propensity according to Psipred [@Jones1999] computed within a window of five positions around i.  eatures are described in detail in methods section \@ref(seq-features).

```{r rf-feature-importance, echo = FALSE, screenshot.alt="img/random_forest_contact_prior/feature_random_forest_optimalhyperparameters_topfeatures.png", out.width = '100%', fig.cap = '(ref:caption-rf-feature-importance)'}
knitr::include_url("img/random_forest_contact_prior/feature_random_forest_optimalhyperparameters_topfeatures.html", height = "600px")
```

Many features have low *Gini importance* scores which means they are rarely considered for splitting a node and can most likely be removed from the dataset. 
Removing irrelevant features from the dataset is a convenient procedure to reduce model complexity.
It has been found, that prediction performance might even increase after removing the most irrelevant features [@Menze2009].
For example, during the development of *EPSILON-CP*, a deep neural network method for contact prediction, the authors performed feature selection using boosted trees.
By removing 75% of the most non-informative features (mostly features related to amino acid composition), the performance of their predictor increased slightly [@Stahl2017]. 
Other studies have also emphasized the importance of feature selection to improve performance and reduce model complexity [@Cheng2007; @Li2011].

As described in methods section \@ref(rf-feature-selection), I performed feature selection by evaluating model performance on subsets of features of decreasing importance.
Most models trained on subsets of the total feature space perform nearly identical compared to the model trained on all features, as can be seen in Figure \@ref(fig:rf-feature-selection-performance).
Performance of the random forest models drops noticeably when using only the 25 most important features.
For the further analysis I am using the random forest model trained on the 75 most important features as this model constitutes the smallest set of features while performing nearly identical compared to the model trained on the complete feature set. 

(ref:caption-rf-feature-selection-performance) Mean precision of top ranked predictions over 200 proteins for random forest models trained on subsets of features of decreasing importance. Subsets of features have been selected as described in methods section \@ref(rf-feature-selection).  

```{r rf-feature-selection-performance, echo = FALSE, screenshot.alt="img/random_forest_contact_prior/precision_vs_rank_featureselection_random_forest_optimized_hyperparameters.png", out.width = '100%', fig.cap = '(ref:caption-rf-feature-selection-performance)'}
knitr::include_url("img/random_forest_contact_prior/precision_vs_rank_featureselection_random_forest_optimized_hyperparameters.html", height = "500px")
```


Figure \@ref(fig:performance-rf) shows the mean precision for the random forest model trained on the 75 most important features. 
The random forest model has a mean precision of 0.33 for the top $0.5\cdot L$ contacts compared to a precision of 0.47 for pseudo-likelihood.
Furthermore, the random forest model improves approximately ten percentage points in precision over the local statistical contact scores, *OMES* and mutual information (MI).
Both methods comprise important features of the random forest model as can be seen in Figure \@ref(fig:rf-feature-importance).

When analysing performance with respect to alignment size it can be found that the random forest model outperforms the pseudo-likelihood score for small alignments (see Figure \@ref(fig:performance-neff-rf)).  
Both, local statistial models *OMES* and [MI](#abbrev) also perform weak on small alignments, leading to the conclusion that the remaining sequence derived features are highly relevant when the alignment contains only few sequences.
This finding is expected, as it is well known that models trained on simple sequence features perform almost independent of alignment size [@Stahl2017, @Skwark2016]. 


(ref:caption-performance-rf) Mean precision for top ranked contacts on a test set of 774 proteins. **random forest (pLL)** = random forest model using sequence derived features and pseudo-likelihood contact score ([APC](#abbrev) corrected Frobenius norm of couplings). **pseudo-likelihood** = [APC](#abbrev) corrected Frobenius norm of couplings computed with pseudo-likelihood. **random forest** = random forest model trained on 75 sequence derived features.  **OMES** = [APC](#abbrev) corrected *OMES* contact score according to Fodor&Aldrich [@Fodor2004a]. **mutual information** = [APC](#abbrev) corrected mutual information between amino acid counts (using pseudo-counts).   

```{r performance-rf, echo = FALSE, screenshot.alt="img/random_forest_contact_prior/precision_vs_rank_notitle.png", out.width = '100%', fig.cap = '(ref:caption-performance-rf)'}
knitr::include_url("img/random_forest_contact_prior/precision_vs_rank_notitle.html", height = "500px")
```


(ref:caption-performance-neff-rf) Mean precision for top ranked contacts on a test set of 774 proteins splitted into four equally sized subsets with respect to [Neff](#abbrev). Subsets are defined according to quantiles of [Neff](#abbrev) values. Upper left: Subset of proteins with [Neff](#abbrev) < Q1. Upper right: Subset of proteins with Q1 <= [Neff](#abbrev) < Q2. Lower left: Subset of proteins with Q2 <= [Neff](#abbrev) < Q3. Lower right: Subset of proteins with Q3 <= [Neff](#abbrev) < Q4. **random forest (pLL)** = random forest model using sequence derived features and pseudo-likelihood contact score ([APC](#abbrev) corrected Frobenius norm of couplings). **pseudo-likelihood** = [APC](#abbrev) corrected Frobenius norm of couplings computed with pseudo-likelihood. **random forest** = random forest model trained on 75 sequence derived features.  **OMES** = [APC](#abbrev) corrected *OMES* contact score according to Fodor&Aldrich [@Fodor2004a]. **mutual information** = [APC](#abbrev) corrected mutual information between amino acid counts (using pseudo-counts).   

```{r performance-neff-rf, echo = FALSE, screenshot.alt="img/random_forest_contact_prior/precision_vs_rank_facetted_by_neff_notitle.png", out.width = '100%', fig.cap = '(ref:caption-performance-neff-rf)'}
knitr::include_url("img/random_forest_contact_prior/precision_vs_rank_facetted_by_neff_notitle.html", height = "600px")
```



Figure \@ref(fig:performance-neff-rf) showed that the random forest predictor improves over the pseudo-likelihood coevolution method when the alignment consists of only few sequences. 
In order to assess this improvement in a more direct manner, it is possible to build a combined random forest predictor that is not only trained on the sequence derived features but also on the pseudo-likelihood contact score as an additional feature. 
As expected, the pseudo-likelihood score comprises the most important feature in the model (see Appendix Figure \@ref(fig:feature-importance-rf-with-pll-score)) followed by the same sequence features that were found in the previous analysis in Figure \@ref(fig:rf-feature-importance).
Models trained on subsets of features as described in method section \@ref(rf-feature-selection) perform equally well as the model trained on the complete set of features (see Appendix Figure \@ref(fig:feature-selection-rf-with-pll-score)).
Only the model trained on the 26 most important features has slighlty decreased precision for the top L/10 ranked contacts. 
The model trained on 76 features was selected as the final model.

Finally, comparing the random forest model trained on sequence features and pseudo-likelihood contact score to the pseudo-likelihood score in Figure \@ref(fig:performance-rf) reveals that combining both types of information indeed improves predictive power over both single approaches.
Especially for small alignments, the improvement is substantial as can be seen in in the left upper plot in Figure \@ref(fig:performance-neff-rf).
In contrast, the improvement on large alignments (right lower plot in Figure \@ref(fig:performance-neff-rf)) is small, as the gain from simple sequence features compared to the much more powerful coevolution signals is neglectable. 

















