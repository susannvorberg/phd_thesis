<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>PhD thesis: residue-residue contact prediction</title>
  <meta name="description" content="This is my PhD thesis on residue-residue contact prediction.">
  <meta name="generator" content="bookdown 0.3.9 and GitBook 2.6.7">

  <meta property="og:title" content="PhD thesis: residue-residue contact prediction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my PhD thesis on residue-residue contact prediction." />
  <meta name="github-repo" content="susannvorberg/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PhD thesis: residue-residue contact prediction" />
  
  <meta name="twitter:description" content="This is my PhD thesis on residue-residue contact prediction." />
  

<meta name="author" content="Susann Vorberg">


<meta name="date" content="2017-09-27">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="methods-optimizing-full-likelihood.html">
<link rel="next" href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
  tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
  TeX: { 
    extensions: ["mediawiki-texvc.js", "sinuitx.js"],
    Macros: {
      Cb: "C_\\beta",
      eq: "\\!=\\!",
      Gauss: "\\mathcal{N}",
      H: "\\mathbf{H}",
      Hij : "\\H_{ij}",
      I: "\\mathbf{I}",
      Lijk: "\\mathbf{\\Lambda}_{ij,k}",
      Lk: "\\mathbf{\\Lambda}_k",
      LLreg: "L\\!L_\\mathrm{reg}",
      muijk: "\\mathbf{\\mu}_{ij,k}",
      muk: "\\mathbf{\\mu}_k",
      neff: "N_\\mathrm{eff}",
      r: "\\mathbf{r}",
      rij: "r_{ij}",
      seq: "\\mathbf{x}",
      Qij: "\\mathbf{Q}_{ij}",
      q: "\\mathbf{q}",
      qij: "\\mathbf{q'}_{ij}",
      Sn: "\\mathcal{S}_n",
      v: "\\mathbf{v}",
      vi: "\\mathcal{v}_{i}",
      via: "\\mathcal{v}_{ia}",
      w: "\\mathbf{w}",
      wij: "\\mathbf{w}_{ij}",
      wijab: "\\mathcal{w}_{ijab}",
      wijcd: "\\mathcal{w}_{ijcd}",
      wklcd: "\\mathcal{w}_{klcd}",
      X: "\\mathbf{X}",
      angstrom: "\\AA \\;"
      }
  }
});
</script>


 
<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js", "[siunitx]/siunitx.js"],
  TeX: { TagSide: "left" }
});
MathJax.Ajax.config.path['siunitx']  = '../latex/MathJax-siunitx-master/';
</script>
//-->



<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD Thesis Susann Vorberg</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="general-intro.html"><a href="general-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="protein-structure.html"><a href="protein-structure.html"><i class="fa fa-check"></i><b>1.1</b> Protein Structure</a><ul>
<li class="chapter" data-level="1.1.1" data-path="protein-structure.html"><a href="protein-structure.html#amino-acid-interactions"><i class="fa fa-check"></i><b>1.1.1</b> Amino Acid Interactions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="structure-prediction.html"><a href="structure-prediction.html"><i class="fa fa-check"></i><b>1.2</b> Structure Prediction</a><ul>
<li class="chapter" data-level="1.2.1" data-path="structure-prediction.html"><a href="structure-prediction.html#template-based-methods"><i class="fa fa-check"></i><b>1.2.1</b> Template-based methods</a></li>
<li class="chapter" data-level="1.2.2" data-path="structure-prediction.html"><a href="structure-prediction.html#template-free-structure-prediction"><i class="fa fa-check"></i><b>1.2.2</b> Template-free structure prediction</a></li>
<li class="chapter" data-level="1.2.3" data-path="structure-prediction.html"><a href="structure-prediction.html#contact-assisted-str-pred"><i class="fa fa-check"></i><b>1.2.3</b> contact assisted de-novo predictions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="contact-prediction.html"><a href="contact-prediction.html"><i class="fa fa-check"></i><b>1.3</b> Contact Prediction</a><ul>
<li class="chapter" data-level="1.3.1" data-path="contact-prediction.html"><a href="contact-prediction.html#local-methods"><i class="fa fa-check"></i><b>1.3.1</b> Local Statistical Models</a></li>
<li class="chapter" data-level="1.3.2" data-path="contact-prediction.html"><a href="contact-prediction.html#global-methods"><i class="fa fa-check"></i><b>1.3.2</b> Global Statistical Models</a></li>
<li class="chapter" data-level="1.3.3" data-path="contact-prediction.html"><a href="contact-prediction.html#meta-predictors"><i class="fa fa-check"></i><b>1.3.3</b> Machine Learning Methods and Meta-Predictors</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="maxent.html"><a href="maxent.html"><i class="fa fa-check"></i><b>1.4</b> Modelling Protein Families with Potts Model</a><ul>
<li class="chapter" data-level="1.4.1" data-path="maxent.html"><a href="maxent.html#potts-model-properties"><i class="fa fa-check"></i><b>1.4.1</b> Model Properties</a></li>
<li class="chapter" data-level="1.4.2" data-path="maxent.html"><a href="maxent.html#potts-mle"><i class="fa fa-check"></i><b>1.4.2</b> Inferring Parameters for the Potts Model</a></li>
<li class="chapter" data-level="1.4.3" data-path="maxent.html"><a href="maxent.html#potts-model-solutions"><i class="fa fa-check"></i><b>1.4.3</b> Solving the Inverse Potts Problem</a></li>
<li class="chapter" data-level="1.4.4" data-path="maxent.html"><a href="maxent.html#post-processing-heuristics"><i class="fa fa-check"></i><b>1.4.4</b> Computing Contact Maps</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html"><i class="fa fa-check"></i><b>1.5</b> Evaluating Contact Prediction Methods</a><ul>
<li class="chapter" data-level="1.5.1" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#seq-sep"><i class="fa fa-check"></i><b>1.5.1</b> Sequence Separation</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>1.6</b> Challenges in Coevolutionary Inference</a><ul>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#phylogenetic-bias"><i class="fa fa-check"></i>Phylogenetic Bias</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#entropic-bias"><i class="fa fa-check"></i>Entropic bias</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#finite-sampling-effects"><i class="fa fa-check"></i>Finite Sampling Effects</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#transitive-effects"><i class="fa fa-check"></i>Transitive Effects</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#multiple-sequence-alignments"><i class="fa fa-check"></i>Multiple Sequence Alignments</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#evaluation-strategy"><i class="fa fa-check"></i>Evaluation Strategy</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#alternative-sources-of-coevolution"><i class="fa fa-check"></i>Alternative Sources of Coevolution</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="developing-a-bayesian-model-for-contact-prediction.html"><a href="developing-a-bayesian-model-for-contact-prediction.html"><i class="fa fa-check"></i><b>1.7</b> Developing a Bayesian Model for Contact Prediction</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpreting-coupling-matrices.html"><a href="interpreting-coupling-matrices.html"><i class="fa fa-check"></i><b>2</b> Interpretation of Coupling Matrices</a><ul>
<li class="chapter" data-level="2.1" data-path="correlation-between-couplings-and-class.html"><a href="correlation-between-couplings-and-class.html"><i class="fa fa-check"></i><b>2.1</b> Single Coupling Values Carry Evidence of Contacts</a></li>
<li class="chapter" data-level="2.2" data-path="physico-chemical-fingerprints-in-coupling-matrices.html"><a href="physico-chemical-fingerprints-in-coupling-matrices.html"><i class="fa fa-check"></i><b>2.2</b> Physico-Chemical Fingerprints in Coupling Matrices</a></li>
<li class="chapter" data-level="2.3" data-path="coupling-profiles-vary-with-distance.html"><a href="coupling-profiles-vary-with-distance.html"><i class="fa fa-check"></i><b>2.3</b> Coupling Profiles Vary with Distance</a></li>
<li class="chapter" data-level="2.4" data-path="higher-order-dependencies-between-couplings.html"><a href="higher-order-dependencies-between-couplings.html"><i class="fa fa-check"></i><b>2.4</b> Higher Order Dependencies Between Couplings</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="optimizing-full-likelihood.html"><a href="optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>3</b> Optimizing the Full-Likelihood</a><ul>
<li class="chapter" data-level="3.1" data-path="the-likelihood-of-the-sequences-as-a-potts-model.html"><a href="the-likelihood-of-the-sequences-as-a-potts-model.html"><i class="fa fa-check"></i><b>3.1</b> The Likelihood of the Sequences as a Potts Model</a></li>
<li class="chapter" data-level="3.2" data-path="gap-treatment.html"><a href="gap-treatment.html"><i class="fa fa-check"></i><b>3.2</b> Treating Gaps as Missing Information</a></li>
<li class="chapter" data-level="3.3" data-path="the-regularized-log-likelihood-and-its-gradient.html"><a href="the-regularized-log-likelihood-and-its-gradient.html"><i class="fa fa-check"></i><b>3.3</b> The Regularized Log Likelihood and its Gradient</a></li>
<li class="chapter" data-level="3.4" data-path="prior-v.html"><a href="prior-v.html"><i class="fa fa-check"></i><b>3.4</b> The prior on <span class="math inline">\(\v\)</span></a></li>
<li class="chapter" data-level="3.5" data-path="full-likelihood-gradient.html"><a href="full-likelihood-gradient.html"><i class="fa fa-check"></i><b>3.5</b> Optimizing the Full Likelihood with Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><a href="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><i class="fa fa-check"></i><b>4</b> A Bayesian Statistical Model for Residue-Residue Contact Prediction</a><ul>
<li class="chapter" data-level="4.1" data-path="overview-posterior-distances.html"><a href="overview-posterior-distances.html"><i class="fa fa-check"></i><b>4.1</b> Computing the Posterior Distribution of Distances <span class="math inline">\(p(\r | \X)\)</span></a></li>
<li class="chapter" data-level="4.2" data-path="coupling-prior.html"><a href="coupling-prior.html"><i class="fa fa-check"></i><b>4.2</b> Modelling the prior over couplings with dependence on <span class="math inline">\(\rij\)</span></a></li>
<li class="chapter" data-level="4.3" data-path="laplace-approx.html"><a href="laplace-approx.html"><i class="fa fa-check"></i><b>4.3</b> Gaussian approximation to the posterior of couplings</a><ul>
<li class="chapter" data-level="4.3.1" data-path="laplace-approx.html"><a href="laplace-approx.html#laplace-approx-improvement"><i class="fa fa-check"></i><b>4.3.1</b> Iterative improvement of Laplace approximation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="likelihood-fct-distances.html"><a href="likelihood-fct-distances.html"><i class="fa fa-check"></i><b>4.4</b> Computing the likelihood function of distances <span class="math inline">\(p(\X | \r)\)</span></a></li>
<li class="chapter" data-level="4.5" data-path="posterior-of-rij.html"><a href="posterior-of-rij.html"><i class="fa fa-check"></i><b>4.5</b> The posterior probability distribution for <span class="math inline">\(\rij\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="contact-prior.html"><a href="contact-prior.html"><i class="fa fa-check"></i><b>5</b> Contact Prior</a><ul>
<li class="chapter" data-level="5.1" data-path="random-forest-classifiers.html"><a href="random-forest-classifiers.html"><i class="fa fa-check"></i><b>5.1</b> Random Forest Classifiers</a></li>
<li class="chapter" data-level="5.2" data-path="evaluating-random-forest-model-as-contact-predictor.html"><a href="evaluating-random-forest-model-as-contact-predictor.html"><i class="fa fa-check"></i><b>5.2</b> Evaluating Random Forest Model as Contact Predictor</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>6</b> Methods</a><ul>
<li class="chapter" data-level="6.1" data-path="dataset.html"><a href="dataset.html"><i class="fa fa-check"></i><b>6.1</b> Dataset</a></li>
<li class="chapter" data-level="6.2" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html"><i class="fa fa-check"></i><b>6.2</b> Optimizing Pseudo-Likelihood</a><ul>
<li class="chapter" data-level="6.2.1" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#pseudo-likelihood-objective-function-and-its-gradients"><i class="fa fa-check"></i><b>6.2.1</b> Pseudo-Likelihood Objective Function and its Gradients</a></li>
<li class="chapter" data-level="6.2.2" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#diff-ccmpred-ccmpredpy"><i class="fa fa-check"></i><b>6.2.2</b> Differences between CCMpred and CCMpredpy</a></li>
<li class="chapter" data-level="6.2.3" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#seq-reweighting"><i class="fa fa-check"></i><b>6.2.3</b> Sequence Reweighting</a></li>
<li class="chapter" data-level="6.2.4" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#amino-acid-frequencies"><i class="fa fa-check"></i><b>6.2.4</b> Computing Amino Acid Frequencies</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html"><i class="fa fa-check"></i><b>6.3</b> Analysis of Coupling Matrices</a><ul>
<li class="chapter" data-level="6.3.1" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-correlation"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Couplings with Contact Class</a></li>
<li class="chapter" data-level="6.3.2" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-profile"><i class="fa fa-check"></i><b>6.3.2</b> Coupling Distribution Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>6.4</b> Optimizing the Full-Likelihood</a><ul>
<li class="chapter" data-level="6.4.1" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html#hyperparameter-optimization-for-stochastic-gradient-descent"><i class="fa fa-check"></i><b>6.4.1</b> Hyperparameter Optimization for Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="6.4.2" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html#optimizing-regularization-coefficients-for-contrastive-divergence"><i class="fa fa-check"></i><b>6.4.2</b> Optimizing Regularization Coefficients for Contrastive Divergence</a></li>
<li class="chapter" data-level="6.4.3" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html#optimizing-the-sampling-scheme-for-contrastive-divergence"><i class="fa fa-check"></i><b>6.4.3</b> Optimizing the Sampling Scheme for Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html"><i class="fa fa-check"></i><b>6.5</b> Bayesian Model for Residue-Resdiue Contact Prediction</a><ul>
<li class="chapter" data-level="6.5.1" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#neg-Hessian-computation"><i class="fa fa-check"></i><b>6.5.1</b> Efficiently Computing the negative Hessian of the regularized log-likelihood</a></li>
<li class="chapter" data-level="6.5.2" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#inv-lambda-ij-k"><i class="fa fa-check"></i><b>6.5.2</b> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></a></li>
<li class="chapter" data-level="6.5.3" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#training-hyperparameters"><i class="fa fa-check"></i><b>6.5.3</b> Training the Hyperparameters <span class="math inline">\(\muk\)</span>, <span class="math inline">\(\Lk\)</span> and <span class="math inline">\(\gamma_k\)</span></a></li>
<li class="chapter" data-level="6.5.4" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#the-gradient-of-the-log-likelihood-with-respect-to-mathbfmu"><i class="fa fa-check"></i><b>6.5.4</b> The gradient of the log likelihood with respect to <span class="math inline">\(\mathbf{\mu}\)</span></a></li>
<li class="chapter" data-level="6.5.5" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#the-gradient-of-the-log-likelihood-with-respect-to-lk"><i class="fa fa-check"></i><b>6.5.5</b> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></a></li>
<li class="chapter" data-level="6.5.6" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#the-gradient-of-the-log-likelihood-with-respect-to-gamma_k"><i class="fa fa-check"></i><b>6.5.6</b> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><a href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><i class="fa fa-check"></i><b>6.6</b> Bayesian Statistical Model for Prediction of Protein Residue-Residue Distances</a><ul>
<li class="chapter" data-level="6.6.1" data-path="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><a href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html#modelling-the-dependence-of-wij-on-distance"><i class="fa fa-check"></i><b>6.6.1</b> Modelling the dependence of <span class="math inline">\(\wij\)</span> on distance</a></li>
<li class="chapter" data-level="6.6.2" data-path="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><a href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html#training-the-hyperparameters-rho_k-and-alpha_k-for-distance-dependent-prior"><i class="fa fa-check"></i><b>6.6.2</b> Training the Hyperparameters <span class="math inline">\(\rho_k\)</span> and <span class="math inline">\(\alpha_k\)</span> for distance-dependent prior</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html"><i class="fa fa-check"></i><b>6.7</b> Training Random Forest Contat Prior</a><ul>
<li class="chapter" data-level="6.7.1" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#seq-features"><i class="fa fa-check"></i><b>6.7.1</b> Sequence Derived Features</a></li>
<li class="chapter" data-level="6.7.2" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#rf-hyperparameter-optimization"><i class="fa fa-check"></i><b>6.7.2</b> Hyperparameter Optimization for Random Forest Prior</a></li>
<li class="chapter" data-level="6.7.3" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#rf-feature-selection"><i class="fa fa-check"></i><b>6.7.3</b> Feature Selection</a></li>
<li class="chapter" data-level="6.7.4" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#rf-with-pll-score"><i class="fa fa-check"></i><b>6.7.4</b> Using Pseudo-likelihood Coevolution Score as Additional Feature</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="abbrev.html"><a href="abbrev.html"><i class="fa fa-check"></i><b>A</b> Abbreviations</a><ul>
<li class="chapter" data-level="A.1" data-path="amino-acids.html"><a href="amino-acids.html"><i class="fa fa-check"></i><b>A.1</b> Amino Acid Alphabet</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="dataset-properties.html"><a href="dataset-properties.html"><i class="fa fa-check"></i><b>B</b> Dataset Properties</a><ul>
<li class="chapter" data-level="B.1" data-path="alignment-diversity.html"><a href="alignment-diversity.html"><i class="fa fa-check"></i><b>B.1</b> Alignment Diversity</a></li>
<li class="chapter" data-level="B.2" data-path="proportion-of-gaps-in-alignment.html"><a href="proportion-of-gaps-in-alignment.html"><i class="fa fa-check"></i><b>B.2</b> Proportion of Gaps in Alignment</a></li>
<li class="chapter" data-level="B.3" data-path="alignment-size-number-of-sequences.html"><a href="alignment-size-number-of-sequences.html"><i class="fa fa-check"></i><b>B.3</b> Alignment Size (number of sequences)</a></li>
<li class="chapter" data-level="B.4" data-path="protein-length.html"><a href="protein-length.html"><i class="fa fa-check"></i><b>B.4</b> Protein Length</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><a href="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><i class="fa fa-check"></i><b>C</b> Amino Acid Interaction Preferences Reflected in Coupling Matrices</a><ul>
<li class="chapter" data-level="C.1" data-path="pi-cation.html"><a href="pi-cation.html"><i class="fa fa-check"></i><b>C.1</b> Pi-Cation interactions</a></li>
<li class="chapter" data-level="C.2" data-path="disulfide.html"><a href="disulfide.html"><i class="fa fa-check"></i><b>C.2</b> Disulfide Bonds</a></li>
<li class="chapter" data-level="C.3" data-path="aromatic-proline.html"><a href="aromatic-proline.html"><i class="fa fa-check"></i><b>C.3</b> Aromatic-Proline Interactions</a></li>
<li class="chapter" data-level="C.4" data-path="aromatic-network.html"><a href="aromatic-network.html"><i class="fa fa-check"></i><b>C.4</b> Network-like structure of aromatic residues</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="optimizing-full-likelihood-with-gradient-descent.html"><a href="optimizing-full-likelihood-with-gradient-descent.html"><i class="fa fa-check"></i><b>D</b> Optimizing Full Likelihood with Gradient Descent</a><ul>
<li class="chapter" data-level="D.1" data-path="cd-large-alpha-large-neff.html"><a href="cd-large-alpha-large-neff.html"><i class="fa fa-check"></i><b>D.1</b> Divergence of objective function for big learning rates and Neff values</a></li>
<li class="chapter" data-level="D.2" data-path="number-of-iterations-for-different-learning-rates.html"><a href="number-of-iterations-for-different-learning-rates.html"><i class="fa fa-check"></i><b>D.2</b> Number of iterations for different learning rates</a></li>
<li class="chapter" data-level="D.3" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html"><i class="fa fa-check"></i><b>D.3</b> Number of iterations for different learning rate schedules and fixed initial learning rate <span class="math inline">\(\alpha_0 =\)</span> 1e-4</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="training-of-the-random-forest-contact-prior.html"><a href="training-of-the-random-forest-contact-prior.html"><i class="fa fa-check"></i><b>E</b> Training of the Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="E.1" data-path="rf-window-size.html"><a href="rf-window-size.html"><i class="fa fa-check"></i><b>E.1</b> Evaluating window size with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="E.2" data-path="rf-noncontact-threshold.html"><a href="rf-noncontact-threshold.html"><i class="fa fa-check"></i><b>E.2</b> Evaluating non-contact threshold with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="E.3" data-path="rf-ratio-noncontacts.html"><a href="rf-ratio-noncontacts.html"><i class="fa fa-check"></i><b>E.3</b> Evaluating ratio of non-contacts and contacts in the training set with 5-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PhD thesis: residue-residue contact prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-model-for-residue-resdiue-contact-prediction" class="section level2">
<h2><span class="header-section-number">6.5</span> Bayesian Model for Residue-Resdiue Contact Prediction</h2>
<div id="neg-Hessian-computation" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Efficiently Computing the negative Hessian of the regularized log-likelihood</h3>
<p>Surprisingly, the elements of the Hessian at the mode <span class="math inline">\(\w^*\)</span> are easy to compute. Let <span class="math inline">\(i,j,k,l \in \{1,\ldots,L\}\)</span> be columns in the <a href="abbrev.html#abbrev">MSA</a> and let <span class="math inline">\(a, b, c, d \in \{1,\ldots,20\}\)</span> represent amino acids.</p>
<p>The partial derivative <span class="math inline">\(\partial / \partial \w_{klcd}\)</span> of the second term in the gradient of the couplings in eq. <a href="the-regularized-log-likelihood-and-its-gradient.html#eq:gradient-LLreg-pair">(3.4)</a> is</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab } 
    &amp;=&amp;  - \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} \frac{\partial \left( \frac{\exp \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}^L w_{ij}(y_i,y_j) \right) }{Z_n(\v,\w)} \right)}{\partial \wklcd}   I(y_i \eq a, y_j \eq b) \\
    &amp;&amp;- \lambda_w \delta_{ijab,klcd} \,,
\end{eqnarray}\]</span>
<p>where <span class="math inline">\(\delta_{ijab,klcd} = I(ijab=klcd)\)</span> is the Kronecker delta. Applying the product rule, we find</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab  } 
    &amp;=&amp;  - \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} \frac{\exp \left(\sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}^L w_{ij}(y_i,y_j)  \right)}{Z_n(\v,\w)}  I(y_i \eq a, y_j \eq b) \\
    &amp; \times &amp; \left[ \frac{\partial}{\partial \wklcd} \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}  w_{ij}(y_i,y_j)  \right) 
                  - \frac{1}{Z_n(\v,\w)} \frac{\partial  Z_n(\v,\w) }{\partial\wklcd} \right] \\
    &amp;-&amp; \lambda_w \delta_{ijab,klcd} \\
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab  } 
    &amp;=&amp;  - \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} \frac{\exp \left(\sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}^L w_{ij}(y_i,y_j)  \right)}{Z_n(\v,\w)}  I(y_i \eq a, y_j \eq b) \\
    &amp; \times &amp; \left[ I(y_k \eq c, y_l \eq d) - \frac{\partial}{\partial \wklcd} \log Z_n(\v,\w) \right] \\
    &amp;-&amp; \lambda_w \delta_{ijab,klcd} \,.
\end{eqnarray}\]</span>
<p>We simplify this expression using</p>
<span class="math display">\[\begin{equation}
    p(\mathbf{y} | \v,\w) = \frac{\exp \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L} w_{ij}(y_i,y_j) \right)}{Z_n(\v,\w)}  ,
\end{equation}\]</span>
<p>yielding</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab} 
    &amp;=&amp;  -  \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} p(\mathbf{y} | \v,\w) \, I(y_i \eq a, y_j \eq b, y_k \eq c, y_l \eq d)  \\
    &amp;+&amp; \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \mathcal{S}_n} p(\mathbf{y} | \v,\w) \, I(y_i \eq a, y_j \eq b ) \sum_{\mathbf{y} \in \Sn} p(\mathbf{y} | \v,\w)  I(y_k \eq c, y_l \eq d ) \\
    &amp;-&amp; \lambda_w \delta_{ijab,klcd} \,.
\end{eqnarray}\]</span>
<p>If <span class="math inline">\(\X\)</span> does not contain too many gaps, this expression can be approximated by</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab  } 
    &amp;=&amp; - N_{ijkl} \: p(x_i \eq a, x_j \eq b, x_k \eq c, x_l \eq d | \v,\w)  \nonumber \\
    &amp;&amp; +  N_{ijkl} \: p(x_i \eq a, x_j \eq b | \v,\w) \, p(x_k \eq c, x_l \eq d | \v,\w) - \lambda_w \delta_{ijab,klcd} \,,
\end{eqnarray}\]</span>
<p>where <span class="math inline">\(N_{ijkl}\)</span> is the number of sequences that have a residue in <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span>, <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span>.</p>
<p>Looking at three cases separately:</p>
<ul>
<li>case 1: <span class="math inline">\((k,l) = (i,j)\)</span> and <span class="math inline">\((c,d) = (a,b)\)</span></li>
<li>case 2: <span class="math inline">\((k,l) = (i,j)\)</span> and <span class="math inline">\((c,d) \ne (a,b)\)</span></li>
<li>case 3: <span class="math inline">\((k,l) \ne (i,j)\)</span> and <span class="math inline">\((c,d) \ne (a,b)\)</span>,</li>
</ul>
<p>the elements of <span class="math inline">\(\H\)</span>, which are the negative second partial derivatives of <span class="math inline">\(\LLreg(\v^*,\w)\)</span> with respect to the components of <span class="math inline">\(\w\)</span>, are</p>
<span class="math display" id="eq:Hw-offdiag">\[\begin{eqnarray}
    \mathrm{case~1:} (\H)_{ijab, ijab}  
    &amp;=&amp;  N_{ij} \, p(x_i \eq a, x_j \eq b| \v^*,\w^*) \, ( 1 - p(x_i \eq a, x_j \eq b| \v^*,\w^*) \,) \\
    &amp;&amp;   + \lambda_w \\
    \mathrm{case~2:} (\H)_{ijcd, ijab}  
    &amp;=&amp;  - N_{ij} \, p(x_i \eq a, x_j \eq b |\v^*,\w^*) \, p(x_i \eq c, x_j \eq d |\v^*,\w^*) \\
    \mathrm{case~3:} (\H)_{klcd, ijab}  
    &amp;=&amp;   N_{ijkl} \, p(x_i \eq a, x_j \eq b, x_k \eq c, x_l \eq d  | \v^*,\w^*) \nonumber \\
    &amp;&amp;    - N_{ijkl} \, p(x_i \eq a, x_j \eq b | \v^*,\w^*)\, p(x_k \eq c, x_l \eq d | \v^*,\w^*) \,.
\tag{6.5}
\end{eqnarray}\]</span>
<p>We know from eq. <a href="full-likelihood-gradient.html#eq:gradient-LLreg-approx">(3.10)</a> that at the mode <span class="math inline">\(\w^*\)</span> the model probabilities match the empirical frequencies up to a small regularization term,</p>
<span class="math display">\[\begin{equation}
    p(x_i \eq a, x_j \eq b | \v^*,\w^*) = q(x_i \eq a, x_j \eq b) - \frac{\lambda_w}{N_{ij}}  \wijab^* \,,
\end{equation}\]</span>
<p>and therefore the negative Hessian elements in cases 1 and 2 can be expressed as</p>
<!-- The first term ($N_{ij} \left(\,q(x_i\!=\!a, x_j\!=\!b)  - \frac{\lambda_w}{N_{ij}} \wijab^* \right)$) actually is accurate and only the second $(\dots)$ iss approximated according to the gap approximation in eq. \@ref(eq:gradient-LLreg-approx) -->
<span class="math display" id="eq:Hw-diag">\[\begin{align}
   (\H)_{ijab, ijab} =&amp; N_{ij} \left( q(x_i \eq a, x_j \eq b)  - \frac{\lambda_w}{N_{ij}} \wijab^* \right) \left( 1 - q(x_i \eq a, x_j \eq b) +\frac{\lambda_w}{N_{ij}} \wijab^* \right) \\
   &amp; + \lambda_w \\
   (\H)_{ijcd, ijab} =&amp; -N_{ij} \left(\,q(x_i \eq a, x_j \eq b)  - \frac{\lambda_w}{N_{ij}} \wijab^* \right) \left( q(x_i \eq c, x_j \eq d) -\frac{\lambda_w}{N_{ij}} \wijcd^* \right) .
\tag{6.6}
\end{align}\]</span>
<p>In order to write the previous eq. <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:Hw-diag">(6.6)</a> in matrix form, the <em>regularised</em> empirical frequencies <span class="math inline">\(\qij\)</span> will be defined as</p>
<span class="math display">\[\begin{equation}
    (\qij)_{ab} = q&#39;_{ijab} := q(x_i \eq a, x_j \eq b) - \lambda_w  \wijab^* / N_{ij} \,,
\end{equation}\]</span>
<p>and the <span class="math inline">\(400 \times 400\)</span> diagonal matrix <span class="math inline">\(\Qij\)</span> will be defined as</p>
<span class="math display">\[\begin{equation}
    \Qij := \text{diag}(\qij) \; .
\end{equation}\]</span>
<p>Now eq. <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:Hw-diag">(6.6)</a> can be written in matrix form</p>
<span class="math display" id="eq:mat-Hij">\[\begin{equation}
     \H_{ij} = N_{ij} \left( \Qij -  \qij \qij^{\mathrm{T}} \right)  + \lambda_w \I \; .
\tag{6.7}
\end{equation}\]</span>
</div>
<div id="inv-lambda-ij-k" class="section level3">
<h3><span class="header-section-number">6.5.2</span> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></h3>
<p>It is possible to efficiently invert the matrix <span class="math inline">\(\Lijk = \H_{ij} - \lambda_w \I + \Lambda_k\)</span>, that is introduced in <a href="coupling-prior.html#coupling-prior">4.2</a> where <span class="math inline">\(\H_{ij}\)</span> is the <span class="math inline">\(400 \times 400\)</span> diagonal block submatrix <span class="math inline">\((\H_{ij})_{ab,cd} := (\H)_{ijab,ijcd}\)</span> and <span class="math inline">\(\Lambda_k\)</span> is an invertible diagonal precision matrix that is introduced in section <a href="#modeling-dep-of-wij"><strong>??</strong></a>.</p>
<p>Equation <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:mat-Hij">(6.7)</a> can be used to write <span class="math inline">\(\Lijk\)</span> in matrix form as</p>
<span class="math display" id="eq:mat-Lijk">\[\begin{equation}
     \Lijk = \H_{ij} - \lambda_w \I + \Lk = N_{ij} \Qij- N_{ij} \qij \qij^{\mathrm{T}} + \Lk \,.
\tag{6.8}
\end{equation}\]</span>
<p>Owing to eqs. <a href="gap-treatment.html#eq:normalized-emp-freq">(3.2)</a> and <a href="prior-v.html#eq:zero-sum-wij">(3.6)</a>, <span class="math inline">\(\sum_{a,b=1}^{20} q&#39;_{ijab} = 1\)</span>. The previous equation <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:mat-Lijk">(6.8)</a> facilitates the calculation of the inverse of this matrix using the <em>Woodbury identity</em> for matrices</p>
<span class="math display">\[\begin{equation}
    (\mathbf{A} + \mathbf{B} \mathbf{D}^{-1} \mathbf{C})^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{B} (\mathbf{D} + \mathbf{C} \mathbf{A}^{-1} \mathbf{B}) ^{-1} \mathbf{C} \mathbf{A}^{-1} \;. 
\end{equation}\]</span>
<p>by setting</p>
<span class="math display">\[\begin{align}
  \mathbf{A} &amp;= N_{ij} \Qij + \Lk \\
  \mathbf{B} &amp;= \qij \\
  \mathbf{C} &amp;= \qij^\mathrm{T} \\
  \mathbf{D} &amp;=- N_{ij}^{-1} \\
\end{align}\]</span>
<span class="math display" id="eq:fast-inverse-mat-Lijk">\[\begin{align}
      \left( \H_{ij} - \lambda_w \I + \Lk \right)^{-1} &amp; = \mathbf{A}^{-1} - \mathbf{A}^{-1} \qij  \left( -N_{ij}^{-1}  + \qij^\mathrm{T} \mathbf{A}^{-1} \qij \right)^{-1}  \qij^\mathrm{T} \mathbf{A}^{-1} \\
     &amp; = \mathbf{A}^{-1} + \frac{ (\mathbf{A}^{-1} \qij) (\mathbf{A}^{-1} \qij)^{\mathrm{T}} }{ N_{ij}^{-1} - \qij^\mathrm{T} \mathbf{A}^{-1} \qij} \,.
\tag{6.9}
\end{align}\]</span>
<p>Note that <span class="math inline">\(\mathbf{A}\)</span> is diagonal as <span class="math inline">\(\Qij\)</span> and <span class="math inline">\(\Lk\)</span> are diagonal matrices: <span class="math inline">\(\mathbf{A} = \text{diag}(N_{ij} q&#39;_{ijab} + (\Lk)_{ab,ab})\)</span>. Moreover, <span class="math inline">\(\mathbf{A}\)</span> has only positive diagonal elements, because <span class="math inline">\(\Lk\)</span> is invertible and has only positive diagonal elements and because <span class="math inline">\(q&#39;_{ijab} = p(x_i \eq a, x_j \eq b | \v^*,\w^*) \ge 0\)</span>.</p>
<p>Therefore <span class="math inline">\(\mathbf{A}\)</span> is invertible: <span class="math inline">\(\mathbf{A}^{-1} = \text{diag}(N_{ij} q&#39;_{ijab} + (\Lk)_{ab,ab} )^{-1}\)</span>.</p>
<p>Because <span class="math inline">\(\sum_{a,b=1}^{20} q&#39;_{ijab} = 1\)</span>, the denominator of the second term is</p>
<span class="math display">\[\begin{equation}
    N_{ij}^{-1} - \sum_{a,b=1}^{20}  \frac{{q&#39;}_{ijab}^2}{N_{ij} q&#39;_{ijab} + {(\Lk)}_{ab,ab} } &gt; N_{ij}^{-1} - \sum_{a,b=1}^{20} \frac{{q&#39;}^2_{ijab}}{N_{ij} q&#39;_{ijab}} = 0
\end{equation}\]</span>
<p>and therefore the inverse of <span class="math inline">\(\Lijk\)</span> in eq. <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:fast-inverse-mat-Lijk">(6.9)</a> is well defined.</p>
<p>The log determinant of <span class="math inline">\(\Lijk\)</span> is necessary to compute the ratio of Gaussians (see equation <a href="#eq:p-X-r-final">(<strong>??</strong>)</a>) and can be computed using the matrix determinant lemma:</p>
<span class="math display">\[\begin{equation}
  \det(\mathbf{A} + \mathbf{uv}^\mathrm{T}) = (1+\mathbf{v}^\mathrm{T} \mathbf{A}^{-1} \mathbf{u}) \det(\mathbf{A})
\end{equation}\]</span>
<p>Setting <span class="math inline">\(\mathbf{A} = N_{ij} \Qij + \Lk\)</span> and <span class="math inline">\(\v = \qij\)</span> and <span class="math inline">\(\mathbf{u} = - N_{ij} \qij\)</span> yields</p>
<span class="math display">\[\begin{equation}
  \det(\Lijk ) = \det(\H_{ij} - \lambda_w \I + \Lk) = (1 - N_{ij}\qij^\mathrm{T} \mathbf{A}^{-1}\qij) \det(\mathbf{A}) \,.
\end{equation}\]</span>
<p><span class="math inline">\(\mathbf{A}\)</span> is diagonal and has only positive diagonal elements so that <span class="math inline">\(\log(\det(\mathbf{A})) = \sum \log \left( \text{diag}(\mathbf{A}) \right)\)</span>.</p>
</div>
<div id="training-hyperparameters" class="section level3">
<h3><span class="header-section-number">6.5.3</span> Training the Hyperparameters <span class="math inline">\(\muk\)</span>, <span class="math inline">\(\Lk\)</span> and <span class="math inline">\(\gamma_k\)</span></h3>
<p>The model parameters <span class="math inline">\(\mathbf{\mu} = (\mathbf{\mu}_{1},\ldots,\mathbf{\mu}_K)\)</span>, <span class="math inline">\(\mathbf{\Lambda} = (\mathbf{\Lambda}_1,\ldots,\mathbf{\Lambda}_K)\)</span> and <span class="math inline">\(\mathbf{\gamma} = (\mathbf{\gamma}_1,\ldots,\mathbf{\gamma}_K)\)</span> will be trained by maximizing the logarithm of the full likelihood over a set of training <a href="abbrev.html#abbrev">MSAs</a> <span class="math inline">\(\X^1,\ldots,\X^N\)</span> and associated structures with distance vectors <span class="math inline">\(\r^1,\ldots,\r^N\)</span> plus a regularizer <span class="math inline">\(R(\mathbf{\mu}, \mathbf{\Lambda})\)</span>:</p>
<span class="math display">\[\begin{equation}
    L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \mathbf{\gamma}) + R(\mathbf{\mu}, \mathbf{\Lambda}) = \sum_{n=1}^N  \log p(\X^n | \r^n, \mathbf{\mu}, \mathbf{\Lambda}, \mathbf{\gamma} ) + R(\mathbf{\mu}, \mathbf{\Lambda})  \rightarrow \max \, .
\end{equation}\]</span>
<p>The regulariser penalizes values of <span class="math inline">\(\muk\)</span> and <span class="math inline">\(\Lk\)</span> that deviate too far from zero:</p>
<span class="math display" id="eq:reg">\[\begin{align}
    R(\mathbf{\mu}, \mathbf{\Lambda}) = -\frac{1}{2 \sigma_{\mu}^2} \sum_{k=1}^K \sum_{ab=1}^{400} \mu_{k,ab}^2 
                        -\frac{1}{2 \sigma_\text{diag}^2} \sum_{k=1}^K \sum_{ab=1}^{400} \Lambda_{k,ab,ab}^2
\tag{6.10}
\end{align}\]</span>
<p>Reasonable values are <span class="math inline">\(\sigma_{\mu}=0.1\)</span>, <span class="math inline">\(\sigma_\text{diag} = 100\)</span>.</p>
<p>The log likelihood can be optimized using LBFG-S-B<span class="citation">[<span class="citeproc-not-found" data-reference-id="CITE"><strong>???</strong></span>]</span>, which requires the computation of the gradient of the log likelihood. For simplicity of notation, the following calculations consider the contribution of the log likelihood for just one protein, which allows to drop the index <span class="math inline">\(n\)</span> in <span class="math inline">\(\rij^n\)</span>, <span class="math inline">\((\wij^n)^*\)</span> and <span class="math inline">\(\Hij^n\)</span>.</p>
<p>From eq. <a href="likelihood-fct-distances.html#eq:pXr-final">(4.12)</a> the log likelihood for a single protein is</p>
<span class="math display" id="eq:ll-coupling-prior">\[\begin{equation}
    L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k) =  \sum_{1 \le i &lt; j \le L}  \log \sum_{k=0}^K g_{k}(\rij) \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})}  + R(\mathbf{\mu}, \mathbf{\Lambda}) + \text{const.}\,.
\tag{6.11}
\end{equation}\]</span>
</div>
<div id="the-gradient-of-the-log-likelihood-with-respect-to-mathbfmu" class="section level3">
<h3><span class="header-section-number">6.5.4</span> The gradient of the log likelihood with respect to <span class="math inline">\(\mathbf{\mu}\)</span></h3>
<p>By applying the formula <span class="math inline">\(d f(x) / dx = f(x) \, d \log f(x) / dx\)</span> to compute the gradient of eq. <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:ll-coupling-prior">(6.11)</a> (neglecting the regularization term) with respect to <span class="math inline">\(\mu_{k,ab}\)</span>, one obtains</p>
<span class="math display" id="eq:gradient-mukab">\[\begin{equation}
 \frac{\partial}{\partial \mu_{k,ab}} L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    = \sum_{1\le i&lt;j\le L}  
    \frac{ 
        g_{k}(\rij) \frac{  \Gauss ( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} 
             \frac{\partial}{\partial \mu_{k,ab}}  \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right)  
     } { \sum_{k&#39;=0}^K g_{k&#39;}(\rij) \, \frac{ \Gauss(\mathbf{0} | \muk&#39;, \Lk&#39;^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})}  } .
\tag{6.12}
\end{equation}\]</span>
<p>To simplify this expression, we define the responsibility of component <span class="math inline">\(k\)</span> for the posterior distribution of <span class="math inline">\(\wij\)</span>, the probability that <span class="math inline">\(\wij\)</span> has been generated by component <span class="math inline">\(k\)</span>:</p>
<span class="math display" id="eq:responsibilities">\[\begin{align}
      p(k|ij)  = 
      \frac{ g_{k}(\rij) \frac{ \Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})} } 
    {\sum_{k&#39;=0}^K g_{k&#39;}(\rij) \frac{ \Gauss(\mathbf{0} | \muk&#39;, \Lk&#39;^{-1})}{\Gauss( \mathbf{0} | \muijk&#39;, \Lijk&#39;^{-1})} }  \,.
\tag{6.13}
\end{align}\]</span>
<p>By substituting the definition for responsibility, <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:gradient-mukab">(6.12)</a> simplifies</p>
<span class="math display" id="eq:gradient-LL-mukab">\[\begin{equation}
  \frac{\partial}{\partial \mu_{k,ab}}  L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    = \sum_{1\le i&lt;j\le L}  p(k | ij)  \frac{\partial}{\partial \mu_{k,ab}} \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right) ,
\tag{6.14}
\end{equation}\]</span>
<p>and analogously for partial derivatives with respect to <span class="math inline">\(\Lambda_{k,ab,cd}\)</span>.</p>
The partial derivative inside the sum can be written
<span class="math display">\[\begin{equation}
     \frac{\partial}{\partial \mu_{k,ab}} \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right)
    = \frac{1}{2}  \frac{\partial}{\partial \mu_{k,ab}}   \left( \log | \Lk | - \muk^\mathrm{T} \Lk \muk - \log | \Lijk | + \muijk^\mathrm{T} \Lijk \muijk \right)\,.
\end{equation}\]</span>
Using the following formula for a matrix <span class="math inline">\(\mathbf{A}\)</span>, a real variable <span class="math inline">\(x\)</span> and a vector <span class="math inline">\(\mathbf{y}\)</span> that depends on <span class="math inline">\(x\)</span>,
<span class="math display" id="eq:matrix-gradient">\[\begin{equation}
    \frac{\partial}{\partial x} \left( \mathbf{y}^\mathrm{T} \mathbf{A} \mathbf{y} \right) = \frac{\partial \mathbf{y}^\mathrm{T}}{\partial x}  \mathbf{A} \mathbf{y} + \mathbf{y}^\mathrm{T} \mathbf{A} \frac{\partial \mathbf{y}}{\partial x}  =  \mathbf{y}^\mathrm{T} (\mathbf{A} + \mathbf{A}^\mathrm{T}) \frac{\partial \mathbf{y}}{\partial x} 
\tag{6.15}
\end{equation}\]</span>
<p>the partial derivative therefore becomes</p>
<span class="math display">\[\begin{align}
     \frac{\partial}{\partial \mu_{k,ab}} \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right)
    =&amp; \left( -\muk^\mathrm{T} \Lk \mathbf{e}_{ab} \, +  \muijk^\mathrm{T} \Lijk \Lijk^{-1} \Lk \mathbf{e}_{ab} \right) \\
    =&amp; \mathbf{e}^\mathrm{T}_{ab} \Lk ( \muijk - \muk ) \; . 
\end{align}\]</span>
<p>Finally, the gradient of the log likelihood with respect to <span class="math inline">\(\mathbf{\mu}\)</span> becomes</p>
<span class="math display" id="eq:gradient-muk-final">\[\begin{align}
    \nabla_{\muk} L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    =  \sum_{1\le i&lt;j\le L}  p(k|ij)  \,  \Lk \left(  \muijk  - \muk \right) \; .
\tag{6.16}
\end{align}\]</span>
</div>
<div id="the-gradient-of-the-log-likelihood-with-respect-to-lk" class="section level3">
<h3><span class="header-section-number">6.5.5</span> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></h3>
<p>Analogously to eq. <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:gradient-LL-mukab">(6.14)</a> one first needs to solve</p>
<span class="math display" id="eq:grad-log-N-N-lambdakabcd">\[\begin{align}
     &amp; \frac{\partial}{\partial \Lambda_{k,ab,cd}} \log \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} 
    = \\
    &amp;\frac{1}{2}  \frac{\partial}{\partial \Lambda_{k,ab,cd}}  \left( \log |\Lk| - \muk^\mathrm{T} \Lk \muk - \log |\Lijk| + \muijk^\mathrm{T} \Lijk \muijk \right) \,,
\tag{6.17}
\end{align}\]</span>
<p>by applying eq. <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:matrix-gradient">(6.15)</a> as before as well as the formulas</p>
<span class="math display">\[\begin{align}
    \frac{\partial}{\partial x} \log |\mathbf{A} | &amp;= \text{Tr}\left( \mathbf{A}^{-1} \frac{\partial \mathbf{A}}{\partial x}  \right) , \\
    \frac{\partial \mathbf{A}^{-1}}{\partial x} &amp;= - \mathbf{A}^{-1} \frac{\partial \mathbf{A}}{\partial x} \mathbf{A}^{-1} \,.
\end{align}\]</span>
<p>This yields</p>
<span class="math display">\[\begin{align}
\frac{\partial}{\partial \Lambda_{k,ab,cd}}  \log |\Lk|
     &amp;= \text{Tr} \left( \Lk^{-1} \frac{\partial \Lk}{\partial \Lambda_{k,ab,cd}} \right) 
     = \text{Tr} \left( \Lk^{-1} \mathbf{e}_{ab} \mathbf{e}_{cd}^\mathrm{T} \right) 
     = \Lambda^{-1}_{k,cd,ab} \\
\frac{\partial}{\partial \Lambda_{k,ab,cd}}  \log |\Lijk|
     &amp;= \text{Tr} \left( \Lijk^{-1} \frac{\partial (\H_{ij} - \lambda_w \I + \Lk)}{\partial \Lambda_{k,ab,cd}}   \right) 
     = \Lambda^{-1}_{ij,k,cd,ab} \\
\frac{\partial (\muk^\mathrm{T} \Lk \muk)}{\partial \Lambda_{k,ab,cd}} 
    &amp;= \muk^\mathrm{T} \mathbf{e}_{ab} \mathbf{e}_{cd}^\mathrm{T} \muk 
    = \mathbf{e}_{ab}^\mathrm{T} \muk \muk^\mathrm{T} \mathbf{e}_{cd} = (\muk \muk^\mathrm{T})_{ab,cd} \\
\frac{\partial ( \muijk^\mathrm{T} \Lijk \muijk) }{\partial \Lambda_{k,ab,cd}} 
    &amp;= \muijk^\mathrm{T} \frac{\partial \Lijk}{\partial \Lambda_{k,ab,cd}} \muijk 
    + 2 \muijk^\mathrm{T} \Lijk \frac{\partial \Lijk^{-1}}{\partial \Lambda_{k,ab,cd}}  (\Hij \wij^* + \Lk \muk) 
    + 2 \muijk^\mathrm{T} \frac{\partial \Lk}{\partial \Lambda_{k,ab,cd}} \muk \nonumber \\
    &amp;= (\muijk \muijk^\mathrm{T} + 2 \muijk \muk^\mathrm{T})_{ab,cd} 
    - 2 \muijk^\mathrm{T} \Lijk  \Lijk^{-1} \frac{\partial \Lijk}{\partial \Lambda_{k,ab,cd}} \Lijk^{-1} (\Hij\wij^* + \Lk \muk) \\
    &amp;= (\muijk \muijk^\mathrm{T} + 2 \muijk \muk^\mathrm{T})_{ab,cd} 
    - 2 \muijk^\mathrm{T}  \frac{\partial \Lijk}{\partial \Lambda_{k,ab,cd}} \muijk\\
    &amp;= (- \muijk \muijk^\mathrm{T} + 2 \muijk \muk^\mathrm{T})_{ab,cd} \,.
\end{align}\]</span>
<p>Inserting these results into eq. <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:grad-log-N-N-lambdakabcd">(6.17)</a> yields</p>
<span class="math display">\[\begin{align}
     \frac{\partial}{\partial \Lambda_{k,ab,cd}} \log \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} 
    = \frac{1}{2} \left( \Lk^{-1} - \Lijk^{-1} - (\muijk - \muk) (\muijk - \muk)^\mathrm{T} \right)_{ab,cd}\,.
\end{align}\]</span>
<p>Substituting this expression into the equation <a href="bayesian-model-for-residue-resdiue-contact-prediction.html#eq:gradient-LL-mukab">(6.14)</a> analogous to the derivation of gradient for <span class="math inline">\(\mu_{k,ab}\)</span> yields the equation</p>
<span class="math display" id="eq:gradient-lambdak-final">\[\begin{align}
    \nabla_{\Lk}  L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    =  \frac{1}{2} \sum_{1\le i&lt;j\le L}  p(k|ij)  \, 
        \left( \Lk^{-1} - \Lijk^{-1} - (\muijk - \muk) (\muijk - \muk)^\mathrm{T} \right). 
\tag{6.18}
\end{align}\]</span>
</div>
<div id="the-gradient-of-the-log-likelihood-with-respect-to-gamma_k" class="section level3">
<h3><span class="header-section-number">6.5.6</span> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></h3>
<p>With <span class="math inline">\(\rij \in \{0,1\}\)</span> defining a residue pair in physical contact or not in contact, the mixing weights can be modelled as a softmax function according to eq. <a href="coupling-prior.html#eq:def-g-k-binary">(4.5)</a>. The derivative of the mixing weights <span class="math inline">\(g_k(\rij)\)</span> is:</p>
<span class="math display">\[\begin{eqnarray}
\frac{\partial g_{k&#39;}(\rij)} {\partial \gamma_k} = \left\{
  \begin{array}{lr}
    g_k(\rij) (1 - g_k(\rij)) &amp; : k&#39; = k\\
    g_{k&#39;}(\rij) - g_k(\rij)  &amp; : k&#39; \neq k
  \end{array}
  \right.
\end{eqnarray}\]</span>
<p>The partial derivative of the likelihood function with respect to <span class="math inline">\(\gamma_k\)</span> is:</p>
<span class="math display">\[\begin{align}
\frac{\partial} {\partial \gamma_k}     L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k) 
  =&amp;  \sum_{1\le i&lt;j\le L} \frac{\sum_{k&#39;=0}^K  \frac{\partial}{\partial \gamma_k} g_{k&#39;}(\rij)  
  \frac{\Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( 0 | \muijk, \Lijk^{-1})}}
  {\sum_{k&#39;=0}^K g_{k&#39;}(\rij)  \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})}} \\
  =&amp;  \sum_{1\le i&lt;j\le L} \frac{\sum_{k&#39;=0}^K  g_{k&#39;}(\rij)  
  \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \cdot 
  \begin{cases} 
   1-g_k(\rij) &amp; \text{if } k&#39; = k \\
   -g_k(\rij)  &amp; \text{if } k&#39; \neq k
  \end{cases}}
  {\sum_{k&#39;=0}^K g_{k&#39;}(\rij)  \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})}} \\
  =&amp; \sum_{1\le i&lt;j\le L} \sum_{k&#39;=0}^K p(k&#39;|ij) 
  \begin{cases} 
    1-g_k(\rij) &amp; \text{if } k&#39; = k \\
    -g_k(\rij)  &amp; \text{if } k&#39; \neq k 
  \end{cases} \\
  =&amp; \sum_{1 \leq i&lt;j\leq L} p(k|ij) - g_k(\rij) \sum_{k&#39;=0}^K p(k&#39;|ij) \nonumber\\
  =&amp; \sum_{1 \leq i&lt;j\leq L} p(k|ij) - g_k(\rij)
\end{align}\]</span>
</div>
</div>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://contactpredictionthesis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="methods-optimizing-full-likelihood.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/susannvorberg/phd_thesis/edit/master/04-methods.Rmd",
"text": "Edit"
},
"download": ["PhD_thesis_Susann_Vorberg.pdf"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
