<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>PhD thesis: residue-residue contact prediction</title>
  <meta name="description" content="This is my PhD thesis on residue-residue contact prediction.">
  <meta name="generator" content="bookdown 0.3.9 and GitBook 2.6.7">

  <meta property="og:title" content="PhD thesis: residue-residue contact prediction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my PhD thesis on residue-residue contact prediction." />
  <meta name="github-repo" content="susannvorberg/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PhD thesis: residue-residue contact prediction" />
  
  <meta name="twitter:description" content="This is my PhD thesis on residue-residue contact prediction." />
  

<meta name="author" content="Susann Vorberg">


<meta name="date" content="2017-10-30">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="general-intro.html">
<link rel="next" href="application-contact-prediction.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
  tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
  TeX: { 
    extensions: ["mediawiki-texvc.js", "sinuitx.js"],
    Macros: {
      Cb: "C_\\beta",
      eq: "\\!=\\!",
      Gauss: "\\mathcal{N}",
      H: "\\mathbf{H}",
      Hij : "\\H_{ij}",
      I: "\\mathbf{I}",
      Lijk: "\\mathbf{\\Lambda}_{ij,k}",
      Lk: "\\mathbf{\\Lambda}_k",
      LL: "L\\!L(\\mathbf{v}, \\mathbf{w})",
      LLreg: "L\\!L_\\mathrm{reg}",
      muijk: "\\mathbf{\\mu}_{ij,k}",
      muk: "\\mathbf{\\mu}_k",
      neff: "N_\\mathrm{eff}",
      r: "\\mathbf{r}",
      rij: "r_{ij}",
      c: "\\mathbf{c}",
      cij: "c_{ij}",
      seq: "\\mathbf{x}",
      Qij: "\\mathbf{Q}_{ij}",
      q: "\\mathbf{q}",
      qij: "\\mathbf{q'}_{ij}",
      Sn: "\\mathcal{S}_n",
      v: "\\mathbf{v}",
      vi: "\\mathcal{v}_{i}",
      vj: "\\mathcal{v}_{j}",
      via: "\\mathcal{v}_{ia}",
      vja: "\\mathcal{v}_{ja}",
      w: "\\mathbf{w}",
      wij: "\\mathbf{w}_{ij}",
      wijab: "\\mathcal{w}_{ijab}",
      wijcd: "\\mathcal{w}_{ijcd}",
      wklcd: "\\mathcal{w}_{klcd}",
      X: "\\mathbf{X}",
      angstrom: "\\AA \\; \\;"
      }
  }
});
</script>


 
<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js", "[siunitx]/siunitx.js"],
  TeX: { TagSide: "left" }
});
MathJax.Ajax.config.path['siunitx']  = '../latex/MathJax-siunitx-master/';
</script>
//-->



<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD Thesis Susann Vorberg</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>1</b> Background</a><ul>
<li class="chapter" data-level="1.1" data-path="general-intro.html"><a href="general-intro.html"><i class="fa fa-check"></i><b>1.1</b> Biological Background</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html"><i class="fa fa-check"></i><b>1.2</b> Introduction to Contact Prediction</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html#local-methods"><i class="fa fa-check"></i><b>1.2.1</b> Local Statistical Models</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html#global-methods"><i class="fa fa-check"></i><b>1.2.2</b> Global Statistical Models</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html#meta-predictors"><i class="fa fa-check"></i><b>1.2.3</b> Machine Learning Methods and Meta-Predictors</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html#maxent"><i class="fa fa-check"></i><b>1.2.4</b> Modelling Protein Families with Potts Model</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="application-contact-prediction.html"><a href="application-contact-prediction.html"><i class="fa fa-check"></i><b>1.3</b> Applications</a></li>
<li class="chapter" data-level="1.4" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html"><i class="fa fa-check"></i><b>1.4</b> Evaluating Contact Prediction Methods</a><ul>
<li class="chapter" data-level="1.4.1" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#seq-sep"><i class="fa fa-check"></i><b>1.4.1</b> Sequence Separation</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#interpretation-of-evaluation-results"><i class="fa fa-check"></i><b>1.4.2</b> Interpretation of Evaluation Results</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>1.5</b> Challenges for Coevolutionary Inference</a><ul>
<li class="chapter" data-level="1.5.1" data-path="challenges.html"><a href="challenges.html#phylogenetic-noise"><i class="fa fa-check"></i><b>1.5.1</b> Phylogenetic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="1.5.2" data-path="challenges.html"><a href="challenges.html#entropic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>1.5.2</b> Entropic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="1.5.3" data-path="challenges.html"><a href="challenges.html#finite-sampling-effects"><i class="fa fa-check"></i><b>1.5.3</b> Finite Sampling Effects</a></li>
<li class="chapter" data-level="1.5.4" data-path="challenges.html"><a href="challenges.html#multiple-sequence-alignments"><i class="fa fa-check"></i><b>1.5.4</b> Multiple Sequence Alignments</a></li>
<li class="chapter" data-level="1.5.5" data-path="challenges.html"><a href="challenges.html#alternative-sources-of-coevolution"><i class="fa fa-check"></i><b>1.5.5</b> Alternative Sources of Coevolution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpreting-coupling-matrices.html"><a href="interpreting-coupling-matrices.html"><i class="fa fa-check"></i><b>2</b> Interpretation of Coupling Matrices</a><ul>
<li class="chapter" data-level="2.1" data-path="correlation-between-couplings-and-class.html"><a href="correlation-between-couplings-and-class.html"><i class="fa fa-check"></i><b>2.1</b> Single Coupling Values Carry Evidence of Contacts</a></li>
<li class="chapter" data-level="2.2" data-path="coupling-profiles.html"><a href="coupling-profiles.html"><i class="fa fa-check"></i><b>2.2</b> Coupling Profiles Vary with Distance</a></li>
<li class="chapter" data-level="2.3" data-path="physico-chemical-fingerprints-in-coupling-matrices.html"><a href="physico-chemical-fingerprints-in-coupling-matrices.html"><i class="fa fa-check"></i><b>2.3</b> Physico-Chemical Fingerprints in Coupling Matrices</a></li>
<li class="chapter" data-level="2.4" data-path="higher-order-coupling-profiles.html"><a href="higher-order-coupling-profiles.html"><i class="fa fa-check"></i><b>2.4</b> Higher Order Dependencies Between Couplings</a></li>
<li class="chapter" data-level="2.5" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
<li class="chapter" data-level="2.6" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2.6</b> Methods</a><ul>
<li class="chapter" data-level="2.6.1" data-path="methods.html"><a href="methods.html#dataset"><i class="fa fa-check"></i><b>2.6.1</b> Dataset</a></li>
<li class="chapter" data-level="2.6.2" data-path="methods.html"><a href="methods.html#diff-ccmpred-ccmpredpy"><i class="fa fa-check"></i><b>2.6.2</b> Computing Pseudo-Likelihood Couplings</a></li>
<li class="chapter" data-level="2.6.3" data-path="methods.html"><a href="methods.html#seq-reweighting"><i class="fa fa-check"></i><b>2.6.3</b> Sequence Reweighting</a></li>
<li class="chapter" data-level="2.6.4" data-path="methods.html"><a href="methods.html#amino-acid-frequencies"><i class="fa fa-check"></i><b>2.6.4</b> Computing Amino Acid Frequencies</a></li>
<li class="chapter" data-level="2.6.5" data-path="methods.html"><a href="methods.html#methods-regularization"><i class="fa fa-check"></i><b>2.6.5</b> Regularization</a></li>
<li class="chapter" data-level="2.6.6" data-path="methods.html"><a href="methods.html#method-coupling-correlation"><i class="fa fa-check"></i><b>2.6.6</b> Correlation of Couplings with Contact Class</a></li>
<li class="chapter" data-level="2.6.7" data-path="methods.html"><a href="methods.html#method-coupling-profile"><i class="fa fa-check"></i><b>2.6.7</b> Coupling Distribution Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="optimizing-full-likelihood.html"><a href="optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>3</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="3.1" data-path="full-likelihood-gradient.html"><a href="full-likelihood-gradient.html"><i class="fa fa-check"></i><b>3.1</b> Approximating the Gradient of the Full Likelihood with Contrastive Divergence</a></li>
<li class="chapter" data-level="3.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html"><i class="fa fa-check"></i><b>3.2</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="3.2.1" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#convergence-criteria-sgd"><i class="fa fa-check"></i><b>3.2.1</b> Convergence Criterion for Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="3.2.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#sgd-hyperparameter-tuning"><i class="fa fa-check"></i><b>3.2.2</b> Tuning Hyperparameters of Stochastic Gradient Descent Optimizer</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html"><i class="fa fa-check"></i><b>3.3</b> Tuning the Gibbs Sampling Scheme for Contrastive Divergence</a><ul>
<li class="chapter" data-level="3.3.1" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#regularization-for-cd-with-sgd"><i class="fa fa-check"></i><b>3.3.1</b> Tuning Regularization Coefficients for Contrastive Divergence</a></li>
<li class="chapter" data-level="3.3.2" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-sampling-size"><i class="fa fa-check"></i><b>3.3.2</b> Varying the Sample Size</a></li>
<li class="chapter" data-level="3.3.3" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>3.3.3</b> Varying the number of Gibbs Steps</a></li>
<li class="chapter" data-level="3.3.4" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>3.3.4</b> Persistent Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="adam-results.html"><a href="adam-results.html"><i class="fa fa-check"></i><b>3.4</b> Using ADAM to Optimize Contrastive Divergence</a><ul>
<li class="chapter" data-level="3.4.1" data-path="adam-results.html"><a href="adam-results.html#adam-violates-sum-wij"><i class="fa fa-check"></i><b>3.4.1</b> A <em>Potts</em> model specific convergence criterion</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="comparing-pll-cd.html"><a href="comparing-pll-cd.html"><i class="fa fa-check"></i><b>3.5</b> Comparing CD couplings to pLL couplings</a><ul>
<li class="chapter" data-level="3.5.1" data-path="comparing-pll-cd.html"><a href="comparing-pll-cd.html#protein-1c75a00"><i class="fa fa-check"></i><b>3.5.1</b> Protein 1c75A00</a></li>
<li class="chapter" data-level="3.5.2" data-path="comparing-pll-cd.html"><a href="comparing-pll-cd.html#protein-1ss3a00-and-1c55a00"><i class="fa fa-check"></i><b>3.5.2</b> Protein 1ss3A00 and 1c55A00</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="discussion-1.html"><a href="discussion-1.html"><i class="fa fa-check"></i><b>3.6</b> Discussion</a></li>
<li class="chapter" data-level="3.7" data-path="methods-1.html"><a href="methods-1.html"><i class="fa fa-check"></i><b>3.7</b> Methods</a><ul>
<li class="chapter" data-level="3.7.1" data-path="methods-1.html"><a href="methods-1.html#potts-full-likelihood"><i class="fa fa-check"></i><b>3.7.1</b> The Potts Model</a></li>
<li class="chapter" data-level="3.7.2" data-path="methods-1.html"><a href="methods-1.html#gap-treatment"><i class="fa fa-check"></i><b>3.7.2</b> Treating Gaps as Missing Information</a></li>
<li class="chapter" data-level="3.7.3" data-path="methods-1.html"><a href="methods-1.html#the-regularized-full-log-likelihood-and-its-gradient-with-gap-treatment"><i class="fa fa-check"></i><b>3.7.3</b> The Regularized Full Log Likelihood and its Gradient With Gap Treatment</a></li>
<li class="chapter" data-level="3.7.4" data-path="methods-1.html"><a href="methods-1.html#prior-v"><i class="fa fa-check"></i><b>3.7.4</b> The prior on single potentials</a></li>
<li class="chapter" data-level="3.7.5" data-path="methods-1.html"><a href="methods-1.html#methods-sgd"><i class="fa fa-check"></i><b>3.7.5</b> Stochastic Gradien Descent</a></li>
<li class="chapter" data-level="3.7.6" data-path="methods-1.html"><a href="methods-1.html#methods-cd-sampling"><i class="fa fa-check"></i><b>3.7.6</b> Computing the Gradient with Contrastive Divergence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="contact-prior.html"><a href="contact-prior.html"><i class="fa fa-check"></i><b>4</b> Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="4.1" data-path="random-forest-classifiers.html"><a href="random-forest-classifiers.html"><i class="fa fa-check"></i><b>4.1</b> Random Forest Classifiers</a></li>
<li class="chapter" data-level="4.2" data-path="hyperparameter-optimization-for-random-forest.html"><a href="hyperparameter-optimization-for-random-forest.html"><i class="fa fa-check"></i><b>4.2</b> Hyperparameter Optimization for Random Forest</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-random-forest-model-as-contact-predictor.html"><a href="evaluating-random-forest-model-as-contact-predictor.html"><i class="fa fa-check"></i><b>4.3</b> Evaluating Random Forest Model as Contact Predictor</a></li>
<li class="chapter" data-level="4.4" data-path="contact-prior-add-features.html"><a href="contact-prior-add-features.html"><i class="fa fa-check"></i><b>4.4</b> Using Contact Scores as Additional Features</a></li>
<li class="chapter" data-level="4.5" data-path="discussion-2.html"><a href="discussion-2.html"><i class="fa fa-check"></i><b>4.5</b> Discussion</a></li>
<li class="chapter" data-level="4.6" data-path="methods-2.html"><a href="methods-2.html"><i class="fa fa-check"></i><b>4.6</b> Methods</a><ul>
<li class="chapter" data-level="4.6.1" data-path="methods-2.html"><a href="methods-2.html#seq-features"><i class="fa fa-check"></i><b>4.6.1</b> Features used to train Random Forest Model</a></li>
<li class="chapter" data-level="4.6.2" data-path="methods-2.html"><a href="methods-2.html#simple-contact-prior-with-respect-to-protein-length"><i class="fa fa-check"></i><b>4.6.2</b> Simple Contact Prior with Respect to Protein Length</a></li>
<li class="chapter" data-level="4.6.3" data-path="methods-2.html"><a href="methods-2.html#rf-training"><i class="fa fa-check"></i><b>4.6.3</b> Cross-validation for Random Forest Training</a></li>
<li class="chapter" data-level="4.6.4" data-path="methods-2.html"><a href="methods-2.html#rf-feature-selection"><i class="fa fa-check"></i><b>4.6.4</b> Feature Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayesian-approach.html"><a href="bayesian-approach.html"><i class="fa fa-check"></i><b>5</b> A Bayesian Statistical Model for Residue-Residue Contact Prediction</a><ul>
<li class="chapter" data-level="5.1" data-path="overview-posterior-distances.html"><a href="overview-posterior-distances.html"><i class="fa fa-check"></i><b>5.1</b> Computing the Posterior Probabilty of a Contact</a></li>
<li class="chapter" data-level="5.2" data-path="coupling-prior.html"><a href="coupling-prior.html"><i class="fa fa-check"></i><b>5.2</b> Modelling the Prior Over Couplings Depending on Contact States</a></li>
<li class="chapter" data-level="5.3" data-path="bayesian-model-training-hyperparameters.html"><a href="bayesian-model-training-hyperparameters.html"><i class="fa fa-check"></i><b>5.3</b> Training the Hyperparameters in the Likelihood Function of Contact States</a><ul>
<li class="chapter" data-level="5.3.1" data-path="bayesian-model-training-hyperparameters.html"><a href="bayesian-model-training-hyperparameters.html#training-hyperparameters-for-a-gaussian-mixture-with-three-components"><i class="fa fa-check"></i><b>5.3.1</b> Training Hyperparameters for a Gaussian Mixture with Three Components</a></li>
<li class="chapter" data-level="5.3.2" data-path="bayesian-model-training-hyperparameters.html"><a href="bayesian-model-training-hyperparameters.html#training-hyperparameters-for-a-gaussian-mixture-with-five-and-ten-components"><i class="fa fa-check"></i><b>5.3.2</b> Training Hyperparameters for a Gaussian Mixture with Five and Ten Components</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="posterior-of-rij.html"><a href="posterior-of-rij.html"><i class="fa fa-check"></i><b>5.4</b> Evaluating the Bayesian Models for Contact Prediction</a></li>
<li class="chapter" data-level="5.5" data-path="analysing-contact-maps-predicted-with-bayesian-framework.html"><a href="analysing-contact-maps-predicted-with-bayesian-framework.html"><i class="fa fa-check"></i><b>5.5</b> Analysing Contact Maps Predicted With Bayesian Framework</a></li>
<li class="chapter" data-level="5.6" data-path="discussion-3.html"><a href="discussion-3.html"><i class="fa fa-check"></i><b>5.6</b> Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="methods-3.html"><a href="methods-3.html"><i class="fa fa-check"></i><b>5.7</b> Methods</a><ul>
<li class="chapter" data-level="5.7.1" data-path="methods-3.html"><a href="methods-3.html#methods-coupling-prior"><i class="fa fa-check"></i><b>5.7.1</b> Modelling the Prior Over Couplings Depending on Contact States</a></li>
<li class="chapter" data-level="5.7.2" data-path="methods-3.html"><a href="methods-3.html#laplace-approx"><i class="fa fa-check"></i><b>5.7.2</b> Gaussian Approximation to the Posterior of Couplings</a></li>
<li class="chapter" data-level="5.7.3" data-path="methods-3.html"><a href="methods-3.html#likelihood-fct-distances"><i class="fa fa-check"></i><b>5.7.3</b> Integrating out the Hidden Variables to Obtain the Likelihood Function of the Contact States</a></li>
<li class="chapter" data-level="5.7.4" data-path="methods-3.html"><a href="methods-3.html#Hessian-offdiagonal"><i class="fa fa-check"></i><b>5.7.4</b> The Hessian off-diagonal Elements Carry a Negligible Signal</a></li>
<li class="chapter" data-level="5.7.5" data-path="methods-3.html"><a href="methods-3.html#neg-Hessian-computation"><i class="fa fa-check"></i><b>5.7.5</b> Efficiently Computing the negative Hessian of the regularized log-likelihood</a></li>
<li class="chapter" data-level="5.7.6" data-path="methods-3.html"><a href="methods-3.html#inv-lambda-ij-k"><i class="fa fa-check"></i><b>5.7.6</b> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></a></li>
<li class="chapter" data-level="5.7.7" data-path="methods-3.html"><a href="methods-3.html#gradient-muk"><i class="fa fa-check"></i><b>5.7.7</b> The gradient of the log likelihood with respect to <span class="math inline">\(\muk\)</span></a></li>
<li class="chapter" data-level="5.7.8" data-path="methods-3.html"><a href="methods-3.html#gradient-lambdak"><i class="fa fa-check"></i><b>5.7.8</b> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></a></li>
<li class="chapter" data-level="5.7.9" data-path="methods-3.html"><a href="methods-3.html#the-gradient-of-the-log-likelihood-with-respect-to-gamma_k"><i class="fa fa-check"></i><b>5.7.9</b> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></a></li>
<li class="chapter" data-level="5.7.10" data-path="methods-3.html"><a href="methods-3.html#bayesian-model-distances"><i class="fa fa-check"></i><b>5.7.10</b> Extending the Bayesian Statistical Model for the Prediction of Protein Residue-Residue Distances</a></li>
<li class="chapter" data-level="5.7.11" data-path="methods-3.html"><a href="methods-3.html#training-hyperparameters-bayesian-model"><i class="fa fa-check"></i><b>5.7.11</b> Training the Hyperparameters in the Likelihood Function of Contact States</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="conclusion-and-outlook.html"><a href="conclusion-and-outlook.html"><i class="fa fa-check"></i><b>6</b> Conclusion and Outlook</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="abbrev.html"><a href="abbrev.html"><i class="fa fa-check"></i><b>A</b> Abbreviations</a></li>
<li class="chapter" data-level="B" data-path="amino-acids.html"><a href="amino-acids.html"><i class="fa fa-check"></i><b>B</b> Amino Acid Alphabet</a></li>
<li class="chapter" data-level="C" data-path="dataset-properties.html"><a href="dataset-properties.html"><i class="fa fa-check"></i><b>C</b> Dataset Properties</a></li>
<li class="chapter" data-level="D" data-path="interpretation-of-coupling-matrices.html"><a href="interpretation-of-coupling-matrices.html"><i class="fa fa-check"></i><b>D</b> Interpretation of Coupling Matrices</a></li>
<li class="chapter" data-level="E" data-path="optimizing-full-likelihood-with-gradient-descent.html"><a href="optimizing-full-likelihood-with-gradient-descent.html"><i class="fa fa-check"></i><b>E</b> Optimizing Full Likelihood with Gradient Descent</a></li>
<li class="chapter" data-level="F" data-path="training-of-the-random-forest-contact-prior.html"><a href="training-of-the-random-forest-contact-prior.html"><i class="fa fa-check"></i><b>F</b> Training of the Random Forest Contact Prior</a></li>
<li class="chapter" data-level="G" data-path="bayesian-statistical-model-for-contact-prediction.html"><a href="bayesian-statistical-model-for-contact-prediction.html"><i class="fa fa-check"></i><b>G</b> Bayesian statistical model for contact prediction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PhD thesis: residue-residue contact prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-contact-prediction" class="section level2">
<h2><span class="header-section-number">1.2</span> Introduction to Contact Prediction</h2>
<p>Contact prediction refers to the prediction of physical contacts between amino acid side chains in the 3D protein structure, given the protein sequence as input.</p>
<p>Historically, contact prediction was motivated by the idea that compensatory mutations between spatially neighboring residues can be traced down from evolutionary records <span class="citation">[<a href="#ref-Gobel1994">46</a>]</span>. As proteins evolve, they are under selective pressure to maintain their function and correspondingly their structure. Consequently, residues and interactions between residues constraining the fold, protein complex formation, or other aspects of function are under selective pressure. Highly constrained residues and interactions will be strongly conserved <span class="citation">[<a href="#ref-Godzik1989">47</a>]</span>. Another possibility to maintain structural integrity is the mutual compensation of unbeneficial mutations. For example, the unfavourable mutation of a small amino acid residue into a bulky residue in the densely packed protein core might have been compensated in the course of evolution by a particularly small side chain in a neighboring position. Other physico-chemical quantities such as amino acid charge or hydrogen bonding capacity can also induce compensatory effects<span class="citation">[<a href="#ref-Neher1994">48</a>]</span>. The <a href="abbrev.html#abbrev">MSA</a> of a protein family comprises homolog sequences that have descended from a common ancestor and are aligned relative to each other. According to the hypothesis, compensatory mutations show up as correlations between the amino acid types of pairs of <a href="abbrev.html#abbrev">MSA</a> columns and can be used to infer spatial proximity of residue pairs (see Figure <a href="introduction-to-contact-prediction.html#fig:correlated-mutations">1.3</a>).</p>

<div class="figure" style="text-align: center"><span id="fig:correlated-mutations"></span>
<img src="img/intro/correlated-mutations-transparent.png" alt="The evolutionary record of a protein family reveals evidence of compensatory mutations between spatially neighboring residues that are under selective pressure with respect to some physico-chemical constraints. Mining protein family sequence alignments for residue pairs with strong coevolutionary signals using statistical methods allows inference of spatial proximity for these residue pairs." width="90%" />
<p class="caption">
Figure 1.3: The evolutionary record of a protein family reveals evidence of compensatory mutations between spatially neighboring residues that are under selective pressure with respect to some physico-chemical constraints. Mining protein family sequence alignments for residue pairs with strong coevolutionary signals using statistical methods allows inference of spatial proximity for these residue pairs.
</p>
</div>
<p>The following sections will give an overview over important methods and developments in the field of contact prediction.</p>
<div id="local-methods" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Local Statistical Models</h3>
<p>Early contact prediction methods used local pairwise statistics to infer contacts that regard pairs of amino acids in a sequence as statistically independent from another.</p>
<p>Several of these methods use correlation coefficient based measures, such as Pearson correlation between amino acid counts, properties associated with amino acids or mutational propensities at the sites of a <a href="abbrev.html#abbrev">MSA</a> <span class="citation">[<a href="#ref-Gobel1994">46</a>,<a href="#ref-Neher1994">48</a>–<a href="#ref-Shindyalov1994">51</a>]</span>.</p>
<p>Many methods have been developed that are rooted in information theory and use <a href="abbrev.html#abbrev">MI</a> measures to describe the dependencies between sites in the alignment <span class="citation">[<a href="#ref-Clarke1995">52</a>–<a href="#ref-Martin2005">54</a>]</span>. Phylogenetic and entropic biases have been identified as strong sources of noise that confound the true coevolution signal <span class="citation">[<a href="#ref-Martin2005">54</a>–<a href="#ref-Fodor2004">56</a>]</span>. Different variants of <a href="abbrev.html#abbrev">MI</a> based approaches address these effects and improve on the signal-to-noise ratio <span class="citation">[<a href="#ref-Atchley2000">55</a>,<a href="#ref-Tillier2003">57</a>,<a href="#ref-Gouveia_Oliveira2007">58</a>]</span>. The most prominent correction for background noises is <a href="abbrev.html#abbrev">APC</a> that is still used by many modern methods and is discussed in section <a href="introduction-to-contact-prediction.html#post-processing-heuristics">1.2.4.6</a> <span class="citation">[<a href="#ref-Dunn2008">59</a>]</span>. Another popular method is <em>OMES</em> that essentially computes a chi-squared statistic to detect the differences between observed and expected pairwise amino acid frequencies for a pair of columns <span class="citation">[<a href="#ref-Kass2002">60</a>,<a href="#ref-Noivirt2005">61</a>]</span>.</p>
<p>The traditional covariance approaches suffered from high false positive rates because of their inability to cope with transitive effects that arise from chains of correlations between multiple residue pairs <span class="citation">[<a href="#ref-Weigt2009">38</a>,<a href="#ref-Lapedes1999">62</a>,<a href="#ref-Burger2010">63</a>]</span>. The concept of transitve effects is illustrated in Figure <a href="introduction-to-contact-prediction.html#fig:transitive-effect">1.4</a>. Considering three residues A, B and C, where A physically interacts with B and B with C. Strong statistical dependencies between pairs (A,B) and (B,C) can induce strong indirect signals for residues A and C, eventhough they are not physically interacting. These indirect correlations can become even larger than signals of other directly interacting pairs (D,E) and thus lead to false predictions <span class="citation">[<a href="#ref-Burger2010">63</a>]</span>.</p>
<p>Local statistical methods consider residue pairs independent of one another which is why they cannot distinguish between direct and indirect correlation signals. In contrast, global statistical models presented in the next section learn a joint probability distribution over all residues allowing to disentangle transitive effects <span class="citation">[<a href="#ref-Weigt2009">38</a>,<a href="#ref-Burger2010">63</a>]</span>. Eventhough local statistical methods cannot compete with modern predictors, <em>OMES</em> and <a href="abbrev.html#abbrev">MI</a> based scores often serve as a baseline in performance benchmarks for contact prediction <span class="citation">[<a href="#ref-DeJuan2013">64</a>,<a href="#ref-Jones2012">65</a>]</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:transitive-effect"></span>
<img src="img/intro/transitive_effects.png" alt="Effects of chained covariation obscure signals from true physical interactions. Consider residues A through E with physical interactions between the residue pairs A-B, B-C and D-E. The thickness of blue lines between residues reflects the strength of statistical dependencies between the corresponding alignment columns. Strong statistical dependencies between residue pairs (A,B) and (B,C) can induce a strong dependency between the spatially distant residues A and C. Covariation signals arising from transitive effects can become even stronger than other direct covariation signals and lead to false positive predictions." width="25%" />
<p class="caption">
Figure 1.4: Effects of chained covariation obscure signals from true physical interactions. Consider residues A through E with physical interactions between the residue pairs A-B, B-C and D-E. The thickness of blue lines between residues reflects the strength of statistical dependencies between the corresponding alignment columns. Strong statistical dependencies between residue pairs (A,B) and (B,C) can induce a strong dependency between the spatially distant residues A and C. Covariation signals arising from transitive effects can become even stronger than other direct covariation signals and lead to false positive predictions.
</p>
</div>
</div>
<div id="global-methods" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Global Statistical Models</h3>
<p>A huge leap forward was the development of sophisticated statistical models that make predictions for a single residue pair while considering all other pairs in the protein. These global models allow for the distinction between transitive and causal interactions which has been referred to in the literature as <a href="abbrev.html#abbrev">DCA</a> <span class="citation">[<a href="#ref-Weigt2009">38</a>,<a href="#ref-Lapedes1999">62</a>]</span>.</p>
<p>In 1999 Lapedes et al. were the first to propose a global statistical approach for the prediction of residue-residue contacts in order to disentangle transitive effects <span class="citation">[<a href="#ref-Lapedes1999">62</a>]</span>. They consider a Pott’s model that can be derived under a maximum entropy assumption and use the model specific coupling parameters to infer interactions. At that time the wider implications of this advancement went unnoted, but meanwhile the Pott’s Model has become the most prominent statistical model for contact prediction. Section <a href="introduction-to-contact-prediction.html#maxent">1.2.4</a> deals extensively with the derivation and properties of the Pott’s model, its application to contact prediction and its numerous realizations.</p>
<p>A global statistical model not motivated by the maximum entropy approach was proposed by Burger and Nijmwegen in 2010 <span class="citation">[<a href="#ref-Burger2010">63</a>,<a href="#ref-Burger2008">66</a>]</span>. Their fast Bayesian network model incorporates additional prior information and phylogenetic correction via <a href="abbrev.html#abbrev">APC</a> but cannot compete with the pseudo-likelihood approaches presented in section <a href="introduction-to-contact-prediction.html#pseudo-likelihood">1.2.4.5</a>.</p>
</div>
<div id="meta-predictors" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Machine Learning Methods and Meta-Predictors</h3>
<p>With the steady increase in protein sequence data, machine learning based methods have emerged that extract features from <a href="abbrev.html#abbrev">MSAs</a> in order to learn associations between input features and residue-residue contacts. Sequence features typically include predicted solvent accessibility, predicted secondary structure, contact potentials, conservation scores, global protein features, pairwise coevolution statistics and averages of certain features over sequence windows. Numerous sequence-based methods have been developed using machine learning algorithms, such as support support vector machines (<em>SVMCon</em> <span class="citation">[<a href="#ref-Cheng2007">67</a>]</span>, <em>SVM-SEQ</em> <span class="citation">[<a href="#ref-Wu2008">68</a>]</span>), random forests (<em>ProC_S3</em> <span class="citation">[<a href="#ref-Li2011">69</a>]</span>, <em>TMhhcp</em> <span class="citation">[<a href="#ref-Wang2011">70</a>]</span>, <em>PhyCMap</em> <span class="citation">[<a href="#ref-Wang2013">71</a>]</span>), neural networks (<em>NETCSS</em> <span class="citation">[<a href="#ref-Fariselli2001a">72</a>]</span>, <em>SAM</em> <span class="citation">[<a href="#ref-Shackelford2007">73</a>]</span>, <span class="citation">[<a href="#ref-Hamilton2004a">74</a>]</span>, <em>SPINE-2D</em> <span class="citation">[<a href="#ref-Xue2009a">75</a>]</span>, <em>NNCon</em> <span class="citation">[<a href="#ref-Tegge2009a">76</a>]</span>) deep neural networks (<em>DNCon</em> <span class="citation">[<a href="#ref-Eickholt2012">77</a>]</span>, <em>CMAPpro</em> <span class="citation">[<a href="#ref-DiLena2012a">78</a>]</span>) and ensembles of genetic algorithm classfiers (<em>GaC</em> <span class="citation">[<a href="#ref-Chen2010">79</a>]</span>).</p>
<p>Different contact predictors, especially when rooted in distinct principles like sequence-based and coevolution methods, provide orthogonal information on the likelihood that a pair of residues makes a contact <span class="citation">[<a href="#ref-Cheng2007">67</a>,<a href="#ref-Jones2015">80</a>]</span>. The next logical step in method development therefore constitutes the combination of several base predictors and classical sequence-derived features in the form of meta-predictors.</p>
<p>The first published meta-predictor was <em>PconsC</em> in 2013, combining sequence features and predictions from the coevolution methods <em>PSICOV</em> and <em>plmDCA</em> <span class="citation">[<a href="#ref-Skwark2013">81</a>]</span>. In a follow-up version <em>PSICOV</em> has been replaced with <em>gaussianDCA</em> and the sequence-based method <em>PhyCMap</em> <span class="citation">[<a href="#ref-Skwark2016">82</a>]</span>. <em>EPC-MAP</em> was published in 2014 integrating <em>GREMLIN</em> as a coevolution feature with physicochemical information from predicted ab initio protein structures <span class="citation">[<a href="#ref-Schneider2014">83</a>]</span>. In 2015, <em>MetaPSICOV</em> was released combining predictions from <em>PSICOV</em>, <em>mfDCA</em> and <em>CCMpred</em> with other sequence derived feautures <span class="citation">[<a href="#ref-Jones2015a">84</a>]</span>. <em>RaptorX</em> uses <em>CCMpred</em> as coevolution feature and other standard contact prediction features within an ultra-deep neural network <span class="citation">[<a href="#ref-Wang2016a">85</a>]</span>. The newest developments <em>EPSILON-CP</em> and <em>NeBcon</em> both comprise the most comprehensive usage of contact prediction methods so far, combining five and eight state-of-the-art contact predictors, respectively <span class="citation">[<a href="#ref-Stahl2017">86</a>,<a href="#ref-He2017">87</a>]</span>.</p>
<p>Another conceptual advancement besides the combination of sources of information is based on the fact that contacts are not randomly or independently distributed. DiLena and colleagues found that over 98% of long-range contacts (sequence separation &gt; 24 positions) are in close proximity of other contacts, compared to 30% for non-contacting pairs <span class="citation">[<a href="#ref-DiLena2012a">78</a>]</span>. The distribution of contacts is governed by local structural elements, like interactions between helices or <span class="math inline">\(\beta\)</span>-sheets, leading to characteristic patterns in the contact map that can be recognised <span class="citation">[<a href="#ref-Andreani2015a">88</a>]</span>. Deep learning provides the means to model higher level abstractions of data and several methods apply multi-layered algorithms to refine predictions by learning patterns that reflect the local neighborhood of a contact <span class="citation">[<a href="#ref-DiLena2012a">78</a>,<a href="#ref-Jones2015a">84</a>,<a href="#ref-Wang2016a">85</a>,<a href="#ref-Skwark2014a">89</a>]</span>.</p>
<p>Eventhough a benchmark comparing the recently developed meta-predictors is yet to be made, it becomes clear from the recent <a href="abbrev.html#abbrev">CASP</a> experiments, that meta-predictors outperform pure coevolution methods <span class="citation">[<a href="#ref-Monastyrskyy2015">90</a>]</span>. As coevolution scores comprise the most informative feautures among the set of input features, it is clear that meta-predictors will benefit from further improvements of pure coevolution methods <span class="citation">[<a href="#ref-Wang2016a">85</a>,<a href="#ref-Stahl2017">86</a>]</span>.</p>
</div>
<div id="maxent" class="section level3">
<h3><span class="header-section-number">1.2.4</span> Modelling Protein Families with Potts Model</h3>
<p>Infering contacts from a joint probability distribution over all residues in a protein sequence instead of using simple pairwise statistics has been proven to enable the distinction of direct statistical dependencies between residues from indirect dependencies mediated through other residues. The global statistical model that is commonly used to describe this joint probability distribution is the <em>Potts model</em>. It is a well-established model in statistical mechanics and can be derived from a maximum entropy assumption which is explained in the following.</p>
<p>The principle of maximum entropy, proposed by Jaynes in 1957 <span class="citation">[<a href="#ref-Jaynes1957a">91</a>,<a href="#ref-Jaynes1957b">92</a>]</span>, states that the probability distribution which makes minimal assumptions and best represents observed data is the one that is in agreement with measured constraints (prior information) and has the largest entropy. In other words, from all distributions that are consistent with measured data, the distribution with maximal entropy should be chosen.</p>
<p>A protein family is represented by a <a href="abbrev.html#abbrev">MSA</a> <span class="math inline">\(\X = \{ \seq_1, \ldots, \seq_N \}\)</span> of <span class="math inline">\(N\)</span> protein sequences. Every protein sequence of the protein family represents a sample drawn from a target distribution <span class="math inline">\(p(\seq)\)</span>, so that each protein sequence is associated with a probability. Each sequence <span class="math inline">\(\seq_n = (\seq_{n1}, ..., \seq_{nL})\)</span> is of length <span class="math inline">\(L\)</span> and every position constitutes a categorical variable <span class="math inline">\(x_{i}\)</span> that can take values from an alphabet indexed by <span class="math inline">\(\{0, ..., 20\}\)</span>, where 0 stands for a gap and <span class="math inline">\(\{1, ... , 20\}\)</span> stand for the 20 types of amino acids. The measured constraints are given by the empirically observed single and pairwise amino acid frequencies that can be calculated as</p>
<!--
Applied to the problem of modelling protein families, one seeks a probability distribution $p(\seq)$ for protein sequences $\seq = (x_1, \ldots, x_L)$ of length $L$ from the protein family under study. 
The categorical variables $x_{i}$ can take one of $q=21$ values representing the 20 naturally occuring amino acids and a gap ('-').
Given $N$ sequences of the protein family in a [MSA](#abbrev) with $\X = \{ \seq_1, \ldots, \seq_N \}$, the empirically observed single and pairwise amino acid frequencies can be calculated as
-->
<span class="math display">\[\begin{equation}
    f_i(a) = f(x_i\eq a) = \frac{1}{N}\sum_{n=1}^N I(x_{ni} \eq a) \; ,
\end{equation}\]</span>
<span class="math display" id="eq:emp-freq">\[\begin{equation}
    f_{ij}(a,b) = f(x_i\eq a, x_j\eq b) = \frac{1}{N} \sum_{n=1}^N I(x_{ni} \eq a, x_{nj} \eq b) \; .
 \tag{1.1}
\end{equation}\]</span>
<p>According to the maximum entropy principle, the distribution <span class="math inline">\(p(\seq)\)</span> should have maximal entropy and reproduce the empirically observed amino acid frequencies, so that</p>
<span class="math display" id="eq:maxent-reproducing-emp-freq">\[\begin{align}
   f(x_i\eq a)            &amp;\equiv p(x_i\eq a)  \nonumber\\
                                    &amp;= \sum_{\seq\prime_1, \ldots, \seq\prime_L = 1}^{q} p(x\prime) I(x\prime_i \eq a) \\
  f(x_i\eq a, x_j\eq b)   &amp;\equiv p(x_i\eq a, x_j \eq b) \nonumber \\
                                    &amp;= \sum_{\seq\prime_1, \ldots, \seq\prime_L = 1}^{q}  p(x\prime) I(x\prime_i\eq a, x\prime_j \eq b)  \; .
 \tag{1.2}
\end{align}\]</span>
<p>Solving for the distribution <span class="math inline">\(p(\seq)\)</span> that maximizes the Shannon entropy <span class="math inline">\(S= -\sum_{\seq\prime} p(\seq\prime) \log p(\seq\prime)\)</span> while satisfying the constraints given by the empircial amino acid frequencies in eq. <a href="introduction-to-contact-prediction.html#eq:maxent-reproducing-emp-freq">(1.2)</a> by introducing Lagrange multipliers <span class="math inline">\(\wij\)</span> and <span class="math inline">\(\vi\)</span>, results in the formulation of the <em>Potts model</em>,</p>
<!--
\begin{align}
F \left[ p(\seq) \right] =& -\sum_{\seq\prime} p(\seq\prime) \log p(\seq\prime) \\
        & + \sum_{i=1}^L \sum_{a=1}^{q} \vi(a) \left( p(x_i\eq a) - \mathcal{f}(x_i\eq a) \right) \\
        & + \sum_{1 \leq i < j \leq L}^L \; \sum_{a,b=1}^{q} \wij(a,b) \left( p(x_i\eq a, x_j \eq b) - \mathcal{f}(x_i\eq a, x_j\eq b) \right) \\
        & + \Omega \left( 1-\sum_{\seq\prime} p(\seq\prime)  \right)
(\#eq:derivation-max-ent-model)
\end{align}
-->
<span class="math display" id="eq:max-ent-model">\[\begin{equation}
    p(\seq | \v, \w ) = \frac{1}{Z(\v, \w)} \exp \left( \sum_{i=1}^L v_i(x_i) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_i, x_j) \right) \; .
\tag{1.3}
\end{equation}\]</span>
<p>The Lagrange multipliers <span class="math inline">\(\wij\)</span> and <span class="math inline">\(\vi\)</span> remain as model parameters to be fitted to data. <span class="math inline">\(Z\)</span> is a normalization constant also known as <em>partition function</em> that ensures the total probabilty adds up to one by summing over all possible assignments to <span class="math inline">\(\seq\)</span>,</p>
<span class="math display" id="eq:partition-fct-likelihood">\[\begin{equation}
  Z(\v, \w) = \sum_{\seq\prime_1, \ldots, \seq\prime_L = 1}^{q} \exp  \left( \sum_{i=1}^L v_i(x_i) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_i, x_j) \right) \; .
  \tag{1.4}
\end{equation}\]</span>
<div id="potts-model-properties" class="section level4">
<h4><span class="header-section-number">1.2.4.1</span> Model Properties</h4>
<p>The Potts model is specified by singlet terms <span class="math inline">\(\via\)</span> which describe the tendency for each amino acid a to appear at position <span class="math inline">\(i\)</span>, and pair terms <span class="math inline">\(\wijab\)</span>, also called couplings, which describe the tendency of amino acid a at position <span class="math inline">\(i\)</span> to co-occur with amino acid b at position <span class="math inline">\(j\)</span>. In contrast to mere correlations, the couplings explain the causative dependence structure between positions by jointly modelling the distribution of all positions in a protein sequence and thus account for transitive effects. By doing so, a major source of noise in contact prediction methods is eliminated.</p>
<p>To get some intuition for the coupling coefficients, note that <span class="math inline">\(\wijab = 1\)</span> corresponds to a 2.7-fold higher probability for a and b to occur together than what is expected from the singlet frequencies if a and b were independent. Pairs of residues that are not in contact tend to have negligable couplings, <span class="math inline">\(\wij \approx 0\)</span>, whereas pairs in contact tend to have vectors significantly different from 0. For contacting residues <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in real world <a href="abbrev.html#abbrev">MSAs</a> typical coupling strengths are on the order of <span class="math inline">\(||\wij || \approx 0.1\)</span> (regularization dependent).</p>
<p>Maximum entropy models naturally give rise to exponential family distributions that express useful properties for statistical modelling, such as the convexity of the likelihood function which consequently has a unique, global minimum <span class="citation">[<a href="#ref-Wainwright2007">93</a>,<a href="#ref-Murphy2012">94</a>]</span>.</p>
<p>The Potts model is a discrete instance of what is referred to as a pairwise <a href="abbrev.html#abbrev">Markov random field</a> in the statistics community. <a href="abbrev.html#abbrev">MRFs</a> belong to the class of undirected graphical models, that represent the probability distribution in terms of a graph with nodes and edges characterizing the variables and the dependence structure between variables, respectively.</p>
</div>
<div id="gauge-invariance" class="section level4">
<h4><span class="header-section-number">1.2.4.2</span> Gauge Invariance</h4>
<p>As every variable <span class="math inline">\(x_{ni}\)</span> can take <span class="math inline">\(q=21\)</span> values, the model has <span class="math inline">\(L \! \times \! q + L(L-1)/2 \! \times \! q^2\)</span> parameters. But the parameters are not uniquely determined and multiple parametrizations yield identical probability distributions.</p>
<p>For example, adding a constant to all elements in <span class="math inline">\(v_i\)</span> for any fixed position <span class="math inline">\(i\)</span> or similarly adding a constant to <span class="math inline">\(\via\)</span> for any fixed position <span class="math inline">\(i\)</span> and amino acid <span class="math inline">\(a\)</span> and subtracting the same constant from the <span class="math inline">\(qL\)</span> coefficients <span class="math inline">\(\wijab\)</span> with <span class="math inline">\(b \in \{1, \ldots, q\}\)</span> and <span class="math inline">\(j \in \{1, \ldots, L \}\)</span> leaves the probabilities for all sequences under the model unchanged, since such a change will be compensated by a change of <span class="math inline">\(Z(\v, \w)\)</span> in eq. <a href="introduction-to-contact-prediction.html#eq:partition-fct-likelihood">(1.4)</a>.</p>
<p>The overparametrization is referred to as <em>gauge invariance</em> in statistical physics literature and can be eliminated by removing parameters <span class="citation">[<a href="#ref-Weigt2009">38</a>,<a href="#ref-Morcos2011">95</a>]</span>. An appropriate choice of which parameters to remove, referred to as <em>gauge choice</em>, reduces the number of parameters to <span class="math inline">\(L \! \times \! (q-1) + L(L-1)/2 \! \times \! (q-1)^2\)</span>. Popular gauge choices are the <em>zero-sum gauge</em> or <em>Ising-gauge</em> used by Weigt et al. <span class="citation">[<a href="#ref-Weigt2009">38</a>]</span> imposed by the restraints,</p>
<span class="math display" id="eq:zero-sum-gauge">\[\begin{equation}
    \sum_{a=1}^{q} v_{ia} = \sum_{a=1}^{q} \wijab = \sum_{a=1}^{q} w_{ijba} = 0
\tag{1.5}
\end{equation}\]</span>
<p>for all <span class="math inline">\(i,j,b\)</span> or the <em>lattice-gas gauge</em> used by Morcos et al <span class="citation">[<a href="#ref-Morcos2011">95</a>]</span> and Marks et al <span class="citation">[<a href="#ref-Marks2011">39</a>]</span> imposed by restraints</p>
<span class="math display" id="eq:ising-gauge">\[\begin{equation}
    \wij(q,a) = \wij(a,q) = \vi(q) = 0
\tag{1.6}
\end{equation}\]</span>
<p>for all <span class="math inline">\(i,j,a\)</span> <span class="citation">[<a href="#ref-Cocco2017">96</a>]</span>.</p>
<p>Alternatively, the indeterminacy can be fixed by including a regularization prior (see next section). The regularizer selects for a unique solution among all parametrizations of the optimal distribution and therefore eliminates the need to choose a gauge <span class="citation">[<a href="#ref-Koller2009">97</a>–<a href="#ref-Stein2015a">99</a>]</span>.</p>
</div>
<div id="potts-mle" class="section level4">
<h4><span class="header-section-number">1.2.4.3</span> Inferring Parameters for the Potts Model</h4>
<p>Typically, parameter estimates are obtained by maximizing the log-likelihood function of the parameters over observed data. For the Potts model, the log-likelihood function is computed over sequences in the alignment <span class="math inline">\(\mathbf{X}\)</span>:</p>
<span class="math display" id="eq:full-log-likelihood">\[\begin{align}
    \text{LL}(\v, \w | \mathbf{X}) =&amp; \sum_{n=1}^N \log p(\seq_n)  \nonumber\\
    =&amp; \sum_{n=1}^N \left[ \sum_{i=1}^L v_i(x_{ni}) + \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_{xn}, x_{nj}) - \log Z \right]
\tag{1.7}
\end{align}\]</span>
<p>The number of parameters in a Potts model is typically larger than the number of observations, i.e. the number of sequences in the <a href="abbrev.html#abbrev">MSA</a>. Considering a protein of length <span class="math inline">\(L=100\)</span>, there are approximately <span class="math inline">\(2 \times 10^6\)</span> parameters in the model whereas the largest protein families comprise only around <span class="math inline">\(10^5\)</span> sequences (see Figure <a href="challenges.html#fig:pfam">1.11</a>). An underdetermined problem like this renders the use of regularizers neccessary in order to prevent overfitting.</p>
<p>Typically, an L2-regularization is used that pushes the single and pairwise terms smoothly towards zero and is equivalent to the logarithm of a zero-centered Gaussian prior,</p>
<span class="math display" id="eq:l2-reg">\[\begin{align}
  R(\v, \w)  &amp;= \log \left[ \mathcal{N}(\v | \mathbf{0}, \lambda_v^{-1} I) \mathcal{N}(\w | \mathbf{0}, \lambda_w^{-1} I) \right] \nonumber \\
             &amp;= -\frac{\lambda_v}{2} ||\v||_2^2 - \frac{\lambda_w}{2} ||\w||_2^2 + \text{const.} \; ,
\tag{1.8}
\end{align}\]</span>
<p>where the strength of regularization is tuned via the regularization coefficients <span class="math inline">\(\lambda_v\)</span> and <span class="math inline">\(\lambda_w\)</span> <span class="citation">[<a href="#ref-Seemayer2014">100</a>–<a href="#ref-Kamisetty2013">102</a>]</span>.</p>
<p>However, optimizing the log-likelihood requires computing the partition function <span class="math inline">\(Z\)</span> given in eq. <a href="introduction-to-contact-prediction.html#eq:partition-fct-likelihood">(1.4)</a> that sums <span class="math inline">\(q^L\)</span> terms. Computing this sum is intractable for realistic protein domains with more than 100 residues. Consequently, evaluating the likelihood function at each iteration of an optimization procedure is infeasible due to the exponential complexity of the partition function in protein length <span class="math inline">\(L\)</span>.</p>
<p>Many approximate inference techniques have been developed to sidestep the infeasible computation of the partition function for the specific problem of predicting contacts that are briefly explained in the next section.</p>
</div>
<div id="potts-model-solutions" class="section level4">
<h4><span class="header-section-number">1.2.4.4</span> Solving the Inverse Potts Problem</h4>
<p>In 1999 Lapedes et al. were the first to propose maximum entropy models for the prediction of residue-residue contacts in order to disentangle transitive effects <span class="citation">[<a href="#ref-Lapedes1999">62</a>]</span>. In 2002 they applied their idea to 11 small proteins using an iterative Monte Carlo procedure to obtain estimates of the model parameters and achieved an increase in accuracy of 10-20% compared to the local statistical models <span class="citation">[<a href="#ref-Lapedes2012a">103</a>]</span>. As the calculations involved were very time-consuming and at that time required supercomputing resources, the wider implications were not noted yet.</p>
<p>Ten years later Weight et al proposed an iterative message-passing algorithm, here referred to as <em>mpDCA</em>, to approximate the partition function <span class="citation">[<a href="#ref-Weigt2009">38</a>]</span>. Eventhough their approach is computationally very expensive and in practice only applicable to small proteins, they obtained remarkable results for the two-component signaling system in bacteria.</p>
<p>Balakrishnan et al were the first to apply pseudo-likelihood approximations to the full likelihood in 2011 <span class="citation">[<a href="#ref-Balakrishnan2011">104</a>]</span>. The pseudo-likelihood optimizes a different objective and replaces the global partition function <span class="math inline">\(Z\)</span> with local estimates. Balakrishnan and colleagues applied their method <em>GREMLIN</em> to learn sparse graphical models for 71 protein families. In a follow-up study in 2013, the authors proposed an improved version of <em>GREMLIN</em> that uses additional prior information <span class="citation">[<a href="#ref-Kamisetty2013">102</a>]</span>.</p>
<p>Also in 2011, Morcos et al. introduced a naive mean-field inversion approximation to the partition function, named <em>mfDCA</em> <span class="citation">[<a href="#ref-Morcos2011">95</a>]</span>. This method allows for drastically shorter running times as the mean-field approach boils down to inverting the empirical covariance matrix calculated from observed amino acid frequencies for each residue pair <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> of the alignment. This study performed the first high-throughput analysis of intradomain contacts for 131 protein families and facilitated the prediction of protein structures from accurately predicted contacts in <span class="citation">[<a href="#ref-Marks2011">39</a>]</span>.</p>
<p>The initial work by Balakrishnan and collegueas went almost unnoted as it was not primarily targeted to the problem of contact prediction. Ekeberg and collegueas independently developed the pseudo-likelihood method <em>plmDCA</em> in 2013 and showed its superior precision over <em>mfDCA</em> <span class="citation">[<a href="#ref-Ekeberg2013">98</a>]</span>.</p>
<p>A related approach to mean-field approximation is sparse inverse covariance estimation, named <em>PSICOV</em>, developed by Jones et al. (2012) <span class="citation">[<a href="#ref-Jones2012">65</a>]</span>. PSICOV uses an L1-regularization, known as graphical Lasso, to invert the correlation matrix and learn a sparse graphical model <span class="citation">[<a href="#ref-Friedman2008">105</a>]</span>. Both procedures, <em>mfDCA</em> and <em>PSICOV</em>, assume the model distribution to be a multivariate Gaussian. It has been shown by Banerjee et al. (2008)that this dual optimization solution also applies to binary data, as is the case in this application, where each position is encoded as a 20-dimensional binary vector <span class="citation">[<a href="#ref-Banerjee2008">106</a>]</span>.</p>
<p>Another related approach to <em>mfDCA</em> and <em>PSICOV</em> is <em>gaussianDCA</em>, proposed in 2014 by Baldassi et al. <span class="citation">[<a href="#ref-Baldassi2014">107</a>]</span>. Similar to the other both approaches, they model the data as multivariate Gaussian but within a simple Bayesian formalism by using a suitable prior and estimating parameters over the posterior distribution.</p>
<p>So far, pseudo-likelihood has proven to be the most successful approximation of the likelihood with respect to contact prediction performance. Currently, there exist several implementations of pseudo-likelihood maximization that vary in slight details, perform similarly and thus are equally popular in the community, such as CCMpred <span class="citation">[<a href="#ref-Seemayer2014">100</a>]</span>, plmDCA<span class="citation">[<a href="#ref-Ekeberg2014">101</a>]</span> and GREMLIN <span class="citation">[<a href="#ref-Kamisetty2013">102</a>]</span>.</p>
</div>
<div id="pseudo-likelihood" class="section level4">
<h4><span class="header-section-number">1.2.4.5</span> Maximum Likelihood Inference for Pseudo-Likelihood</h4>
<p>The pseudo-likelihood is a rather old estimation principle that was suggested by Besag already in 1975 <span class="citation">[<a href="#ref-Besag1975">108</a>]</span>. It represents a different objective function than the full likelihood and approximates the joint probability with the product over conditionals for each variable, i.e. the conditional probability of observing one variable given all the others:</p>
<span class="math display">\[\begin{align}
  p(\seq | \v,\w) \approx&amp;   \prod_{i=1}^L p(x_i | \seq_{\backslash xi}, \v,\w) \nonumber \\
                        =&amp;  \prod_{i=1}^L \frac{1}{Z_i} \exp \left(  v_i(x_i) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_i, x_j) \right)
\end{align}\]</span>
<p>Here, the normalization term <span class="math inline">\(Z_i\)</span> sums only over all assignments to one position <span class="math inline">\(i\)</span> in sequence:</p>
<span class="math display" id="eq:partition-fct-pll">\[\begin{equation}
  Z_i = \sum_{a=1}^{q} \exp \left( v_i(a) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(a, x_j) \right)
\tag{1.9}
\end{equation}\]</span>
<p>Replacing the global partition function in the full likelihood with local estimates of lower complexity in the pseudo-likelihood objective resolves the computational intractability of the parameter optimization procedure. Hence, it is feasible to maximize the pseudo-log-likelihood function,</p>
<span class="math display">\[\begin{align}
    \text{pLL}(\v, \w | \mathbf{X}) =&amp; \sum_{n=1}^N \sum_{i=1}^L \log p(x_i | \seq_{\backslash xi}, \v,\w) \nonumber \\
    =&amp; \sum_{n=1}^N \sum_{i=1}^L  \left[ v_i(x_{ni}) + \sum_{j=i+1}^L  w_{ij}(x_{ni}, x_{nj}) - \log Z_{ni} \right] \;,
\end{align}\]</span>
<p>plus an additional regularization term in order to prevent overfitting and to fix the gauge to arrive at a <a href="abbrev.html#abbrev">MAP</a> estimate of the parameters,</p>
<span class="math display">\[\begin{equation}
    \hat{\v}, \hat{\w} = \underset{\v, \w}{\operatorname{argmax}} \; \text{pLL}(\v, \w | \mathbf{X}) + R(\v, \w) \; .
\end{equation}\]</span>
<p>Eventhough the pseudo-likelihood optimizes a different objective than the full-likelihood, it has been found to work well in practice for many problems, including contact prediction <span class="citation">[<a href="#ref-Murphy2012">94</a>,<a href="#ref-Koller2009">97</a>–<a href="#ref-Stein2015a">99</a>]</span>. The pseudo-likelihood function retains the concavity of the likelihood and it has been proven to be a consistent estimator in the limit of infinite data for models of the exponential family <span class="citation">[<a href="#ref-Koller2009">97</a>,<a href="#ref-Besag1975">108</a>,<a href="#ref-Gidas1988">109</a>]</span>. That is, as the number of sequences in the alignment increases, pseudo-likelihood estimates converge towards the true full likelihood parameters.</p>
</div>
<div id="post-processing-heuristics" class="section level4">
<h4><span class="header-section-number">1.2.4.6</span> Computing Contact Maps</h4>
<p>Model inference as described in the last section yields <a href="abbrev.html#abbrev">MAP</a> estimates of the couplings <span class="math inline">\(\hat{\w}_{ij}\)</span>. In order to obtain a scalar measure for the coupling strength between two residues <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, all available methods presented in section <a href="introduction-to-contact-prediction.html#potts-model-solutions">1.2.4.4</a> heuristically map the <span class="math inline">\(21 \! \times \! 21\)</span> dimensional coupling matrix <span class="math inline">\(\wij\)</span> to a single scalar quantity.</p>
<p><em>mpDCA</em> <span class="citation">[<a href="#ref-Weigt2009">38</a>]</span> and <em>mfDCA</em> <span class="citation">[<a href="#ref-Marks2011">39</a>,<a href="#ref-Morcos2011">95</a>]</span> employ a score called <a href="abbrev.html#abbrev">DI</a>, that essentially computes the <a href="abbrev.html#abbrev">MI</a> for two positions <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> using the couplings <span class="math inline">\(\wij\)</span> instead of pairwise amino acid frequencies. Most pseudo-likelihood methods (<em>plmDCA</em> <span class="citation">[<a href="#ref-Ekeberg2013">98</a>,<a href="#ref-Ekeberg2014">101</a>]</span>, <em>CCMpred</em> <span class="citation">[<a href="#ref-Seemayer2014">100</a>]</span>, <em>GREMLIN</em> <span class="citation">[<a href="#ref-Kamisetty2013">102</a>]</span>) compute the <em>Frobenius norm</em> of the coupling matrix <span class="math inline">\(\wij\)</span> to obtain a scalar contact score <span class="math inline">\(C_{ij}\)</span>,</p>
<span class="math display" id="eq:frobenius-norm">\[\begin{equation}
    C_{ij}  = ||\wij||_2 = \sqrt{\sum_{a,b=1}^q \wijab^2} \; .
\tag{1.10}
\end{equation}\]</span>
<p>The Frobenius norm improves prediction performance over <a href="abbrev.html#abbrev">DI</a> and further improvements can be obtained by computing the Frobenius norm only on the <span class="math inline">\(20 \times 20\)</span> submatrix thus ignoring contributions from gaps <span class="citation">[<a href="#ref-Ekeberg2013">98</a>,<a href="#ref-Baldassi2014">107</a>,<a href="#ref-Feinauer2014">110</a>]</span>. <em>PSICOV</em> <span class="citation">[<a href="#ref-Jones2012">65</a>]</span> uses an L1-norm on the <span class="math inline">\(20 \times 20\)</span> submatrix instead of the Frobenius norm.</p>
<p>Furthermore it should be noted that the Frobenius norm is gauge dependent and is minimized by the <em>zero-sum gauge</em> <span class="citation">[<a href="#ref-Weigt2009">38</a>]</span>. Therefore, the coupling matrices should be transformed to <em>zero-sum gauge</em> before computing the Frobenius norm</p>
<span class="math display" id="eq:zero-sum-gauge-transform">\[\begin{equation}
    \w^{\prime}_{ij}  = \wij - \wij(\cdot, b) - \wij(a, \cdot) + \wij(\cdot, \cdot) \; ,
\tag{1.11}
\end{equation}\]</span>
<p>where <span class="math inline">\(\cdot\)</span> denotes average over the respective indices <span class="citation">[<a href="#ref-Ekeberg2013">98</a>,<a href="#ref-Seemayer2014">100</a>,<a href="#ref-Ekeberg2014">101</a>,<a href="#ref-Baldassi2014">107</a>]</span>.</p>
<p>Another commonly applied heuristic known as <a href="abbrev.html#abbrev">APC</a> has been introduced by Dunn et al. in order to reduce background noise arising from correlations between positions with high entropy or phylogenetic couplings <span class="citation">[<a href="#ref-Dunn2008">59</a>]</span>. <a href="abbrev.html#abbrev">APC</a> is a correction term that is computed from the raw contact map as the product over average row and column contact scores <span class="math inline">\(\overline{C_i}\)</span> divided by the average contact score over all pairs <span class="math inline">\(\overline{C_{ij}}\)</span>. The corrected contact score <span class="math inline">\(C_{ij}^{APC}\)</span> is obtained by subtracting the <a href="abbrev.html#abbrev">APC</a> term from the raw contact score <span class="math inline">\(C_{ij}\)</span>,</p>
<span class="math display" id="eq:apc">\[\begin{equation}
    C_{ij}^{APC}  = C_{ij} - \frac{\overline{C_i} \; \overline{C_j}}{\overline{C_{ij}}}\; .
\tag{1.12}
\end{equation}\]</span>
<p>Visually, <a href="abbrev.html#abbrev">APC</a> creates a <em>smoothing</em> effect on the contact maps that is illustrated in Figure <a href="introduction-to-contact-prediction.html#fig:apc-correction">1.5</a> and it has been found to substantially boost contact prediction performance <span class="citation">[<a href="#ref-Dunn2008">59</a>,<a href="#ref-Kamisetty2013">102</a>]</span>. It was first adopted by <em>PSICOV</em> <span class="citation">[<a href="#ref-Jones2012">65</a>]</span> but is now used by most methods to adjust raw contact scores.</p>
<p>It was long under debate why <a href="abbrev.html#abbrev">APC</a> works so well and how it can be interpreted. Zhang et al. showed that <a href="abbrev.html#abbrev">APC</a> essentially approximates the first principal component of the contact matrix and therefore removes the highest variability in the matrix that is assumed to arise from background biases <span class="citation">[<a href="#ref-Zhang2016">111</a>]</span>. Furthermore, they studied an advanced decomposition technique, called LRS matrix decomposition, that decomposes the contact matrix into a low-rank and a sparse component, representing background noise and true correlations, respectively.<br />
Inferring contacts from the sparse component works astonishing well, improving precision further over <a href="abbrev.html#abbrev">APC</a> independent of the underlying statistical model.</p>
<p>Dr Stefan Seemayer could show that the main component of background noise can be attributed to entropic effects and that a substantial part of <a href="abbrev.html#abbrev">APC</a> amounts to correcting for these entropic biases (unpublished). In his doctoral thesis, he developed an entropy correction, computed as the geometric mean of per-column entropies, that correlates well with the <a href="abbrev.html#abbrev">APC</a> correction term and yields similar precision for predicted contacts. The entropy correction has the advantage that it is computed from input statistics and therefore is independent of the statistical model used to infer the couplings. In contrast, <a href="abbrev.html#abbrev">APC</a> and other denoising techniques such as LRS <span class="citation">[<a href="#ref-Zhang2016">111</a>]</span> discussed above, estimate a background model from the final contact matrix, thus depending on the statistical model used to infer the contact matrix.</p>

<div class="figure" style="text-align: center"><span id="fig:apc-correction"></span>
<img src="img/intro/apc_correction_with_entropy.png" alt="Contact maps computed from pseudo-likelihood couplings. Subplot on top of the contact maps illustrates the normalized Shannon entropy (pink line) and percentage of gaps for every position in the alignment (brown line). Left: Contact map computed with Frobenius norm as in eq. (1.10). Overall coupling values are dominated by entropic effects, i.e. the amount of variation for a MSA position, leading to striped brightness patterns. For example, positions with high column entropy (e.g. positions 7, 12 or 31) have higher overall coupling values than positions with low column entropy (e.g. positions 11, 24 or 33). b: previous contact map but corrected for background noise with the APC as in eq. (1.12)." width="90%" />
<p class="caption">
Figure 1.5: Contact maps computed from pseudo-likelihood couplings. Subplot on top of the contact maps illustrates the normalized Shannon entropy (<span style="color:#e7539d;">pink </span> line) and percentage of gaps for every position in the alignment (<span style="color:brown;">brown </span> line). <strong>Left</strong>: Contact map computed with Frobenius norm as in eq. <a href="introduction-to-contact-prediction.html#eq:frobenius-norm">(1.10)</a>. Overall coupling values are dominated by entropic effects, i.e. the amount of variation for a <a href="abbrev.html#abbrev">MSA</a> position, leading to striped brightness patterns. For example, positions with high column entropy (e.g. positions 7, 12 or 31) have higher overall coupling values than positions with low column entropy (e.g. positions 11, 24 or 33). <strong>b</strong>: previous contact map but corrected for background noise with the <a href="abbrev.html#abbrev">APC</a> as in eq. <a href="introduction-to-contact-prediction.html#eq:apc">(1.12)</a>.
</p>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Gobel1994">
<p>46. Göbel, U., Sander, C., Schneider, R., and Valencia, A. (1994). Correlated mutations and residue contacts in proteins. Proteins <em>18</em>, 309–317., doi: <a href="https://doi.org/10.1002/prot.340180402">10.1002/prot.340180402</a>.</p>
</div>
<div id="ref-Godzik1989">
<p>47. Godzik, A., and Sander, C. (1989). Conservation of residue interactions in a family of Ca-binding proteins. &quot;Protein Eng. Des. Sel. <em>2</em>, 589–596., doi: <a href="https://doi.org/10.1093/protein/2.8.589">10.1093/protein/2.8.589</a>.</p>
</div>
<div id="ref-Neher1994">
<p>48. Neher, E. (1994). How frequent are correlated changes in families of protein sequences? Proc. Natl. Acad. Sci. U. S. A. <em>91</em>, 98–102.</p>
</div>
<div id="ref-Shindyalov1994">
<p>51. Shindyalov, I., Kolchanov, N., and Sander, C. (1994). Can three-dimensional contacts in protein structures be predicted by analysis of correlated mutations? &quot;Protein Eng. Des. Sel. <em>7</em>, 349–358., doi: <a href="https://doi.org/10.1093/protein/7.3.349">10.1093/protein/7.3.349</a>.</p>
</div>
<div id="ref-Clarke1995">
<p>52. Clarke, N.D. (1995). Covariation of residues in the homeodomain sequence family. Protein Sci. <em>4</em>, 2269–78., doi: <a href="https://doi.org/10.1002/pro.5560041104">10.1002/pro.5560041104</a>.</p>
</div>
<div id="ref-Martin2005">
<p>54. Martin, L.C., Gloor, G.B., Dunn, S.D., and Wahl, L.M. (2005). Using information theory to search for co-evolving residues in proteins. Bioinformatics <em>21</em>, 4116–24., doi: <a href="https://doi.org/10.1093/bioinformatics/bti671">10.1093/bioinformatics/bti671</a>.</p>
</div>
<div id="ref-Fodor2004">
<p>56. Fodor, A.A., and Aldrich, R.W. (2004). Influence of conservation on calculations of amino acid covariance in multiple sequence alignments. Proteins <em>56</em>, 211–21.</p>
</div>
<div id="ref-Atchley2000">
<p>55. Atchley, W.R., Wollenberg, K.R., Fitch, W.M., Terhalle, W., and Dress, A.W. (2000). Correlations Among Amino Acid Sites in bHLH Protein Domains: An Information Theoretic Analysis. Mol. Biol. Evol. <em>17</em>, 164–178., doi: <a href="https://doi.org/10.1093/oxfordjournals.molbev.a026229">10.1093/oxfordjournals.molbev.a026229</a>.</p>
</div>
<div id="ref-Tillier2003">
<p>57. Tillier, E.R., and Lui, T.W. (2003). Using multiple interdependency to separate functional from phylogenetic correlations in protein alignments. Bioinformatics <em>19</em>, 750–755., doi: <a href="https://doi.org/10.1093/bioinformatics/btg072">10.1093/bioinformatics/btg072</a>.</p>
</div>
<div id="ref-Gouveia_Oliveira2007">
<p>58. Gouveia-Oliveira, R., and Pedersen, A.G. (2007). Finding coevolving amino acid residues using row and column weighting of mutual information and multi-dimensional amino acid representation. Algorithms Mol. Biol. <em>2</em>, 12., doi: <a href="https://doi.org/10.1186/1748-7188-2-12">10.1186/1748-7188-2-12</a>.</p>
</div>
<div id="ref-Dunn2008">
<p>59. Dunn, S.D., Wahl, L.M., and Gloor, G.B. (2008). Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction. Bioinformatics <em>24</em>, 333–40., doi: <a href="https://doi.org/10.1093/bioinformatics/btm604">10.1093/bioinformatics/btm604</a>.</p>
</div>
<div id="ref-Kass2002">
<p>60. Kass, I., and Horovitz, A. (2002). Mapping pathways of allosteric communication in GroEL by analysis of correlated mutations. Proteins <em>48</em>, 611–7., doi: <a href="https://doi.org/10.1002/prot.10180">10.1002/prot.10180</a>.</p>
</div>
<div id="ref-Noivirt2005">
<p>61. Noivirt, O., Eisenstein, M., and Horovitz, A. (2005). Detection and reduction of evolutionary noise in correlated mutation analysis. Protein Eng. Des. Sel. <em>18</em>, 247–53., doi: <a href="https://doi.org/10.1093/protein/gzi029">10.1093/protein/gzi029</a>.</p>
</div>
<div id="ref-Weigt2009">
<p>38. Weigt, M., White, R.A., Szurmant, H., Hoch, J.A., and Hwa, T. (2009). Identification of direct residue contacts in protein-protein interaction by message passing. Proc. Natl. Acad. Sci. U. S. A. <em>106</em>, 67–72., doi: <a href="https://doi.org/10.1073/pnas.0805923106">10.1073/pnas.0805923106</a>.</p>
</div>
<div id="ref-Lapedes1999">
<p>62. Lapedes, A., Giraud, B., Liu, L., and Stormo, G. (1999). Correlated mutations in models of protein sequences: phylogenetic and structural effects. <em>33</em>, 236–256.</p>
</div>
<div id="ref-Burger2010">
<p>63. Burger, L., and Nimwegen, E. van (2010). Disentangling direct from indirect co-evolution of residues in protein alignments. PLoS Comput. Biol. <em>6</em>, e1000633., doi: <a href="https://doi.org/10.1371/journal.pcbi.1000633">10.1371/journal.pcbi.1000633</a>.</p>
</div>
<div id="ref-DeJuan2013">
<p>64. Juan, D. de, Pazos, F., and Valencia, A. (2013). Emerging methods in protein co-evolution. Nat. Rev. Genet. <em>14</em>, 249–61., doi: <a href="https://doi.org/10.1038/nrg3414">10.1038/nrg3414</a>.</p>
</div>
<div id="ref-Jones2012">
<p>65. Jones, D.T., Buchan, D.W.A., Cozzetto, D., and Pontil, M. (2012). PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments. Bioinformatics <em>28</em>, 184–90., doi: <a href="https://doi.org/10.1093/bioinformatics/btr638">10.1093/bioinformatics/btr638</a>.</p>
</div>
<div id="ref-Burger2008">
<p>66. Burger, L., and Nimwegen, E. van (2008). Accurate prediction of protein-protein interactions from sequence alignments using a Bayesian method. Mol. Syst. Biol. <em>4</em>, 165., doi: <a href="https://doi.org/10.1038/msb4100203">10.1038/msb4100203</a>.</p>
</div>
<div id="ref-Cheng2007">
<p>67. Cheng, J., and Baldi, P. (2007). Improved residue contact prediction using support vector machines and a large feature set. BMC Bioinformatics <em>8</em>., doi: <a href="https://doi.org/10.1186/1471-2105-8-113">10.1186/1471-2105-8-113</a>.</p>
</div>
<div id="ref-Wu2008">
<p>68. Wu, S., and Zhang, Y. (2008). A comprehensive assessment of sequence-based and template-based methods for protein contact prediction. Bioinformatics <em>24</em>, 924–31.</p>
</div>
<div id="ref-Li2011">
<p>69. Li, Y., Fang, Y., and Fang, J. (2011). Predicting residue-residue contacts using random forest models. Bioinformatics <em>27</em>., doi: <a href="https://doi.org/10.1093/bioinformatics/btr579">10.1093/bioinformatics/btr579</a>.</p>
</div>
<div id="ref-Wang2011">
<p>70. Wang, X.-F., Chen, Z., Wang, C., Yan, R.-X., Zhang, Z., and Song, J. (2011). Predicting residue-residue contacts and helix-helix interactions in transmembrane proteins using an integrative feature-based random forest approach. PLoS One <em>6</em>, e26767., doi: <a href="https://doi.org/10.1371/journal.pone.0026767">10.1371/journal.pone.0026767</a>.</p>
</div>
<div id="ref-Wang2013">
<p>71. Wang, Z., and Xu, J. (2013). Predicting protein contact map using evolutionary and physical constraints by integer programming. Bioinformatics <em>29</em>, i266–73.</p>
</div>
<div id="ref-Fariselli2001a">
<p>72. Fariselli, P., Olmea, O., Valencia, A., and Casadio, R. (2001). Prediction of contact maps with neural networks and correlated mutations. Protein Eng. Des. Sel. <em>14</em>, 835–843.</p>
</div>
<div id="ref-Shackelford2007">
<p>73. Shackelford, G., and Karplus, K. (2007). Contact prediction using mutual information and neural nets. Proteins <em>69 Suppl 8</em>, 159–64., doi: <a href="https://doi.org/10.1002/prot.21791">10.1002/prot.21791</a>.</p>
</div>
<div id="ref-Hamilton2004a">
<p>74. Hamilton, N., Burrage, K., Ragan, M.A., and Huber, T. (2004). Protein contact prediction using patterns of correlation. Proteins Struct. Funct. Bioinforma. <em>56</em>, 679–684., doi: <a href="https://doi.org/10.1002/PROT.20160">10.1002/PROT.20160</a>.</p>
</div>
<div id="ref-Xue2009a">
<p>75. Xue, B., Faraggi, E., and Zhou, Y. (2009). Predicting residue-residue contact maps by a two-layer, integrated neural-network method. Proteins <em>76</em>, 176–83.</p>
</div>
<div id="ref-Tegge2009a">
<p>76. Tegge, A.N., Wang, Z., Eickholt, J., and Cheng, J. (2009). NNcon: improved protein contact map prediction using 2D-recursive neural networks. Nucleic Acids Res. <em>37</em>, W515–8.</p>
</div>
<div id="ref-Eickholt2012">
<p>77. Eickholt, J., and Cheng, J. (2012). Predicting protein residue–residue contacts using deep networks and boosting. <em>28</em>, 3066–3072., doi: <a href="https://doi.org/10.1093/bioinformatics/bts598">10.1093/bioinformatics/bts598</a>.</p>
</div>
<div id="ref-DiLena2012a">
<p>78. Di Lena, P., Nagata, K., and Baldi, P. (2012). Deep architectures for protein contact map prediction. Bioinformatics <em>28</em>, 2449–57.</p>
</div>
<div id="ref-Chen2010">
<p>79. Chen, P., and Li, J. (2010). Prediction of protein long-range contacts using an ensemble of genetic algorithm classifiers with sequence profile centers. BMC Struct. Biol. <em>10 Suppl 1</em>, S2., doi: <a href="https://doi.org/10.1186/1472-6807-10-S1-S2">10.1186/1472-6807-10-S1-S2</a>.</p>
</div>
<div id="ref-Jones2015">
<p>80. Jones, D.T., Singh, T., Kosciolek, T., and Tetchner, S. (2015). MetaPSICOV: combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins. Bioinformatics <em>31</em>, 999–1006., doi: <a href="https://doi.org/10.1093/bioinformatics/btu791">10.1093/bioinformatics/btu791</a>.</p>
</div>
<div id="ref-Skwark2013">
<p>81. Skwark, M.J., Abdel-Rehim, A., and Elofsson, A. (2013). PconsC: combination of direct information methods and alignments improves contact prediction. Bioinformatics <em>29</em>, 1815–6.</p>
</div>
<div id="ref-Skwark2016">
<p>82. Skwark, M.J., Michel, M., Menendez Hurtado, D., Ekeberg, M., and Elofsson, A. (2016). Accurate contact predictions for thousands of protein families using PconsC3. bioRxiv.</p>
</div>
<div id="ref-Schneider2014">
<p>83. Schneider, M., and Brock, O. (2014). Combining Physicochemical and Evolutionary Information for Protein Contact Prediction. PLoS One <em>9</em>, e108438.</p>
</div>
<div id="ref-Jones2015a">
<p>84. Jones, D.T., Singh, T., Kosciolek, T., and Tetchner, S. (2015). MetaPSICOV: combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins. Bioinformatics <em>31</em>, 999–1006.</p>
</div>
<div id="ref-Wang2016a">
<p>85. Wang, S., Sun, S., Li, Z., Zhang, R., and Xu, J. (2016). Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model. PLoS Comput. Biol. <em>13</em>, e1005324., doi: <a href="https://doi.org/10.1371/journal.pcbi.1005324">10.1371/journal.pcbi.1005324</a>.</p>
</div>
<div id="ref-Stahl2017">
<p>86. Stahl, K., Schneider, M., and Brock, O. (2017). EPSILON-CP: using deep learning to combine information from multiple sources for protein contact prediction. BMC Bioinformatics <em>18</em>, 303., doi: <a href="https://doi.org/10.1186/s12859-017-1713-x">10.1186/s12859-017-1713-x</a>.</p>
</div>
<div id="ref-He2017">
<p>87. He, B., Mortuza, S.M., Wang, Y., Shen, H.-B., and Zhang, Y. (2017). NeBcon: Protein contact map prediction using neural network training coupled with naïve Bayes classifiers. Bioinformatics., doi: <a href="https://doi.org/10.1093/bioinformatics/btx164">10.1093/bioinformatics/btx164</a>.</p>
</div>
<div id="ref-Andreani2015a">
<p>88. Andreani, J., and Söding, J. (2015). Bbcontacts: Prediction of $$-strand pairing from direct coupling patterns. Bioinformatics <em>31</em>, 1729–1737.</p>
</div>
<div id="ref-Skwark2014a">
<p>89. Skwark, M.J., Raimondi, D., Michel, M., and Elofsson, A. (2014). Improved contact predictions using the recognition of protein like contact patterns. PLoS Comput. Biol. <em>10</em>, e1003889.</p>
</div>
<div id="ref-Monastyrskyy2015">
<p>90. Monastyrskyy, B., D’Andrea, D., Fidelis, K., Tramontano, A., and Kryshtafovych, A. (2015). New encouraging developments in contact prediction: Assessment of the CASP11 results. Proteins., doi: <a href="https://doi.org/10.1002/prot.24943">10.1002/prot.24943</a>.</p>
</div>
<div id="ref-Jaynes1957a">
<p>91. Jaynes, E.T. (1957). Information Theory and Statistical Mechanics I. Phys. Rev. <em>106</em>, 620–630., doi: <a href="https://doi.org/10.1103/PhysRev.106.620">10.1103/PhysRev.106.620</a>.</p>
</div>
<div id="ref-Jaynes1957b">
<p>92. Jaynes, E.T. (1957). Information Theory and Statistical Mechanics. II. Phys. Rev. <em>108</em>, 171–190., doi: <a href="https://doi.org/10.1103/PhysRev.108.171">10.1103/PhysRev.108.171</a>.</p>
</div>
<div id="ref-Wainwright2007">
<p>93. Wainwright, M.J., and Jordan, M.I. (2007). Graphical Models, Exponential Families, and Variational Inference. Found. Trends Mach. Learn. <em>1</em>, 1–305., doi: <a href="https://doi.org/10.1561/2200000001">10.1561/2200000001</a>.</p>
</div>
<div id="ref-Murphy2012">
<p>94. Murphy, K.P. (2012). Machine Learning: A Probabilistic Perspective (MIT Press).</p>
</div>
<div id="ref-Morcos2011">
<p>95. Morcos, F., Pagnani, A., Lunt, B., Bertolino, A., Marks, D.S., Sander, C., Zecchina, R., Onuchic, J.N., Hwa, T., and Weigt, M. (2011). Direct-coupling analysis of residue coevolution captures native contacts across many protein families. Proc. Natl. Acad. Sci. U. S. A. <em>108</em>, E1293–301., doi: <a href="https://doi.org/10.1073/pnas.1111471108">10.1073/pnas.1111471108</a>.</p>
</div>
<div id="ref-Marks2011">
<p>39. Marks, D.S., Colwell, L.J., Sheridan, R., Hopf, T.A., Pagnani, A., Zecchina, R., and Sander, C. (2011). Protein 3D structure computed from evolutionary sequence variation. PLoS One <em>6</em>, e28766., doi: <a href="https://doi.org/10.1371/journal.pone.0028766">10.1371/journal.pone.0028766</a>.</p>
</div>
<div id="ref-Cocco2017">
<p>96. Cocco, S., Feinauer, C., Figliuzzi, M., Monasson, R., and Weigt, M. (2017). Inverse Statistical Physics of Protein Sequences: A Key Issues Review. arXiv.</p>
</div>
<div id="ref-Koller2009">
<p>97. Koller, D., and Friedman, N.I.R. (2009). Probabilistic graphical models: Principles and Techniques (MIT Press).</p>
</div>
<div id="ref-Stein2015a">
<p>99. Stein, R.R., Marks, D.S., and Sander, C. (2015). Inferring Pairwise Interactions from Biological Data Using Maximum-Entropy Probability Models. PLOS Comput. Biol. <em>11</em>, e1004182.</p>
</div>
<div id="ref-Seemayer2014">
<p>100. Seemayer, S., Gruber, M., and Söding, J. (2014). CCMpred-fast and precise prediction of protein residue-residue contacts from correlated mutations. Bioinformatics, btu500.</p>
</div>
<div id="ref-Kamisetty2013">
<p>102. Kamisetty, H., Ovchinnikov, S., and Baker, D. (2013). Assessing the utility of coevolution-based residue-residue contact predictions in a sequence- and structure-rich era. Proc. Natl. Acad. Sci. U. S. A. <em>110</em>, 15674–9., doi: <a href="https://doi.org/10.1073/pnas.1314045110">10.1073/pnas.1314045110</a>.</p>
</div>
<div id="ref-Lapedes2012a">
<p>103. Lapedes, A., Giraud, B., and Jarzynski, C. (2012). Using Sequence Alignments to Predict Protein Structure and Stability With High Accuracy.</p>
</div>
<div id="ref-Balakrishnan2011">
<p>104. Balakrishnan, S., Kamisetty, H., Carbonell, J.G., Lee, S.-I., and Langmead, C.J. (2011). Learning generative models for protein fold families. Proteins <em>79</em>, 1061–78., doi: <a href="https://doi.org/10.1002/prot.22934">10.1002/prot.22934</a>.</p>
</div>
<div id="ref-Ekeberg2013">
<p>98. Ekeberg, M., Lövkvist, C., Lan, Y., Weigt, M., and Aurell, E. (2013). Improved contact prediction in proteins: Using pseudolikelihoods to infer Potts models. Phys. Rev. E <em>87</em>, 012707., doi: <a href="https://doi.org/10.1103/PhysRevE.87.012707">10.1103/PhysRevE.87.012707</a>.</p>
</div>
<div id="ref-Friedman2008">
<p>105. Friedman, J., Hastie, T., and Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics <em>9</em>, 432–41., doi: <a href="https://doi.org/10.1093/biostatistics/kxm045">10.1093/biostatistics/kxm045</a>.</p>
</div>
<div id="ref-Banerjee2008">
<p>106. Banerjee, O., El Ghaoui, L., and D’Aspremont, A. (2008). Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data. J. Mach. Learn. Res. <em>9</em>, 485–516.</p>
</div>
<div id="ref-Baldassi2014">
<p>107. Baldassi, C., Zamparo, M., Feinauer, C., Procaccini, A., Zecchina, R., Weigt, M., and Pagnani, A. (2014). Fast and accurate multivariate gaussian modeling of protein families: predicting residue contacts and protein-interaction partners. PLoS One <em>9</em>, e92721., doi: <a href="https://doi.org/10.1371/journal.pone.0092721">10.1371/journal.pone.0092721</a>.</p>
</div>
<div id="ref-Ekeberg2014">
<p>101. Ekeberg, M., Hartonen, T., and Aurell, E. (2014). Fast pseudolikelihood maximization for direct-coupling analysis of protein structure from many homologous amino-acid sequences. J. Comput. Phys. <em>276</em>, 341–356.</p>
</div>
<div id="ref-Besag1975">
<p>108. Besag, J. (1975). Statistical Analysis of Non-Lattice Data. Source Stat. <em>24</em>, 179–195.</p>
</div>
<div id="ref-Gidas1988">
<p>109. Gidas, B. (1988). Consistency of maximum likelihood and pseudo-likelihood estimators for Gibbs Distributions. Stoch. Differ. Syst. Stoch. Control Theory Appl.</p>
</div>
<div id="ref-Feinauer2014">
<p>110. Feinauer, C., Skwark, M.J., Pagnani, A., and Aurell, E. (2014). Improving contact prediction along three dimensions. 19.</p>
</div>
<div id="ref-Zhang2016">
<p>111. Zhang, H., Huang, Q., Bei, Z., Wei, Y., and Floudas, C.A. (2016). COMSAT: Residue contact prediction of transmembrane proteins based on support vector machines and mixed integer linear programming. Proteins Struct. Funct. Bioinforma., n/a–n/a., doi: <a href="https://doi.org/10.1002/prot.24979">10.1002/prot.24979</a>.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://contactpredictionthesis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="general-intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="application-contact-prediction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/susannvorberg/phd_thesis/edit/master/04-intro-contact-prediction.Rmd",
"text": "Edit"
},
"download": ["PhD_thesis_Susann_Vorberg.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
