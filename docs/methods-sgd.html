<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>PhD thesis: residue-residue contact prediction</title>
  <meta name="description" content="This is my PhD thesis on residue-residue contact prediction.">
  <meta name="generator" content="bookdown 0.3.9 and GitBook 2.6.7">

  <meta property="og:title" content="PhD thesis: residue-residue contact prediction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my PhD thesis on residue-residue contact prediction." />
  <meta name="github-repo" content="susannvorberg/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PhD thesis: residue-residue contact prediction" />
  
  <meta name="twitter:description" content="This is my PhD thesis on residue-residue contact prediction." />
  

<meta name="author" content="Susann Vorberg">


<meta name="date" content="2017-10-16">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="analysis-of-coupling-matrices.html">
<link rel="next" href="regularization-for-cd-with-sgd.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
  tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
  TeX: { 
    extensions: ["mediawiki-texvc.js", "sinuitx.js"],
    Macros: {
      Cb: "C_\\beta",
      eq: "\\!=\\!",
      Gauss: "\\mathcal{N}",
      H: "\\mathbf{H}",
      Hij : "\\H_{ij}",
      I: "\\mathbf{I}",
      Lijk: "\\mathbf{\\Lambda}_{ij,k}",
      Lk: "\\mathbf{\\Lambda}_k",
      LL: "L\\!L(\\mathbf{v}, \\mathbf{w})",
      LLreg: "L\\!L_\\mathrm{reg}",
      muijk: "\\mathbf{\\mu}_{ij,k}",
      muk: "\\mathbf{\\mu}_k",
      neff: "N_\\mathrm{eff}",
      r: "\\mathbf{r}",
      rij: "r_{ij}",
      seq: "\\mathbf{x}",
      Qij: "\\mathbf{Q}_{ij}",
      q: "\\mathbf{q}",
      qij: "\\mathbf{q'}_{ij}",
      Sn: "\\mathcal{S}_n",
      v: "\\mathbf{v}",
      vi: "\\mathcal{v}_{i}",
      vj: "\\mathcal{v}_{j}",
      via: "\\mathcal{v}_{ia}",
      vja: "\\mathcal{v}_{ja}",
      w: "\\mathbf{w}",
      wij: "\\mathbf{w}_{ij}",
      wijab: "\\mathcal{w}_{ijab}",
      wijcd: "\\mathcal{w}_{ijcd}",
      wklcd: "\\mathcal{w}_{klcd}",
      X: "\\mathbf{X}",
      angstrom: "\\AA \\; \\;"
      }
  }
});
</script>


 
<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js", "[siunitx]/siunitx.js"],
  TeX: { TagSide: "left" }
});
MathJax.Ajax.config.path['siunitx']  = '../latex/MathJax-siunitx-master/';
</script>
//-->



<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD Thesis Susann Vorberg</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="general-intro.html"><a href="general-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="protein-structure.html"><a href="protein-structure.html"><i class="fa fa-check"></i><b>1.1</b> Protein Structure</a><ul>
<li class="chapter" data-level="1.1.1" data-path="protein-structure.html"><a href="protein-structure.html#amino-acid-interactions"><i class="fa fa-check"></i><b>1.1.1</b> Amino Acid Interactions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="structure-prediction.html"><a href="structure-prediction.html"><i class="fa fa-check"></i><b>1.2</b> Structure Prediction</a><ul>
<li class="chapter" data-level="1.2.1" data-path="structure-prediction.html"><a href="structure-prediction.html#template-based-methods"><i class="fa fa-check"></i><b>1.2.1</b> Template-based methods</a></li>
<li class="chapter" data-level="1.2.2" data-path="structure-prediction.html"><a href="structure-prediction.html#template-free-structure-prediction"><i class="fa fa-check"></i><b>1.2.2</b> Template-free structure prediction</a></li>
<li class="chapter" data-level="1.2.3" data-path="structure-prediction.html"><a href="structure-prediction.html#contact-assisted-protein-structure-prediction"><i class="fa fa-check"></i><b>1.2.3</b> Contact assisted protein structure prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html"><i class="fa fa-check"></i><b>2</b> Introduction to Contact Prediction</a><ul>
<li class="chapter" data-level="2.1" data-path="local-methods.html"><a href="local-methods.html"><i class="fa fa-check"></i><b>2.1</b> Local Statistical Models</a></li>
<li class="chapter" data-level="2.2" data-path="global-methods.html"><a href="global-methods.html"><i class="fa fa-check"></i><b>2.2</b> Global Statistical Models</a></li>
<li class="chapter" data-level="2.3" data-path="meta-predictors.html"><a href="meta-predictors.html"><i class="fa fa-check"></i><b>2.3</b> Machine Learning Methods and Meta-Predictors</a></li>
<li class="chapter" data-level="2.4" data-path="maxent.html"><a href="maxent.html"><i class="fa fa-check"></i><b>2.4</b> Modelling Protein Families with Potts Model</a><ul>
<li class="chapter" data-level="2.4.1" data-path="maxent.html"><a href="maxent.html#potts-model-properties"><i class="fa fa-check"></i><b>2.4.1</b> Model Properties</a></li>
<li class="chapter" data-level="2.4.2" data-path="maxent.html"><a href="maxent.html#potts-mle"><i class="fa fa-check"></i><b>2.4.2</b> Inferring Parameters for the Potts Model</a></li>
<li class="chapter" data-level="2.4.3" data-path="maxent.html"><a href="maxent.html#potts-model-solutions"><i class="fa fa-check"></i><b>2.4.3</b> Solving the Inverse Potts Problem</a></li>
<li class="chapter" data-level="2.4.4" data-path="maxent.html"><a href="maxent.html#post-processing-heuristics"><i class="fa fa-check"></i><b>2.4.4</b> Computing Contact Maps</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="application-contact-prediction.html"><a href="application-contact-prediction.html"><i class="fa fa-check"></i><b>2.5</b> Applications</a></li>
<li class="chapter" data-level="2.6" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html"><i class="fa fa-check"></i><b>2.6</b> Evaluating Contact Prediction Methods</a><ul>
<li class="chapter" data-level="2.6.1" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#seq-sep"><i class="fa fa-check"></i><b>2.6.1</b> Sequence Separation</a></li>
<li class="chapter" data-level="2.6.2" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#interpretation-of-evaluation-results"><i class="fa fa-check"></i><b>2.6.2</b> Interpretation of Evaluation Results</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>2.7</b> Challenges for Coevolutionary Inference</a><ul>
<li class="chapter" data-level="2.7.1" data-path="challenges.html"><a href="challenges.html#phylogenetic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>2.7.1</b> Phylogenetic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="2.7.2" data-path="challenges.html"><a href="challenges.html#entropic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>2.7.2</b> Entropic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="2.7.3" data-path="challenges.html"><a href="challenges.html#finite-sampling-effects"><i class="fa fa-check"></i><b>2.7.3</b> Finite Sampling Effects</a></li>
<li class="chapter" data-level="2.7.4" data-path="challenges.html"><a href="challenges.html#multiple-sequence-alignments"><i class="fa fa-check"></i><b>2.7.4</b> Multiple Sequence Alignments</a></li>
<li class="chapter" data-level="2.7.5" data-path="challenges.html"><a href="challenges.html#alternative-sources-of-coevolution"><i class="fa fa-check"></i><b>2.7.5</b> Alternative Sources of Coevolution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpreting-coupling-matrices.html"><a href="interpreting-coupling-matrices.html"><i class="fa fa-check"></i><b>3</b> Interpretation of Coupling Matrices</a><ul>
<li class="chapter" data-level="3.1" data-path="correlation-between-couplings-and-class.html"><a href="correlation-between-couplings-and-class.html"><i class="fa fa-check"></i><b>3.1</b> Single Coupling Values Carry Evidence of Contacts</a></li>
<li class="chapter" data-level="3.2" data-path="physico-chemical-fingerprints-in-coupling-matrices.html"><a href="physico-chemical-fingerprints-in-coupling-matrices.html"><i class="fa fa-check"></i><b>3.2</b> Physico-Chemical Fingerprints in Coupling Matrices</a></li>
<li class="chapter" data-level="3.3" data-path="coupling-profiles-vary-with-distance.html"><a href="coupling-profiles-vary-with-distance.html"><i class="fa fa-check"></i><b>3.3</b> Coupling Profiles Vary with Distance</a></li>
<li class="chapter" data-level="3.4" data-path="higher-order-dependencies-between-couplings.html"><a href="higher-order-dependencies-between-couplings.html"><i class="fa fa-check"></i><b>3.4</b> Higher Order Dependencies Between Couplings</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="optimizing-full-likelihood.html"><a href="optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>4</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="4.1" data-path="full-likelihood-gradient.html"><a href="full-likelihood-gradient.html"><i class="fa fa-check"></i><b>4.1</b> Approximating the Gradient of the Full Likelihood with Contrastive Divergence</a></li>
<li class="chapter" data-level="4.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html"><i class="fa fa-check"></i><b>4.2</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="4.2.1" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#cd-sampling-size"><i class="fa fa-check"></i><b>4.2.1</b> Varying the Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="adam-results.html"><a href="adam-results.html"><i class="fa fa-check"></i><b>4.3</b> Using ADAM to optimize Contrastive Divergence</a></li>
<li class="chapter" data-level="4.4" data-path="comparing-cd-couplings-to-pll-couplings.html"><a href="comparing-cd-couplings-to-pll-couplings.html"><i class="fa fa-check"></i><b>4.4</b> Comparing CD couplings to pLL couplings</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><a href="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><i class="fa fa-check"></i><b>5</b> A Bayesian Statistical Model for Residue-Residue Contact Prediction</a><ul>
<li class="chapter" data-level="5.1" data-path="overview-posterior-distances.html"><a href="overview-posterior-distances.html"><i class="fa fa-check"></i><b>5.1</b> Computing the Posterior Distribution of Distances <span class="math inline">\(p(\r | \X)\)</span></a></li>
<li class="chapter" data-level="5.2" data-path="coupling-prior.html"><a href="coupling-prior.html"><i class="fa fa-check"></i><b>5.2</b> Modelling the prior over couplings with dependence on <span class="math inline">\(\rij\)</span></a></li>
<li class="chapter" data-level="5.3" data-path="laplace-approx.html"><a href="laplace-approx.html"><i class="fa fa-check"></i><b>5.3</b> Gaussian approximation to the posterior of couplings</a><ul>
<li class="chapter" data-level="5.3.1" data-path="laplace-approx.html"><a href="laplace-approx.html#laplace-approx-improvement"><i class="fa fa-check"></i><b>5.3.1</b> Iterative improvement of Laplace approximation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="likelihood-fct-distances.html"><a href="likelihood-fct-distances.html"><i class="fa fa-check"></i><b>5.4</b> Computing the likelihood function of distances <span class="math inline">\(p(\X | \r)\)</span></a></li>
<li class="chapter" data-level="5.5" data-path="posterior-of-rij.html"><a href="posterior-of-rij.html"><i class="fa fa-check"></i><b>5.5</b> The posterior probability distribution for <span class="math inline">\(\rij\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="contact-prior.html"><a href="contact-prior.html"><i class="fa fa-check"></i><b>6</b> Contact Prior</a><ul>
<li class="chapter" data-level="6.1" data-path="random-forest-classifiers.html"><a href="random-forest-classifiers.html"><i class="fa fa-check"></i><b>6.1</b> Random Forest Classifiers</a></li>
<li class="chapter" data-level="6.2" data-path="hyperparameter-optimization-for-random-forest.html"><a href="hyperparameter-optimization-for-random-forest.html"><i class="fa fa-check"></i><b>6.2</b> Hyperparameter Optimization for Random Forest</a></li>
<li class="chapter" data-level="6.3" data-path="evaluating-random-forest-model-as-contact-predictor.html"><a href="evaluating-random-forest-model-as-contact-predictor.html"><i class="fa fa-check"></i><b>6.3</b> Evaluating Random Forest Model as Contact Predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>7</b> Methods</a><ul>
<li class="chapter" data-level="7.1" data-path="dataset.html"><a href="dataset.html"><i class="fa fa-check"></i><b>7.1</b> Dataset</a></li>
<li class="chapter" data-level="7.2" data-path="computing-pseudo-likelihood-couplings.html"><a href="computing-pseudo-likelihood-couplings.html"><i class="fa fa-check"></i><b>7.2</b> Computing Pseudo-Likelihood Couplings</a><ul>
<li class="chapter" data-level="7.2.1" data-path="computing-pseudo-likelihood-couplings.html"><a href="computing-pseudo-likelihood-couplings.html#diff-ccmpred-ccmpredpy"><i class="fa fa-check"></i><b>7.2.1</b> Differences between CCMpred and CCMpredpy</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="seq-reweighting.html"><a href="seq-reweighting.html"><i class="fa fa-check"></i><b>7.3</b> Sequence Reweighting</a></li>
<li class="chapter" data-level="7.4" data-path="amino-acid-frequencies.html"><a href="amino-acid-frequencies.html"><i class="fa fa-check"></i><b>7.4</b> Computing Amino Acid Frequencies</a></li>
<li class="chapter" data-level="7.5" data-path="methods-regularization.html"><a href="methods-regularization.html"><i class="fa fa-check"></i><b>7.5</b> Regularization</a></li>
<li class="chapter" data-level="7.6" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html"><i class="fa fa-check"></i><b>7.6</b> The Potts Model</a><ul>
<li class="chapter" data-level="7.6.1" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html#gap-treatment"><i class="fa fa-check"></i><b>7.6.1</b> Treating Gaps as Missing Information</a></li>
<li class="chapter" data-level="7.6.2" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html#the-regularized-full-log-likelihood-and-its-gradient-with-gap-treatment"><i class="fa fa-check"></i><b>7.6.2</b> The Regularized Full Log Likelihood and its Gradient With Gap Treatment</a></li>
<li class="chapter" data-level="7.6.3" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html#prior-v"><i class="fa fa-check"></i><b>7.6.3</b> The prior on <span class="math inline">\(\v\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html"><i class="fa fa-check"></i><b>7.7</b> Analysis of Coupling Matrices</a><ul>
<li class="chapter" data-level="7.7.1" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-correlation"><i class="fa fa-check"></i><b>7.7.1</b> Correlation of Couplings with Contact Class</a></li>
<li class="chapter" data-level="7.7.2" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-profile"><i class="fa fa-check"></i><b>7.7.2</b> Coupling Distribution Plots</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="methods-sgd.html"><a href="methods-sgd.html"><i class="fa fa-check"></i><b>7.8</b> Optimizing Contrastive Divergence with Stochastic Gradient Descent</a><ul>
<li class="chapter" data-level="7.8.1" data-path="methods-sgd.html"><a href="methods-sgd.html#convergence-criterion-for-stochastic-gradient-descent"><i class="fa fa-check"></i><b>7.8.1</b> Convergence Criterion for Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="7.8.2" data-path="methods-sgd.html"><a href="methods-sgd.html#sgd-hyperparameter-tuning"><i class="fa fa-check"></i><b>7.8.2</b> Tuning Hyperparameters of Stochastic Gradient Descent Optimizer</a></li>
<li class="chapter" data-level="7.8.3" data-path="methods-sgd.html"><a href="methods-sgd.html#methods-full-likelihood-adam"><i class="fa fa-check"></i><b>7.8.3</b> Tuning Hyperparameters of <em>ADAM</em> Optimizer</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="regularization-for-cd-with-sgd.html"><a href="regularization-for-cd-with-sgd.html"><i class="fa fa-check"></i><b>7.9</b> Tuning Regularization Coefficients for Contrastive Divergence</a></li>
<li class="chapter" data-level="7.10" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html"><i class="fa fa-check"></i><b>7.10</b> Tuning the Gibbs Sampling Scheme for Contrastive Divergence</a><ul>
<li class="chapter" data-level="7.10.1" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>7.10.1</b> Varying the number of Gibbs Steps</a></li>
</ul></li>
<li class="chapter" data-level="7.11" data-path="seq-features.html"><a href="seq-features.html"><i class="fa fa-check"></i><b>7.11</b> Features used to train Random Forest Model</a><ul>
<li class="chapter" data-level="7.11.1" data-path="seq-features.html"><a href="seq-features.html#seq-features-global"><i class="fa fa-check"></i><b>7.11.1</b> Global Features</a></li>
<li class="chapter" data-level="7.11.2" data-path="seq-features.html"><a href="seq-features.html#seq-features-single"><i class="fa fa-check"></i><b>7.11.2</b> Single Position Features</a></li>
<li class="chapter" data-level="7.11.3" data-path="seq-features.html"><a href="seq-features.html#seq-features-pairwise"><i class="fa fa-check"></i><b>7.11.3</b> Pairwise Features</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="rf-training.html"><a href="rf-training.html"><i class="fa fa-check"></i><b>7.12</b> Training Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="7.12.1" data-path="rf-training.html"><a href="rf-training.html#rf-feature-selection"><i class="fa fa-check"></i><b>7.12.1</b> Feature Selection</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="abbrev.html"><a href="abbrev.html"><i class="fa fa-check"></i><b>A</b> Abbreviations</a></li>
<li class="chapter" data-level="B" data-path="amino-acids.html"><a href="amino-acids.html"><i class="fa fa-check"></i><b>B</b> Amino Acid Alphabet</a></li>
<li class="chapter" data-level="C" data-path="dataset-properties.html"><a href="dataset-properties.html"><i class="fa fa-check"></i><b>C</b> Dataset Properties</a></li>
<li class="chapter" data-level="D" data-path="standard-deviation-of-couplings-for-noncontacts.html"><a href="standard-deviation-of-couplings-for-noncontacts.html"><i class="fa fa-check"></i><b>D</b> Standard Deviation of Couplings for Noncontacts</a></li>
<li class="chapter" data-level="E" data-path="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><a href="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><i class="fa fa-check"></i><b>E</b> Amino Acid Interaction Preferences Reflected in Coupling Matrices</a><ul>
<li class="chapter" data-level="E.1" data-path="pi-cation.html"><a href="pi-cation.html"><i class="fa fa-check"></i><b>E.1</b> Pi-Cation interactions</a></li>
<li class="chapter" data-level="E.2" data-path="disulfide.html"><a href="disulfide.html"><i class="fa fa-check"></i><b>E.2</b> Disulfide Bonds</a></li>
<li class="chapter" data-level="E.3" data-path="aromatic-proline.html"><a href="aromatic-proline.html"><i class="fa fa-check"></i><b>E.3</b> Aromatic-Proline Interactions</a></li>
<li class="chapter" data-level="E.4" data-path="aromatic-network.html"><a href="aromatic-network.html"><i class="fa fa-check"></i><b>E.4</b> Network-like structure of aromatic residues</a></li>
<li class="chapter" data-level="E.5" data-path="aromatic-small-distances.html"><a href="aromatic-small-distances.html"><i class="fa fa-check"></i><b>E.5</b> Aromatic Sidechains at small <span class="math inline">\(Cb\)</span>-<span class="math inline">\(\Cb\)</span> distances</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="training-of-the-random-forest-contact-prior.html"><a href="training-of-the-random-forest-contact-prior.html"><i class="fa fa-check"></i><b>F</b> Training of the Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="F.1" data-path="training-random-forest-model-with-pseudo-likelihood-feature.html"><a href="training-random-forest-model-with-pseudo-likelihood-feature.html"><i class="fa fa-check"></i><b>F.1</b> Training Random Forest Model with pseudo-likelihood Feature</a></li>
<li class="chapter" data-level="F.2" data-path="rf-window-size.html"><a href="rf-window-size.html"><i class="fa fa-check"></i><b>F.2</b> Evaluating window size with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="F.3" data-path="rf-noncontact-threshold.html"><a href="rf-noncontact-threshold.html"><i class="fa fa-check"></i><b>F.3</b> Evaluating non-contact threshold with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="F.4" data-path="rf-ratio-noncontacts.html"><a href="rf-ratio-noncontacts.html"><i class="fa fa-check"></i><b>F.4</b> Evaluating ratio of non-contacts and contacts in the training set with 5-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PhD thesis: residue-residue contact prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="methods-sgd" class="section level2">
<h2><span class="header-section-number">7.8</span> Optimizing Contrastive Divergence with Stochastic Gradient Descent</h2>
<p>This section describes hyperparameter tuning for the stochastic gradient descent optimization of <a href="abbrev.html#abbrev">CD</a>.</p>
<p>The couplings <span class="math inline">\(\wijab\)</span> are initialized at 0 and single potentials <span class="math inline">\(\vi\)</span> will not be optimized but rather kept fixed at their maximum-likleihood estimate <span class="math inline">\(\vi^*\)</span> as described in methods section <a href="potts-full-likelihood.html#prior-v">7.6.3</a>. The gradient of the full likelihood is approximated with <a href="abbrev.html#abbrev">CD</a> which involves Gibbs sampling of protein sequences according to the current model parametrization and is described in detail in methods section <a href="cd-sampling-optimization.html#cd-sampling-optimization">7.10</a>. Zero centered L2-regularization is used to constrain the coupling parameters <span class="math inline">\(\w\)</span> using the regularization coefficient <span class="math inline">\(\lambda_w = 0.2L\)</span> which is the default setting for optimizing the pseudo-likelihood with <em>CCMpredPy</em>. Performance will be evaluated by the mean precision of top ranked contact predictions over a benchmark set of 300 proteins, that is a subset of the data set described in methods section <a href="dataset.html#dataset">7.1</a>. Contact scores for couplings are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm as explained in section <a href="maxent.html#post-processing-heuristics">2.4.4</a>. Pseudo-likelihood couplings are computed with the tool <em>CCMpredPy</em> that is introduced in methods section <a href="computing-pseudo-likelihood-couplings.html#diff-ccmpred-ccmpredpy">7.2.1</a> and the pseudo-likelihood contact score will serve as general reference method for tuning the hyperparameters.</p>
<div id="convergence-criterion-for-stochastic-gradient-descent" class="section level3">
<h3><span class="header-section-number">7.8.1</span> Convergence Criterion for Stochastic Gradient Descent</h3>
<p>In theory the gradient descent algorithm has converged and the optimum of the objective function has been reached when the gradient becomes zero. In practice the gradients will never be exactly zero, especially due to the stochasticity of the gradient estimates when using stochastic gradient descent with <a href="abbrev.html#abbrev">CD</a>. For this reason, it is crucial to define a suitable convergence criterion that can be tested during optimization and once the criterion is met, convergence is assumed and the algorithm is stopped. Typically, the objective function (or a related loss function) is periodically evaluated on a validation set and the optimizer is halted whenever the function value saturates or starts to increase. This technique is called early stopping and additionally prevents overfitting <span class="citation">[<a href="#ref-Mahsereci2017">193</a>,<a href="#ref-Bengio2012">207</a>]</span>. Unfortunately, we cannot compute the full likelihood function due to its complexity and need to define a different convergence criterion.</p>
<p>One possibility is to stop learning when the norm of the gradients is close to zero <span class="citation">[<a href="#ref-Carreira-Perpinan2005">208</a>]</span>. As described earlier, only a subset of alignment sequences is used to estimate the gradient with <a href="abbrev.html#abbrev">CD</a> which increases stochasticity of the gradient estimate and speeds up the algorithm. As a result of stochasticity, the norm of gradients does not converge to zero but saturates at a certain offset as it is described in section <a href="cd-sampling-optimization.html#cd-sampling-optimization">7.10</a>. Only by using large number of sequences to estimate the gradients with <a href="abbrev.html#abbrev">CD</a>, the norm over gradients will converge towards a value close to zero. However, runtine increases linearly in the number of sequences used for estimating the gradient with <a href="abbrev.html#abbrev">CD</a> by Gibbs sampling. Convergence can also be monitored as the relative change of the norm of gradients between a particular number of iterations and optimization can be stopped when the norm of gradients has reached a certain plateau. As gradient estimates are generally very noisy with stochastic gradient descent, gradient fluctuations complicate the proper assessment of this criterion.</p>
Instead of the gradients, it is also possible to observe the relative change over the norm of parameter estimates <span class="math inline">\(||\w||_2\)</span> and stop learning when it falls below a small threshold <span class="math inline">\(\epsilon\)</span>,
<span class="math display" id="eq:parameter-convergence-criterion">\[\begin{equation}
  \frac{||\w_{t-1}||_2 - ||\w_t||_2}{||\w_{t-1}||_2} &lt; \epsilon \; .
  \tag{7.17}
\end{equation}\]</span>
<p>This measure is less noisy than subsequent gradient estimates because the magnitude of parameter updates is bounded by the learning rate.</p>
<p>Another idea is to monitor the direction of gradients, since the assumption goes that gradients will start oscillating when approaching the optimum. However, this theoretical assumption is complicated by the fact that gradient oscillations are also typically observed when the parameter surface contains narrow valleys or when the learning rate is too big, as it is visualized in the right plot in Figure <a href="full-likelihood-optimization.html#fig:gd-learning-rate-intro">4.2</a>. Another interfering factor is momentum, as it is used in the <em>ADAM</em> optimizer. Parameters will be updated into the direction of a smoothed historical gradient and oscillations, regardless of which origin, will be dampened. It is therefore hard to define a general convergence criteria that uses gradient directions and can distinguish these different scenarios.</p>
<p>Of course, the simplest strategy to assume convergence is to specify a maximum number of iterations for the optimization procedure, which also ensures that the algorithm will stop eventually if none of the other convergence criteria is met.</p>
<p>A necessary but not sufficient criterion for convergence for the full likelihood is given by <span class="math inline">\(\sum_{a,b=1}^{20} \wijab = 0\)</span>. This requirement is derived in section <a href="potts-full-likelihood.html#prior-v">7.6.3</a>. When using plain stochastic gradient descent without momentum and without adaptive learning rates, this criterion is never violated when parameters are initialized uniformly. This is due to the fact that the 400 gradients <span class="math inline">\(\wijab\)</span> for <span class="math inline">\(a,b \in \{1, \ldots, 20\}\)</span> are not independent because the sum over the 400 pairwise amino acid counts at positions <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is identical for the observed and the sampled alignment and amounts to,</p>
<span class="math display">\[\begin{equation}
  \sum_{a,b=1}^{20} N_{ij} q(x_i \eq a, q_j \eq b) = N_{ij} \; .
\end{equation}\]</span>
<p>Considering a residue pair (i,j) and assuming amino acid pair (a,b) has higher counts in the sampled alignment than in the observed input alignment, then this difference in counts must be compensated by other amino acid pairs (c,d) having less counts in the sampled alignment compared to the true alignment (see Figure <a href="methods-sgd.html#fig:visualisation-wijab-constraint">7.5</a>). Therefore it holds, <span class="math inline">\(\sum_{a,b=1}^{20} \nabla_{\wijab}\LLreg(\v,\w) = 0\)</span>. This symmetry is translated into parameter updates as long as the same learning rate is used to update all parameters. However, when using adaptive learning rates, this symmetry is broken and the condition <span class="math inline">\(\sum_{a,b=1}^{20} \wijab = 0\)</span> can be violated during the optimization processs. It is therefore interesting to monitor <span class="math inline">\(\sum_{1 \le 1 &lt; j \le L} \sum_{a,b=1}^{20} \wijab\)</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:visualisation-wijab-constraint"></span>
<img src="img/full_likelihood/constraint_wijab.png" alt="The 400 gradients \(\nabla_{\wijab}\LLreg(\v,\w)\) at position \((i,j)\) for \(a,b \in \{1, \ldots, 20 \}\) are not independent. Red bars represent pairwise amino acid counts at position \((i,j)\) for the sampled alignment. Blue bars represent pairwise amino acid counts at position \((i,j)\) for the input alignment. The sum over pairwise amino acid counts at position \((i,j)\) for both alignments is \(N_{ij}\), which is the number of ungapped sequences. The gradient \(\nabla_{\wijab}\LLreg(\v,\w)\) is computed as the difference of pairwise amino acid counts for amino acids a and b at position \((i,j)\). The sum over gradients \(\nabla_{\wijab}\LLreg(\v,\w)\) at position \((i,j)\) for all \(a,b \in \{1, \ldots, 20 \}\) is zero." width="60%" />
<p class="caption">
Figure 7.5: The 400 gradients <span class="math inline">\(\nabla_{\wijab}\LLreg(\v,\w)\)</span> at position <span class="math inline">\((i,j)\)</span> for <span class="math inline">\(a,b \in \{1, \ldots, 20 \}\)</span> are not independent. Red bars represent pairwise amino acid counts at position <span class="math inline">\((i,j)\)</span> for the sampled alignment. Blue bars represent pairwise amino acid counts at position <span class="math inline">\((i,j)\)</span> for the input alignment. The sum over pairwise amino acid counts at position <span class="math inline">\((i,j)\)</span> for both alignments is <span class="math inline">\(N_{ij}\)</span>, which is the number of ungapped sequences. The gradient <span class="math inline">\(\nabla_{\wijab}\LLreg(\v,\w)\)</span> is computed as the difference of pairwise amino acid counts for amino acids a and b at position <span class="math inline">\((i,j)\)</span>. The sum over gradients <span class="math inline">\(\nabla_{\wijab}\LLreg(\v,\w)\)</span> at position <span class="math inline">\((i,j)\)</span> for all <span class="math inline">\(a,b \in \{1, \ldots, 20 \}\)</span> is zero.
</p>
</div>
<p>I set the maximum number of iterations to 5000 and stop the optimization algorithm when the relative change over the L2-norm of parameter estimates <span class="math inline">\(||\w||_2\)</span> falls below the threshold of <span class="math inline">\(\epsilon = 1e-8\)</span>.</p>
</div>
<div id="sgd-hyperparameter-tuning" class="section level3">
<h3><span class="header-section-number">7.8.2</span> Tuning Hyperparameters of Stochastic Gradient Descent Optimizer</h3>
<p>The coupling parameters <span class="math inline">\(\w\)</span> will be updated at each time step <span class="math inline">\(t\)</span> by taking a step of size <span class="math inline">\(\alpha\)</span> along the direction of the negative gradient of the regularized full log likelihood, <span class="math inline">\(- \nabla_w \LLreg(\v,\w)\)</span>, that has been approximated with <a href="abbrev.html#abbrev">CD</a>,</p>
<span class="math display">\[\begin{equation}
  \w_{t+1} = \w_t - \alpha \cdot \nabla_w \LLreg(\v,\w) \; .
\end{equation}\]</span>
<p>In order to get a first intuition of the optimization problem, I tested initial learning rates <span class="math inline">\(\alpha_0 \in \{1\mathrm{e}{-4}, 5\mathrm{e}{-4}, 1\mathrm{e}{-3}, 5\mathrm{e}{-3}\}\)</span> with a standard learning rate annealing schedule, <span class="math inline">\(\alpha = \frac{\alpha_0}{1 + \gamma \cdot t}\)</span> where <span class="math inline">\(t\)</span> is the time step and <span class="math inline">\(\gamma\)</span> is the decay rate that is set to 0.01<span class="citation">[<a href="#ref-Bottou2012">188</a>]</span>.</p>
<p>Figure <a href="methods-sgd.html#fig:performance-cd-alphaopt">7.6</a> shows the mean precision for top ranked contacts computed from pseudo-likelihood couplings and from <a href="abbrev.html#abbrev">CD</a> couplings optimized with stochastic gradient descent using the four different learning rates. Overall, mean precision for <a href="abbrev.html#abbrev">CD</a> contacts is lower than for pseudo-likelihood contacts, especially when using the smallest (<span class="math inline">\(\alpha_0 \eq 1\mathrm{e}{-4}\)</span>) and biggest (<span class="math inline">\(\alpha_0 \eq 5\mathrm{e-}{3}\)</span>) learning rate.</p>

<div class="figure" style="text-align: center"><span id="fig:performance-cd-alphaopt"></span>
<iframe src="img/full_likelihood/sgd/precision_vs_rank_learning_rates.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 7.6: Mean precision for top ranked contact predictions over 286 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. <strong>pseudo-likelihood</strong>: couplings computed with pseudo-likelihood. <strong>CD</strong>: couplings computed with <a href="abbrev.html#abbrev">CD</a> using stochastic gradient descent with different initial learning rates <span class="math inline">\(\alpha_0\)</span> as specified in the legend.
</p>
</div>
<p>Looking at individual proteins it turns out that the optimal learning rate depends on alignment size. The left plot in Figure <a href="methods-sgd.html#fig:sgd-single-proteins-initial-learning-rate">7.7</a> shows a convergence plot of <a href="abbrev.html#abbrev">SGD</a> optimization using different learning rates for a protein with a small alignment. With a small initial learning rate <span class="math inline">\(\alpha_0 \eq \mathrm{1e-4}\)</span> the optimization runs very slowly and does not reach convergence within 5000 iterations. Using a large initial learning rate <span class="math inline">\(\alpha_0 \eq \mathrm{5e-3}\)</span> will result in slighly overshooting the optimum at the beginning of the optimization but with the learning rate decaying over time the parameter estimates converge. In contrast, for a protein with a big alignment (right plot in Figure <a href="methods-sgd.html#fig:sgd-single-proteins-initial-learning-rate">7.7</a>) the choice of learning rate has a more pronounced effect. With a small initial learning rate <span class="math inline">\(\alpha_0 \eq \mathrm{1e-4}\)</span> the optimization runs slowly but almost converges within 5000 iterations. A large initial learning rate <span class="math inline">\(\alpha_0 \eq \mathrm{5e-3}\)</span> lets the parameters diverge quickly and the optimum cannot be revovered. With learning rates <span class="math inline">\(\alpha_0 \eq \mathrm{5e-4}\)</span> and <span class="math inline">\(\alpha_0 \eq \mathrm{1e-3}\)</span>, the optimum is well overshot at the beginning of the optimization but the parameter estimates eventually converge as the learning rate decreases over time.</p>
<p>These observations can be explained by the fact that the magnitude of the gradient scales with the number of sequences in the alignment. The gradient is computed from amino acid counts as explained in section <a href="full-likelihood-gradient.html#full-likelihood-gradient">4.1</a>. Therefore, alignments with many sequences will generally produce larger gradients than alignments with few sequences, especially at the beginning of the optimization procedure when the difference in amino acid counts between sampled and observed sequences is largest. Following these observations, I defined the initial learning rate <span class="math inline">\(\alpha_0\)</span> as a function of <a href="abbrev.html#abbrev">Neff</a>, aiming at values for <span class="math inline">\(\alpha_0\)</span> around 5e-3 for small <a href="abbrev.html#abbrev">Neff</a> and values for <span class="math inline">\(\alpha_0\)</span> around 1e-4 for large <a href="abbrev.html#abbrev">Neff</a>,</p>
<span class="math display" id="eq:learning-rate-wrt-neff">\[\begin{equation}
  \alpha_0 = \frac{5\mathrm{e}{-2}}{\sqrt{N_{\text{eff}}}} \; .
  \tag{7.18}
\end{equation}\]</span>
<p>For small <a href="abbrev.html#abbrev">Neff</a> <span class="math inline">\(\approx 50\)</span> this definition of the learning rate yields <span class="math inline">\(\alpha_0 \approx 7\mathrm{e}{-3}\)</span> and for big <a href="abbrev.html#abbrev">Neff</a> <span class="math inline">\(\approx 20000\)</span> this yields <span class="math inline">\(\alpha_0 \approx 3.5\mathrm{e}{-4}\)</span>. Using this learning rate defined as a function of <a href="#bbrev">Neff</a>, precision improves over the previous fixed learning rates (see Figure <a href="methods-sgd.html#fig:performance-cd-alphaopt">7.6</a>). All following analyses are conducted using the <a href="abbrev.html#abbrev">Neff</a>-dependent learning rate.</p>

<div class="figure" style="text-align: center"><span id="fig:sgd-single-proteins-initial-learning-rate"></span>
<img src="img/full_likelihood/sgd/parameter_norm_1mkca00_alphas_lindecay001.png" alt="Convergence plots for two proteins during SGD optimization with different learning rates and convergence measured as L2-norm of the coupling parameters \(||\w||_2\) . Linear learning rate annealing schedule has been used with decay rate \(\gamma=0.01\) and initial learning rates \(\alpha_0\) have been set as specified in the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808). Figure is cut at the yaxis at \(||\w||_2=1500\), but learning rate of \(5\mathrm{e}{-3}\) reaches \(||\w||_2 \approx 13000\)." width="50%" /><img src="img/full_likelihood/sgd/parameter_norm_1c75a00_alphas_lindecay001.png" alt="Convergence plots for two proteins during SGD optimization with different learning rates and convergence measured as L2-norm of the coupling parameters \(||\w||_2\) . Linear learning rate annealing schedule has been used with decay rate \(\gamma=0.01\) and initial learning rates \(\alpha_0\) have been set as specified in the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808). Figure is cut at the yaxis at \(||\w||_2=1500\), but learning rate of \(5\mathrm{e}{-3}\) reaches \(||\w||_2 \approx 13000\)." width="50%" />
<p class="caption">
Figure 7.7: Convergence plots for two proteins during <a href="abbrev.html#abbrev">SGD</a> optimization with different learning rates and convergence measured as L2-norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span> . Linear learning rate annealing schedule has been used with decay rate <span class="math inline">\(\gamma=0.01\)</span> and initial learning rates <span class="math inline">\(\alpha_0\)</span> have been set as specified in the legend. <strong>Left</strong> Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=96). <strong>Right</strong> Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=16808). Figure is cut at the yaxis at <span class="math inline">\(||\w||_2=1500\)</span>, but learning rate of <span class="math inline">\(5\mathrm{e}{-3}\)</span> reaches <span class="math inline">\(||\w||_2 \approx 13000\)</span>.
</p>
</div>
<p>In a next step, I evaluated the following learning rate annealing schedules and decay rates using the <a href="abbrev.html#abbrev">Neff</a>-dependent initial learning rate given in eq. <a href="methods-sgd.html#eq:learning-rate-wrt-neff">(7.18)</a>:</p>
<ul>
<li>default linear learning rate schedule <span class="math inline">\(\alpha = \frac{\alpha_0}{1 + \gamma t}\)</span> with <span class="math inline">\(\gamma \in \{1\mathrm{e}{-3}, 1\mathrm{e}{-2}, 1\mathrm{e}{-1}, 1 \}\)</span></li>
<li>square root learning rate schedule <span class="math inline">\(\alpha = \frac{\alpha_0}{\sqrt{1 + \gamma t}}\)</span> with <span class="math inline">\(\gamma \in \{1\mathrm{e}{-2}, 1\mathrm{e}{-1}, 1 \}\)</span></li>
<li>sigmoidal learning rate schedule <span class="math inline">\(\alpha_{t+1} = \frac{\alpha_{t}}{1 + \gamma t}\)</span> with <span class="math inline">\(\gamma \in \{1\mathrm{e}{-6}, 1\mathrm{e}{-5}, 1\mathrm{e}{-4}, 1\mathrm{e}{-3}\}\)</span></li>
<li>exponential learning rate schedule <span class="math inline">\(\alpha_{t+1} = \alpha_0 \cdot\exp(- \gamma t)\)</span> with <span class="math inline">\(\gamma \in \{5\mathrm{e}{-4}, 1\mathrm{e}{-4}, 5\mathrm{e}{-3}\}\)</span></li>
</ul>
<p>The learning rate annealing schedules are visualized for different decay rates in Appendix Figure <a href="#fig:learning-rate-schedules"><strong>??</strong></a> and the respective benchmark plots can be found in Appendix <a href="#benchmark-learning-rate-annealing-schedules"><strong>??</strong></a>. Optimizing <a href="abbrev.html#abbrev">CD</a> with <a href="abbrev.html#abbrev">SGD</a> using any of the learning rate schedules listed above yields on average lower precision for the top ranked contacts than the pseudo-likelihood contact score. Several learning rate schedules perform almost equally well as can be seen in Figure <a href="methods-sgd.html#fig:performance-cd-learnignrate-schedules">7.8</a>. The highest precision, being one to two percentage points below the mean precision for the pseudo-likelihood contact score, is obtained with a linear learning rate schedule and decay rate <span class="math inline">\(\gamma \eq 1\mathrm{e}{-2}\)</span>, with a sigmoidal learning rate schedule and decay rates <span class="math inline">\(\gamma \eq 1\mathrm{e}{-5}\)</span> and <span class="math inline">\(\gamma \eq 1\mathrm{e}{-6}\)</span> and with an exponential learning rate schedule and decay rate <span class="math inline">\(\gamma \eq 1\mathrm{e}{-3}\)</span> and <span class="math inline">\(\gamma \eq 1\mathrm{e}{-5}\)</span>. The square root learning rate schedule gives ovarally bad results and does not lead to convergence because the learning rate decays slowly at later time steps.</p>

<div class="figure" style="text-align: center"><span id="fig:performance-cd-learnignrate-schedules"></span>
<iframe src="img/full_likelihood/sgd/precision_vs_rank_schedules.html" width="100%" height="500px">
</iframe>
<p class="caption">
Figure 7.8: Mean precision for top ranked contact predictions over 288 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. <strong>pseudo-likelihood</strong>: couplings computed with pseudo-likelihood. <strong>CD</strong>: couplings computed with <a href="abbrev.html#abbrev">CD</a> using stochastic gradient descent with an initial learning rate defined with respect to <a href="abbrev.html#abbrev">Neff</a>. Learning rate annealing schedules and decay rates as specified in the legend.
</p>
</div>
<p>In contrast to the findings regarding the initial learning rate earlier, an optimal decay rate can be defined independent of the alignment size. Figure <a href="methods-sgd.html#fig:sgd-single-proteins-learning-rate-schedule">7.9</a> shows convergence plots for the same two exemplary proteins as before. Proteins with low <a href="abbrev.html#abbrev">Neff</a> are robust against the particular choice of learning rate schedule and decay rate (see left plot in Figure <a href="methods-sgd.html#fig:sgd-single-proteins-learning-rate-schedule">7.9</a>). The presumed optimum at <span class="math inline">\(||w||_2 \approx 12.5\)</span> is almost always reached. Proteins with high <a href="abbrev.html#abbrev">Neff</a> are stronger adversely affected by quickly decaying learning rates and the optimum at <span class="math inline">\(||w||_2 \approx 125\)</span> cannot be reached before the learning rate diminishes which effectively prevents further optimization progress. Less quickly decaying learning rates, such as <span class="math inline">\(\gamma \eq 1\mathrm{e}{-2}\)</span> with a linear schedule or <span class="math inline">\(\gamma \eq 1\mathrm{e}{-6}\)</span> with a sigmoidal schedule, guide the parameter estimates close to the expected optimum and can be used with proteins having low <a href="abbrev.html#abbrev">Neffs</a> as well as having high <a href="abbrev.html#abbrev">Neffs</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:sgd-single-proteins-learning-rate-schedule"></span>
<img src="img/full_likelihood/sgd/parameter_norm_1mkca00_alpha0_different_schedules.png" alt="L2-norm of the coupling parameters \(||\w||_2\) during stochastic gradient descent optimization with different learning rates schedules. The initial learning rate \(\alpha_0\) is defined with respect to Neff as given in eq. (7.18). Learning rate schedules and decay rates are used according to the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808)." width="50%" /><img src="img/full_likelihood/sgd/parameter_norm_1c75a00_alpha0_different_schedules.png" alt="L2-norm of the coupling parameters \(||\w||_2\) during stochastic gradient descent optimization with different learning rates schedules. The initial learning rate \(\alpha_0\) is defined with respect to Neff as given in eq. (7.18). Learning rate schedules and decay rates are used according to the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808)." width="50%" />
<p class="caption">
Figure 7.9: L2-norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span> during stochastic gradient descent optimization with different learning rates schedules. The initial learning rate <span class="math inline">\(\alpha_0\)</span> is defined with respect to <a href="abbrev.html#abbrev">Neff</a> as given in eq. <a href="methods-sgd.html#eq:learning-rate-wrt-neff">(7.18)</a>. Learning rate schedules and decay rates are used according to the legend. <strong>Left</strong> Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=96). <strong>Right</strong> Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=16808).
</p>
</div>
<p>Several different learning rate annealing schedules yield almost identical mean precision for top ranked contacts, as was shown earlier (see Figure <a href="methods-sgd.html#fig:performance-cd-learnignrate-schedules">7.8</a>). But it can be found that they differ in convergence speed. Figure <a href="methods-sgd.html#fig:distribution-num-iterations">7.10</a> shows the distribution over the number of iterations until convergence for <a href="abbrev.html#abbrev">SGD</a> optimizations with five different learning rate schedules that yield similar performance. The optimization converges on average within less than 2000 iterations only when using either a sigmoidal learning rate annealing schedule with decay rate <span class="math inline">\(\gamma \eq 1\mathrm{e}{-5}\)</span> or an exponential learning rate annealing schedule with decay rate <span class="math inline">\(\gamma \eq 5\mathrm{e}{-3}\)</span>, On the contrary, the distribution of iterations until convergence has a median of 5000 when using a linear learning rate annealing schedule with <span class="math inline">\(\gamma \eq 1\mathrm{e}{-2}\)</span> or an exponential schedule with decay rate <span class="math inline">\(\gamma \eq 1\mathrm{e}{-3}\)</span>. Under these considerations, I chose a sigmoidal learning rate schedule with <span class="math inline">\(\gamma \eq 5\mathrm{e}{-6}\)</span> for all further analysis.</p>

<div class="figure" style="text-align: center"><span id="fig:distribution-num-iterations"></span>
<iframe src="img/full_likelihood/sgd/distribution_numiterations_against_selected_learningrate_schedules.html" width="100%" height="500px">
</iframe>
<p class="caption">
Figure 7.10: Distribution of the number of iterations until convergence for <a href="abbrev.html#abbrev">SGD</a> optimizations of the full likelihood for different learning rate schedules. Convergence is reached when the relative difference of parameter norms <span class="math inline">\(||\w||_2\)</span> falls below <span class="math inline">\(\epsilon \eq 1e-8\)</span>. Initial learning rate <span class="math inline">\(\alpha_0\)</span> is defined with respect to <a href="abbrev.html#abbrev">Neff</a> as given in eq. <a href="methods-sgd.html#eq:learning-rate-wrt-neff">(7.18)</a> and maximum number of iterations is set to 5000. Learning rate schedules and decay rates are used as specified in the legend.
</p>
</div>
</div>
<div id="methods-full-likelihood-adam" class="section level3">
<h3><span class="header-section-number">7.8.3</span> Tuning Hyperparameters of <em>ADAM</em> Optimizer</h3>
<p><em>ADAM</em> <span class="citation">[<a href="#ref-Kingma2014">192</a>]</span> stores an exponentially decaying average of past gradients and squared gradients,</p>
<span class="math display">\[\begin{align}
  m_t &amp;= \beta_1 m_{t−1} + (1 − \beta_1) g \\
  v_t &amp;= \beta_2 v_{t−1} + (1 − \beta_2) g^2 \; ,
\end{align}\]</span>
<p>with <span class="math inline">\(g = \nabla_w \LLreg(\v,\w)\)</span> and the rate of decay being determined by hyperparameters <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>. Both terms <span class="math inline">\(m_t\)</span> and <span class="math inline">\(v_t\)</span> represent estimates of the first and second moments of the gradient, respectively. The following bias correction terms compensates for the fact that the vectors <span class="math inline">\(m_t\)</span> and <span class="math inline">\(v_t\)</span> are both initialized at zero and therefore are biased towards zero especially at the beginning of optimization,</p>
<span class="math display">\[\begin{align}
  \hat{m_t} &amp;= \frac{m_t}{1-\beta_1^t} \\
  \hat{v_t} &amp;= \frac{v_t}{1-\beta_2^t} \; .
\end{align}\]</span>
Parameters are then updated using step size <span class="math inline">\(\alpha\)</span>, a small noise term <span class="math inline">\(\epsilon\)</span> and the corrected moment estimates <span class="math inline">\(\hat{m_t}\)</span>, <span class="math inline">\(\hat{v_t}\)</span>, according to
<span class="math display">\[\begin{equation}
  x_{t+1} = x_t - \alpha \cdot \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}
\end{equation}\]</span>
<p>Kingma et al. proposed the default values <span class="math inline">\(\beta_1=0.9\)</span>, <span class="math inline">\(\beta_2=0.999\)</span> and <span class="math inline">\(\epsilon=1e−8\)</span> and a constant learning rate <span class="math inline">\(\alpha=1e-3\)</span>.</p>
<p>For the two protein chains 1mkc_A_00 and 1c75_A_00, having 142 (<a href="abbrev.html#abbrev">Neff</a>=96) and 28078 (<a href="abbrev.html#abbrev">Neff</a>=16808) aligned sequences respectively, I analysed the convergence for <a href="abbrev.html#abbrev">SGD</a> with different learning rates <span class="math inline">\(\alpha\)</span> (see Figure <a href="methods-sgd.html#fig:adam-learning-rate">7.11</a>). In contrast to plain stochastic gradient descent, with <em>ADAM</em> it is possible to use larger learning rates for proteins having big alignments, because the learning rate will be adapted to the magnitude of the gradient for every parameter individually. For protein 1mkc_A_00 having a small alignment, a learning rate of 5e-3 quickly leads to convergence whereas for protein 1c75_A_00 a larger learning rate can be chosen to obtain quick convergence. As a consequence, I defined the learning rate <span class="math inline">\(\alpha\)</span> as a function of <a href="abbrev.html#abbrev">Neff</a>,</p>
<span class="math display" id="eq:learning-rate-wrt-neff-adam">\[\begin{equation}
  \alpha = 2\mathrm{e}{-3}\log(N_{\text{eff}}) \; ,
  \tag{7.19}
\end{equation}\]</span>
<p>such that it will take values ~5e-3 for proteins with small alignments and values ~1e-2 for proteins with large alignments.</p>

<div class="figure" style="text-align: center"><span id="fig:adam-learning-rate"></span>
<img src="img/full_likelihood/adam/1mkcA00_learningrates_parameternorm.png" alt="L2-norm of the coupling parameters \(||\w||_2\) during optimization with ADAM and different learning rates without annealing. The learning rate \(\alpha\) is specified in the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808)." width="50%" /><img src="img/full_likelihood/adam/1c75A00_learningrates_parameternorm.png" alt="L2-norm of the coupling parameters \(||\w||_2\) during optimization with ADAM and different learning rates without annealing. The learning rate \(\alpha\) is specified in the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808)." width="50%" />
<p class="caption">
Figure 7.11: L2-norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span> during optimization with <em>ADAM</em> and different learning rates without annealing. The learning rate <span class="math inline">\(\alpha\)</span> is specified in the legend. <strong>Left</strong> Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=96). <strong>Right</strong> Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=16808).
</p>
</div>
<p>It is interesting to note in Figure <a href="methods-sgd.html#fig:adam-learning-rate">7.11</a>, that the norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span> converges towards different values depending on the choice of the learning rate <span class="math inline">\(\alpha\)</span>. This inidicates that it is necessary to decrease the learning rate over time. By default, <em>ADAM</em> uses a constant learning rate, because the algorithm performs a kind of step size annealing by nature. However, popular implementations of <em>ADAM</em> in the <a href="https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L385">Keras</a> <span class="citation">[<a href="#ref-Chollet2015">209</a>]</span> and <a href="https://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py#L565-L629">Lasagne</a> <span class="citation">[<a href="#ref-Dieleman2015">210</a>]</span> packages allow the use of an annealing schedule. I therefore tested different learning rate annealing schedules for <em>ADAM</em> assuming that with decreasing learning rates the L2-norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span> will converge towards a consistent value. Indeed, as can be seen in Figure <a href="methods-sgd.html#fig:adam-learning-rate-annealing">7.12</a>, when using a linear or sigmoidal learning rate annealing schedule with <em>ADAM</em>, the L2-norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span> converges roughly towards the same value that has been obtained with plain <a href="abbrev.html#abbrev">SGD</a> shown in Figure <a href="methods-sgd.html#fig:sgd-single-proteins-learning-rate-schedule">7.9</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:adam-learning-rate-annealing"></span>
<img src="img/full_likelihood/adam/1mkcA00_decaying_learningrates_parameternorm.png" alt="L2-norm of the coupling parameters \(||\w||_2\) during optimization with ADAM and different learning rate annealing schedules. The learning rate \(\alpha\) is specified with respect to Neff as \(\alpha = 2\mathrm{e}{-3}\log(N_{\text{eff}})\). The learning rate annealing schedule is specified in the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808)." width="50%" /><img src="img/full_likelihood/adam/1c75A00_decaying_learningrates_parameternorm.png" alt="L2-norm of the coupling parameters \(||\w||_2\) during optimization with ADAM and different learning rate annealing schedules. The learning rate \(\alpha\) is specified with respect to Neff as \(\alpha = 2\mathrm{e}{-3}\log(N_{\text{eff}})\). The learning rate annealing schedule is specified in the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808)." width="50%" />
<p class="caption">
Figure 7.12: L2-norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span> during optimization with <em>ADAM</em> and different learning rate annealing schedules. The learning rate <span class="math inline">\(\alpha\)</span> is specified with respect to <a href="#abrbev">Neff</a> as <span class="math inline">\(\alpha = 2\mathrm{e}{-3}\log(N_{\text{eff}})\)</span>. The learning rate annealing schedule is specified in the legend. <strong>Left</strong> Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=96). <strong>Right</strong> Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=16808).
</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Mahsereci2017">
<p>193. Mahsereci, M., Balles, L., Lassner, C., and Hennig, P. (2017). Early Stopping without a Validation Set. arXiv. Available at: <a href="http://arxiv.org/abs/1703.09580" class="uri">http://arxiv.org/abs/1703.09580</a>.</p>
</div>
<div id="ref-Bengio2012">
<p>207. Bengio, Y. (2012). Practical Recommendations for Gradient-Based Training of Deep Architectures. In Neural networks: Tricks of the trade (Springer Berlin Heidelberg), pp. 437–478. Available at: <a href="https://arxiv.org/pdf/1206.5533v2.pdf" class="uri">https://arxiv.org/pdf/1206.5533v2.pdf</a>.</p>
</div>
<div id="ref-Carreira-Perpinan2005">
<p>208. Carreira-Perpiñán, M. a, and Hinton, G.E. (2005). On Contrastive Divergence Learning. Artif. Intell. Stat. <em>0</em>, 17. Available at: <a href="http://learning.cs.toronto.edu/{~}hinton/absps/cdmiguel.pdf" class="uri">http://learning.cs.toronto.edu/{~}hinton/absps/cdmiguel.pdf</a>.</p>
</div>
<div id="ref-Bottou2012">
<p>188. Bottou, L. (2012). Stochastic Gradient Descent Tricks. In Neural networks: Tricks of the trade (Springer, Berlin, Heidelberg), pp. 421–436. Available at: <a href="http://link.springer.com/10.1007/978-3-642-35289-8{\_}25" class="uri">http://link.springer.com/10.1007/978-3-642-35289-8{\_}25</a>.</p>
</div>
<div id="ref-Kingma2014">
<p>192. Kingma, D., and Ba, J. (2014). Adam: A Method for Stochastic Optimization. Available at: <a href="http://arxiv.org/abs/1412.6980" class="uri">http://arxiv.org/abs/1412.6980</a>.</p>
</div>
<div id="ref-Chollet2015">
<p>209. Chollet, F. and others (2015). Keras. Available at: <a href="https://github.com/fchollet/keras" class="uri">https://github.com/fchollet/keras</a>.</p>
</div>
<div id="ref-Dieleman2015">
<p>210. Dieleman, S., Schlüter, J., Raffel, C., Olson, E., Sønderby, S.K., Nouri, D., Maturana, D., Thoma, M., Battenberg, E., and Kelly, J. <em>et al.</em> (2015). Lasagne: First release. Available at: <a href="https://zenodo.org/record/27878" class="uri">https://zenodo.org/record/27878</a>.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://contactpredictionthesis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="analysis-of-coupling-matrices.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularization-for-cd-with-sgd.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/susannvorberg/phd_thesis/edit/master/04-methods.Rmd",
"text": "Edit"
},
"download": ["PhD_thesis_Susann_Vorberg.pdf"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
