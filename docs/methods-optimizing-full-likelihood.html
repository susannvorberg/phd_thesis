<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>PhD thesis: residue-residue contact prediction</title>
  <meta name="description" content="This is my PhD thesis on residue-residue contact prediction.">
  <meta name="generator" content="bookdown 0.3.9 and GitBook 2.6.7">

  <meta property="og:title" content="PhD thesis: residue-residue contact prediction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my PhD thesis on residue-residue contact prediction." />
  <meta name="github-repo" content="susannvorberg/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PhD thesis: residue-residue contact prediction" />
  
  <meta name="twitter:description" content="This is my PhD thesis on residue-residue contact prediction." />
  

<meta name="author" content="Susann Vorberg">


<meta name="date" content="2017-10-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="analysis-of-coupling-matrices.html">
<link rel="next" href="bayesian-model-for-residue-resdiue-contact-prediction.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
  tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
  TeX: { 
    extensions: ["mediawiki-texvc.js", "sinuitx.js"],
    Macros: {
      Cb: "C_\\beta",
      eq: "\\!=\\!",
      Gauss: "\\mathcal{N}",
      H: "\\mathbf{H}",
      Hij : "\\H_{ij}",
      I: "\\mathbf{I}",
      Lijk: "\\mathbf{\\Lambda}_{ij,k}",
      Lk: "\\mathbf{\\Lambda}_k",
      LLreg: "L\\!L_\\mathrm{reg}",
      muijk: "\\mathbf{\\mu}_{ij,k}",
      muk: "\\mathbf{\\mu}_k",
      neff: "N_\\mathrm{eff}",
      r: "\\mathbf{r}",
      rij: "r_{ij}",
      seq: "\\mathbf{x}",
      Qij: "\\mathbf{Q}_{ij}",
      q: "\\mathbf{q}",
      qij: "\\mathbf{q'}_{ij}",
      Sn: "\\mathcal{S}_n",
      v: "\\mathbf{v}",
      vi: "\\mathcal{v}_{i}",
      vj: "\\mathcal{v}_{j}",
      via: "\\mathcal{v}_{ia}",
      w: "\\mathbf{w}",
      wij: "\\mathbf{w}_{ij}",
      wijab: "\\mathcal{w}_{ijab}",
      wijcd: "\\mathcal{w}_{ijcd}",
      wklcd: "\\mathcal{w}_{klcd}",
      X: "\\mathbf{X}",
      angstrom: "\\AA \\; \\;"
      }
  }
});
</script>


 
<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js", "[siunitx]/siunitx.js"],
  TeX: { TagSide: "left" }
});
MathJax.Ajax.config.path['siunitx']  = '../latex/MathJax-siunitx-master/';
</script>
//-->



<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD Thesis Susann Vorberg</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="general-intro.html"><a href="general-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="protein-structure.html"><a href="protein-structure.html"><i class="fa fa-check"></i><b>1.1</b> Protein Structure</a><ul>
<li class="chapter" data-level="1.1.1" data-path="protein-structure.html"><a href="protein-structure.html#amino-acid-interactions"><i class="fa fa-check"></i><b>1.1.1</b> Amino Acid Interactions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="structure-prediction.html"><a href="structure-prediction.html"><i class="fa fa-check"></i><b>1.2</b> Structure Prediction</a><ul>
<li class="chapter" data-level="1.2.1" data-path="structure-prediction.html"><a href="structure-prediction.html#template-based-methods"><i class="fa fa-check"></i><b>1.2.1</b> Template-based methods</a></li>
<li class="chapter" data-level="1.2.2" data-path="structure-prediction.html"><a href="structure-prediction.html#template-free-structure-prediction"><i class="fa fa-check"></i><b>1.2.2</b> Template-free structure prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html"><i class="fa fa-check"></i><b>2</b> Introduction to Contact Prediction</a><ul>
<li class="chapter" data-level="2.1" data-path="local-methods.html"><a href="local-methods.html"><i class="fa fa-check"></i><b>2.1</b> Local Statistical Models</a></li>
<li class="chapter" data-level="2.2" data-path="global-methods.html"><a href="global-methods.html"><i class="fa fa-check"></i><b>2.2</b> Global Statistical Models</a></li>
<li class="chapter" data-level="2.3" data-path="meta-predictors.html"><a href="meta-predictors.html"><i class="fa fa-check"></i><b>2.3</b> Machine Learning Methods and Meta-Predictors</a></li>
<li class="chapter" data-level="2.4" data-path="maxent.html"><a href="maxent.html"><i class="fa fa-check"></i><b>2.4</b> Modelling Protein Families with Potts Model</a><ul>
<li class="chapter" data-level="2.4.1" data-path="maxent.html"><a href="maxent.html#potts-model-properties"><i class="fa fa-check"></i><b>2.4.1</b> Model Properties</a></li>
<li class="chapter" data-level="2.4.2" data-path="maxent.html"><a href="maxent.html#potts-mle"><i class="fa fa-check"></i><b>2.4.2</b> Inferring Parameters for the Potts Model</a></li>
<li class="chapter" data-level="2.4.3" data-path="maxent.html"><a href="maxent.html#potts-model-solutions"><i class="fa fa-check"></i><b>2.4.3</b> Solving the Inverse Potts Problem</a></li>
<li class="chapter" data-level="2.4.4" data-path="maxent.html"><a href="maxent.html#post-processing-heuristics"><i class="fa fa-check"></i><b>2.4.4</b> Computing Contact Maps</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>2.5</b> Applications</a><ul>
<li class="chapter" data-level="2.5.1" data-path="applications.html"><a href="applications.html#contact-assisted-str-pred"><i class="fa fa-check"></i><b>2.5.1</b> contact assisted de-novo predictions</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html"><i class="fa fa-check"></i><b>2.6</b> Evaluating Contact Prediction Methods</a><ul>
<li class="chapter" data-level="2.6.1" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#seq-sep"><i class="fa fa-check"></i><b>2.6.1</b> Sequence Separation</a></li>
<li class="chapter" data-level="2.6.2" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#interpretation-of-evaluation-results"><i class="fa fa-check"></i><b>2.6.2</b> Interpretation of Evaluation Results</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>2.7</b> Challenges for Coevolutionary Inference</a><ul>
<li class="chapter" data-level="2.7.1" data-path="challenges.html"><a href="challenges.html#phylogenetic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>2.7.1</b> Phylogenetic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="2.7.2" data-path="challenges.html"><a href="challenges.html#entropic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>2.7.2</b> Entropic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="2.7.3" data-path="challenges.html"><a href="challenges.html#finite-sampling-effects"><i class="fa fa-check"></i><b>2.7.3</b> Finite Sampling Effects</a></li>
<li class="chapter" data-level="2.7.4" data-path="challenges.html"><a href="challenges.html#multiple-sequence-alignments"><i class="fa fa-check"></i><b>2.7.4</b> Multiple Sequence Alignments</a></li>
<li class="chapter" data-level="2.7.5" data-path="challenges.html"><a href="challenges.html#alternative-sources-of-coevolution"><i class="fa fa-check"></i><b>2.7.5</b> Alternative Sources of Coevolution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="developing-a-bayesian-model-for-contact-prediction.html"><a href="developing-a-bayesian-model-for-contact-prediction.html"><i class="fa fa-check"></i><b>3</b> Developing a Bayesian Model for Contact Prediction</a></li>
<li class="chapter" data-level="4" data-path="interpreting-coupling-matrices.html"><a href="interpreting-coupling-matrices.html"><i class="fa fa-check"></i><b>4</b> Interpretation of Coupling Matrices</a><ul>
<li class="chapter" data-level="4.1" data-path="correlation-between-couplings-and-class.html"><a href="correlation-between-couplings-and-class.html"><i class="fa fa-check"></i><b>4.1</b> Single Coupling Values Carry Evidence of Contacts</a></li>
<li class="chapter" data-level="4.2" data-path="physico-chemical-fingerprints-in-coupling-matrices.html"><a href="physico-chemical-fingerprints-in-coupling-matrices.html"><i class="fa fa-check"></i><b>4.2</b> Physico-Chemical Fingerprints in Coupling Matrices</a></li>
<li class="chapter" data-level="4.3" data-path="coupling-profiles-vary-with-distance.html"><a href="coupling-profiles-vary-with-distance.html"><i class="fa fa-check"></i><b>4.3</b> Coupling Profiles Vary with Distance</a></li>
<li class="chapter" data-level="4.4" data-path="higher-order-dependencies-between-couplings.html"><a href="higher-order-dependencies-between-couplings.html"><i class="fa fa-check"></i><b>4.4</b> Higher Order Dependencies Between Couplings</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="optimizing-full-likelihood.html"><a href="optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>5</b> Optimizing the Full-Likelihood</a><ul>
<li class="chapter" data-level="5.1" data-path="the-likelihood-of-the-sequences-as-a-potts-model.html"><a href="the-likelihood-of-the-sequences-as-a-potts-model.html"><i class="fa fa-check"></i><b>5.1</b> The Likelihood of the Sequences as a Potts Model</a></li>
<li class="chapter" data-level="5.2" data-path="gap-treatment.html"><a href="gap-treatment.html"><i class="fa fa-check"></i><b>5.2</b> Treating Gaps as Missing Information</a></li>
<li class="chapter" data-level="5.3" data-path="the-regularized-log-likelihood-and-its-gradient.html"><a href="the-regularized-log-likelihood-and-its-gradient.html"><i class="fa fa-check"></i><b>5.3</b> The Regularized Log Likelihood and its Gradient</a></li>
<li class="chapter" data-level="5.4" data-path="prior-v.html"><a href="prior-v.html"><i class="fa fa-check"></i><b>5.4</b> The prior on <span class="math inline">\(\v\)</span></a></li>
<li class="chapter" data-level="5.5" data-path="full-likelihood-gradient.html"><a href="full-likelihood-gradient.html"><i class="fa fa-check"></i><b>5.5</b> Approximation of the Gradient with Contrastive Divergence</a></li>
<li class="chapter" data-level="5.6" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html"><i class="fa fa-check"></i><b>5.6</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="5.6.1" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#full-likelihood-adam"><i class="fa fa-check"></i><b>5.6.1</b> Optimizing with ADAM</a></li>
<li class="chapter" data-level="5.6.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#full-likelihood-sgd"><i class="fa fa-check"></i><b>5.6.2</b> Optimizing with vanilla stochastic gradient descent</a></li>
<li class="chapter" data-level="5.6.3" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#altering-gibbs-sampling-schemes"><i class="fa fa-check"></i><b>5.6.3</b> Altering Gibbs sampling Schemes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><a href="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><i class="fa fa-check"></i><b>6</b> A Bayesian Statistical Model for Residue-Residue Contact Prediction</a><ul>
<li class="chapter" data-level="6.1" data-path="overview-posterior-distances.html"><a href="overview-posterior-distances.html"><i class="fa fa-check"></i><b>6.1</b> Computing the Posterior Distribution of Distances <span class="math inline">\(p(\r | \X)\)</span></a></li>
<li class="chapter" data-level="6.2" data-path="coupling-prior.html"><a href="coupling-prior.html"><i class="fa fa-check"></i><b>6.2</b> Modelling the prior over couplings with dependence on <span class="math inline">\(\rij\)</span></a></li>
<li class="chapter" data-level="6.3" data-path="laplace-approx.html"><a href="laplace-approx.html"><i class="fa fa-check"></i><b>6.3</b> Gaussian approximation to the posterior of couplings</a><ul>
<li class="chapter" data-level="6.3.1" data-path="laplace-approx.html"><a href="laplace-approx.html#laplace-approx-improvement"><i class="fa fa-check"></i><b>6.3.1</b> Iterative improvement of Laplace approximation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="likelihood-fct-distances.html"><a href="likelihood-fct-distances.html"><i class="fa fa-check"></i><b>6.4</b> Computing the likelihood function of distances <span class="math inline">\(p(\X | \r)\)</span></a></li>
<li class="chapter" data-level="6.5" data-path="posterior-of-rij.html"><a href="posterior-of-rij.html"><i class="fa fa-check"></i><b>6.5</b> The posterior probability distribution for <span class="math inline">\(\rij\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="contact-prior.html"><a href="contact-prior.html"><i class="fa fa-check"></i><b>7</b> Contact Prior</a><ul>
<li class="chapter" data-level="7.1" data-path="random-forest-classifiers.html"><a href="random-forest-classifiers.html"><i class="fa fa-check"></i><b>7.1</b> Random Forest Classifiers</a></li>
<li class="chapter" data-level="7.2" data-path="evaluating-random-forest-model-as-contact-predictor.html"><a href="evaluating-random-forest-model-as-contact-predictor.html"><i class="fa fa-check"></i><b>7.2</b> Evaluating Random Forest Model as Contact Predictor</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>8</b> Methods</a><ul>
<li class="chapter" data-level="8.1" data-path="dataset.html"><a href="dataset.html"><i class="fa fa-check"></i><b>8.1</b> Dataset</a></li>
<li class="chapter" data-level="8.2" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html"><i class="fa fa-check"></i><b>8.2</b> Optimizing Pseudo-Likelihood</a><ul>
<li class="chapter" data-level="8.2.1" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#pseudo-likelihood-objective-function-and-its-gradients"><i class="fa fa-check"></i><b>8.2.1</b> Pseudo-Likelihood Objective Function and its Gradients</a></li>
<li class="chapter" data-level="8.2.2" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#diff-ccmpred-ccmpredpy"><i class="fa fa-check"></i><b>8.2.2</b> Differences between CCMpred and CCMpredpy</a></li>
<li class="chapter" data-level="8.2.3" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#seq-reweighting"><i class="fa fa-check"></i><b>8.2.3</b> Sequence Reweighting</a></li>
<li class="chapter" data-level="8.2.4" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#amino-acid-frequencies"><i class="fa fa-check"></i><b>8.2.4</b> Computing Amino Acid Frequencies</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html"><i class="fa fa-check"></i><b>8.3</b> Analysis of Coupling Matrices</a><ul>
<li class="chapter" data-level="8.3.1" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-correlation"><i class="fa fa-check"></i><b>8.3.1</b> Correlation of Couplings with Contact Class</a></li>
<li class="chapter" data-level="8.3.2" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-profile"><i class="fa fa-check"></i><b>8.3.2</b> Coupling Distribution Plots</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>8.4</b> Optimizing the Full-Likelihood</a><ul>
<li class="chapter" data-level="8.4.1" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html#methods-full-likelihood-adam"><i class="fa fa-check"></i><b>8.4.1</b> Full Likelihood Optimization with <em>ADAM</em></a></li>
<li class="chapter" data-level="8.4.2" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html#full-likelihood-optimization-with-stochastic-gradient-descent"><i class="fa fa-check"></i><b>8.4.2</b> Full Likelihood Optimization with Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="8.4.3" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html#optimizing-regularization-coefficients-for-contrastive-divergence"><i class="fa fa-check"></i><b>8.4.3</b> Optimizing Regularization Coefficients for Contrastive Divergence</a></li>
<li class="chapter" data-level="8.4.4" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html#optimizing-the-sampling-scheme-for-contrastive-divergence"><i class="fa fa-check"></i><b>8.4.4</b> Optimizing the Sampling Scheme for Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html"><i class="fa fa-check"></i><b>8.5</b> Bayesian Model for Residue-Resdiue Contact Prediction</a><ul>
<li class="chapter" data-level="8.5.1" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#neg-Hessian-computation"><i class="fa fa-check"></i><b>8.5.1</b> Efficiently Computing the negative Hessian of the regularized log-likelihood</a></li>
<li class="chapter" data-level="8.5.2" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#inv-lambda-ij-k"><i class="fa fa-check"></i><b>8.5.2</b> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></a></li>
<li class="chapter" data-level="8.5.3" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#training-hyperparameters"><i class="fa fa-check"></i><b>8.5.3</b> Training the Hyperparameters <span class="math inline">\(\muk\)</span>, <span class="math inline">\(\Lk\)</span> and <span class="math inline">\(\gamma_k\)</span></a></li>
<li class="chapter" data-level="8.5.4" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#the-gradient-of-the-log-likelihood-with-respect-to-mathbfmu"><i class="fa fa-check"></i><b>8.5.4</b> The gradient of the log likelihood with respect to <span class="math inline">\(\mathbf{\mu}\)</span></a></li>
<li class="chapter" data-level="8.5.5" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#the-gradient-of-the-log-likelihood-with-respect-to-lk"><i class="fa fa-check"></i><b>8.5.5</b> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></a></li>
<li class="chapter" data-level="8.5.6" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#the-gradient-of-the-log-likelihood-with-respect-to-gamma_k"><i class="fa fa-check"></i><b>8.5.6</b> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><a href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><i class="fa fa-check"></i><b>8.6</b> Bayesian Statistical Model for Prediction of Protein Residue-Residue Distances</a><ul>
<li class="chapter" data-level="8.6.1" data-path="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><a href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html#modelling-the-dependence-of-wij-on-distance"><i class="fa fa-check"></i><b>8.6.1</b> Modelling the dependence of <span class="math inline">\(\wij\)</span> on distance</a></li>
<li class="chapter" data-level="8.6.2" data-path="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><a href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html#training-the-hyperparameters-rho_k-and-alpha_k-for-distance-dependent-prior"><i class="fa fa-check"></i><b>8.6.2</b> Training the Hyperparameters <span class="math inline">\(\rho_k\)</span> and <span class="math inline">\(\alpha_k\)</span> for distance-dependent prior</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html"><i class="fa fa-check"></i><b>8.7</b> Training Random Forest Contat Prior</a><ul>
<li class="chapter" data-level="8.7.1" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#seq-features"><i class="fa fa-check"></i><b>8.7.1</b> Sequence Derived Features</a></li>
<li class="chapter" data-level="8.7.2" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#rf-hyperparameter-optimization"><i class="fa fa-check"></i><b>8.7.2</b> Hyperparameter Optimization for Random Forest Prior</a></li>
<li class="chapter" data-level="8.7.3" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#rf-feature-selection"><i class="fa fa-check"></i><b>8.7.3</b> Feature Selection</a></li>
<li class="chapter" data-level="8.7.4" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#rf-with-pll-score"><i class="fa fa-check"></i><b>8.7.4</b> Using Pseudo-likelihood Coevolution Score as Additional Feature</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="abbrev.html"><a href="abbrev.html"><i class="fa fa-check"></i><b>A</b> Abbreviations</a><ul>
<li class="chapter" data-level="A.1" data-path="amino-acids.html"><a href="amino-acids.html"><i class="fa fa-check"></i><b>A.1</b> Amino Acid Alphabet</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="dataset-properties.html"><a href="dataset-properties.html"><i class="fa fa-check"></i><b>B</b> Dataset Properties</a><ul>
<li class="chapter" data-level="B.1" data-path="alignment-diversity.html"><a href="alignment-diversity.html"><i class="fa fa-check"></i><b>B.1</b> Alignment Diversity</a></li>
<li class="chapter" data-level="B.2" data-path="proportion-of-gaps-in-alignment.html"><a href="proportion-of-gaps-in-alignment.html"><i class="fa fa-check"></i><b>B.2</b> Proportion of Gaps in Alignment</a></li>
<li class="chapter" data-level="B.3" data-path="alignment-size-number-of-sequences.html"><a href="alignment-size-number-of-sequences.html"><i class="fa fa-check"></i><b>B.3</b> Alignment Size (number of sequences)</a></li>
<li class="chapter" data-level="B.4" data-path="protein-length.html"><a href="protein-length.html"><i class="fa fa-check"></i><b>B.4</b> Protein Length</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><a href="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><i class="fa fa-check"></i><b>C</b> Amino Acid Interaction Preferences Reflected in Coupling Matrices</a><ul>
<li class="chapter" data-level="C.1" data-path="pi-cation.html"><a href="pi-cation.html"><i class="fa fa-check"></i><b>C.1</b> Pi-Cation interactions</a></li>
<li class="chapter" data-level="C.2" data-path="disulfide.html"><a href="disulfide.html"><i class="fa fa-check"></i><b>C.2</b> Disulfide Bonds</a></li>
<li class="chapter" data-level="C.3" data-path="aromatic-proline.html"><a href="aromatic-proline.html"><i class="fa fa-check"></i><b>C.3</b> Aromatic-Proline Interactions</a></li>
<li class="chapter" data-level="C.4" data-path="aromatic-network.html"><a href="aromatic-network.html"><i class="fa fa-check"></i><b>C.4</b> Network-like structure of aromatic residues</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="optimizing-full-likelihood-with-gradient-descent.html"><a href="optimizing-full-likelihood-with-gradient-descent.html"><i class="fa fa-check"></i><b>D</b> Optimizing Full Likelihood with Gradient Descent</a><ul>
<li class="chapter" data-level="D.1" data-path="visualisation-of-learning-rate-schedules.html"><a href="visualisation-of-learning-rate-schedules.html"><i class="fa fa-check"></i><b>D.1</b> Visualisation of learning rate schedules</a></li>
<li class="chapter" data-level="D.2" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html"><i class="fa fa-check"></i><b>D.2</b> Benchmarking learning rate schedules</a><ul>
<li class="chapter" data-level="D.2.1" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#linear-learning-rate-schedule"><i class="fa fa-check"></i><b>D.2.1</b> Linear learning rate schedule</a></li>
<li class="chapter" data-level="D.2.2" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#sigmoidal-learning-rate-schedule"><i class="fa fa-check"></i><b>D.2.2</b> Sigmoidal learning rate schedule</a></li>
<li class="chapter" data-level="D.2.3" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#square-root-learning-rate-schedule"><i class="fa fa-check"></i><b>D.2.3</b> Square root learning rate schedule</a></li>
<li class="chapter" data-level="D.2.4" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#exponential-learning-rate-schedule"><i class="fa fa-check"></i><b>D.2.4</b> Exponential learning rate schedule</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html"><i class="fa fa-check"></i><b>D.3</b> Number of iterations until convergence for different learning rate schedules</a><ul>
<li class="chapter" data-level="D.3.1" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#linear-learning-rate-schedule-1"><i class="fa fa-check"></i><b>D.3.1</b> Linear learning rate schedule</a></li>
<li class="chapter" data-level="D.3.2" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#sigmoidal-learning-rate-schedule-1"><i class="fa fa-check"></i><b>D.3.2</b> Sigmoidal learning rate schedule</a></li>
<li class="chapter" data-level="D.3.3" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#square-root-learning-rate-schedule-1"><i class="fa fa-check"></i><b>D.3.3</b> Square root learning rate schedule</a></li>
<li class="chapter" data-level="D.3.4" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#exponential-learning-rate-schedule-1"><i class="fa fa-check"></i><b>D.3.4</b> Exponential learning rate schedule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="E" data-path="training-of-the-random-forest-contact-prior.html"><a href="training-of-the-random-forest-contact-prior.html"><i class="fa fa-check"></i><b>E</b> Training of the Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="E.1" data-path="rf-window-size.html"><a href="rf-window-size.html"><i class="fa fa-check"></i><b>E.1</b> Evaluating window size with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="E.2" data-path="rf-noncontact-threshold.html"><a href="rf-noncontact-threshold.html"><i class="fa fa-check"></i><b>E.2</b> Evaluating non-contact threshold with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="E.3" data-path="rf-ratio-noncontacts.html"><a href="rf-ratio-noncontacts.html"><i class="fa fa-check"></i><b>E.3</b> Evaluating ratio of non-contacts and contacts in the training set with 5-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PhD thesis: residue-residue contact prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="methods-optimizing-full-likelihood" class="section level2">
<h2><span class="header-section-number">8.4</span> Optimizing the Full-Likelihood</h2>
<p>The following sections will describe the hyperparameter tuning for the stochastic gradient descent optimization as well as tuning different aspects of the Gibbs sampling scheme used to approximate the gradient with <a href="abbrev.html#abbrev">CD</a>.</p>
<p>In theory the algorithm has converged and the optimum of the objective function has been reached when the gradient becomes zero. In practice the gradients will never be exactly zero, especially due to the stochasticity of the gradient estimates when using stochastic gradient descent.</p>
<p>For this reason, usually some kind of convergence criterion is designed and convergence is assumed whenever the criterion is met. A common criterion is the when the relative change of objective function value between iterations is close to zero. However, because the evaluation of the full likelihood function is too expensive, the function value cannot be used to define a criterion.</p>
<p>Another possibility is to stop learning when the norm of the gradient is close to zero <span class="citation">[<a href="#ref-Carreira-Perpinan2005">149</a>]</span>. For <a href="abbrev.html#abbrev">CD</a> however, the gradients will be far off zero depending on how many sequences are used for sampling. Only when sampling large number of sequences, the gradients will eventually be close to zero (see section @ref(sampling more seq)). This is however achieved at the expense of runtine which increases linearly in the number of sequences for sampling.</p>
<p>An alternative is to check the relative change over the norm of gradients between iterations and stop the algorithm whenever it falls below a small threshold <span class="math inline">\(\epsilon\)</span>,</p>
<span class="math display" id="eq:gradient-convergence-criterion">\[\begin{equation}
  \frac{|\nabla_\theta f(\theta_{t-1}) - \nabla_\theta f(\theta_{t})|}{|\nabla_\theta f(\theta_{t-1})|}  &lt; \; \epsilon \; .
  \tag{8.5}
\end{equation}\]</span>
<p>However, as gradient estimates are very noisy for stochastic gradient descent, gradient fluctuations complicate the proper assessment of this criterion. It is also possible to monitor the relative change over the norm of parameter estimates between iterations,</p>
<span class="math display" id="eq:parameter-convergence-criterion">\[\begin{equation}
  \frac{|\theta_{t-1} - \theta_t|}{|\theta_t|}  &lt; \; \epsilon  \; .
  \tag{8.6}
\end{equation}\]</span>
<p>This criterion is more stable as parameter updates are dampened by the step size are not quite as noisy compared to subsequent gradient estimates.</p>
<p>Another idea is to monitor the direction of gradients. Once getting close to the optimum, gradients will start to fluctuate as the optimizer will oscillate around the true optimum. From my experience, it is hard to find a general threshold that applies to all proteins, as every protein reflects a problem of varying complexity (number of parameters scales with <span class="math inline">\(L^2\)</span>, L being protein length). Furthermore this analysis is complicated when using momentum, as the computed gradient is not actually used to change the parameters but it is combined with previous gradient estimates.</p>
<p>A necessary but not sufficient criterion for convergence is given by <span class="math inline">\(\sum_{a,b=1}^{20} \wijab = 0\)</span> as derived in section <a href="prior-v.html#prior-v">5.4</a>. When using plain stochastic gradient descent without momentum and without adaptive learning rates, this criterion is never violated when parameters are initialized uniformly. This is due to the fact that the 400 gradients <span class="math inline">\(\wijab\)</span> for <span class="math inline">\(a,b \in \{1, \ldots, 20\}\)</span> are not independent. The sum over the 400 pairwise amino acid counts for two positions i and j is identical for the observed and sampled alignment,</p>
<span class="math display">\[\begin{equation}
  \sum_{a,b=1}^{20} N_{ij} q(x_i \eq a, q_j \eq b) = N_{ij} \; ,
\end{equation}\]</span>
<p>and therefore the gradients, computed as the difference of pairwise counts between observed and sampled alignment, are symmetrical. Considering residue pair (i,j) and assuming amino acid pair (a,b) has higher counts in the sampled alignment compared to the true alignment, then this differnce in counts must be compensated by other amino acid pairs (c,d) having less counts in the sampled alignment compared to the true alignment. This symmetry is translated into parameter updates when the same learning rate is used to update all parameters. However, when using adaptive learning rates, this symmetry is broken and the condition <span class="math inline">\(\sum_{a,b=1}^{20} \wijab = 0\)</span> can be violated during the optimization processs. It is therefore interesting to monitor <span class="math inline">\(\sum_{1 \le 1 &lt; j \le L} \sum_{a,b=1}^{20} \wijab\)</span>.</p>
<p>Finally, the simplest strategy is to specify a maximum number of iterations for the optimization procedure. This also ensures that the algorithm will stop eventually if none of the other convergence criteria is met.</p>
<p>In the following, I will set the maximum number of iterations to 5000 and I will stop the optimization when the relative change over the norm of parameter estimates falls below the threshold of <span class="math inline">\(\epsilon = 1e-8\)</span>. Furthermore, I will follow the pragmatic standard strategy and run the optimization algorithm with many hyperparameter settings and pick the model that gives the best performance on a validation set <span class="citation">[<a href="#ref-Le2011">150</a>]</span>.</p>
<p>The performance will be evaluated as the mean precision of the top ranked contact predictions over a benchmark set of 300 proteins, that is a subset of the data set described in methods section <a href="dataset.html#dataset">8.1</a>. Pseudo-likelihood couplings are computed with the tool CCMpredPy that is introduced in methods section <a href="optimizing-pseudo-likelihood.html#diff-ccmpred-ccmpredpy">8.2.2</a>. Contact scores for couplings obtained with pseudo-likelihood and <a href="abbrev.html#abbrev">CD</a> are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm as explained in section <a href="maxent.html#post-processing-heuristics">2.4.4</a>.</p>
<p>Coupling parameters are initialized at 0.</p>
<div id="methods-full-likelihood-adam" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Full Likelihood Optimization with <em>ADAM</em></h3>
<p><em>ADAM</em> <span class="citation">[<a href="#ref-Kingma2014">136</a>]</span> stores an exponentially decaying average of past gradients and squared gradients,</p>
<span class="math display">\[\begin{align}
  m_t &amp;= \beta_1 m_{t−1} + (1 − \beta_1) g \\
  v_t &amp;= \beta_2 v_{t−1} + (1 − \beta_2) g^2 \; ,
\end{align}\]</span>
<p>with <span class="math inline">\(g = \nabla_w \LLreg(\v,\w)\)</span> and the rate of decay being determined by hyperparameters <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>. Both terms <span class="math inline">\(m_t\)</span> and <span class="math inline">\(v_t\)</span> represent estimates of the first and second moments of the gradient, respectively.</p>
<p>The full Adam update also includes a bias correction mechanism, which compensates for the fact that in the first few time steps the vectors m,v are both initialized and therefore biased at zero, before they fully “warm up”. Because <span class="math inline">\(m_t\)</span> and <span class="math inline">\(v_t\)</span> are initialized as vectors of zeros, the following bias correction terms are supposed to counteract the initialization bias towards zero,</p>
<span class="math display">\[\begin{align}
  \hat{m_t} &amp;= \frac{m_t}{1-\beta_1^t} \\
  \hat{v_t} &amp;= \frac{v_t}{1-\beta_2^t} \; .
\end{align}\]</span>
Parameters are then updated using step size <span class="math inline">\(\alpha\)</span>, a small noise term <span class="math inline">\(\epsilon\)</span> and the corrected moment estimates <span class="math inline">\(\hat{m_t}\)</span>, <span class="math inline">\(\hat{v_t}\)</span>, according to
<span class="math display">\[\begin{equation}
  x_{t+1} = x_t - \alpha \cdot \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}
\end{equation}\]</span>
<p>Kingma et al. proposed the default values <span class="math inline">\(\beta_1=0.9\)</span>, <span class="math inline">\(\beta_2=0.999\)</span> and <span class="math inline">\(\epsilon=1e−8\)</span> and a constant learning rate <span class="math inline">\(\alpha_0=1e-3\)</span>, because <em>ADAM</em> performs a kind of step size annealing by nature. However, popular implementations of <em>ADAM</em> in the <a href="https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L385">Keras</a> <span class="citation">[<a href="#ref-Chollet2015">151</a>]</span> and <a href="https://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py#L565-L629">Lasagne</a> <span class="citation">[<a href="#ref-Dieleman2015">152</a>]</span> packages allow the use of a linear annealing schedule for the learning rate <span class="math inline">\(\alpha\)</span> of the form <span class="math inline">\(\alpha = \frac{\alpha_0}{1 + \gamma t}\)</span>, for an initial learning rate <span class="math inline">\(\alpha_0\)</span>, decay rate <span class="math inline">\(\gamma\)</span> and timestep <span class="math inline">\(t\)</span>.</p>
<p>I first tested three different constant learning rates <span class="math inline">\(\alpha \in \{1e-4, 1e-3, 5e-3\}\)</span> and default parameters <span class="math inline">\(\beta_1=0.9\)</span>, <span class="math inline">\(\beta_2=0.999\)</span> and <span class="math inline">\(\epsilon=1e−8\)</span>.</p>
<p>PLOT</p>
<p>As can be seen in the performance: not goot. Looking at individual proteins:</p>
</div>
<div id="full-likelihood-optimization-with-stochastic-gradient-descent" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Full Likelihood Optimization with Stochastic Gradient Descent</h3>
<p>The coupling parameters <span class="math inline">\(\w\)</span> will be updated in each iteration <span class="math inline">\(t\)</span> by taking a step of size <span class="math inline">\(\alpha\)</span> along the direction of the negative gradient of the regularized full log likelihood <span class="math inline">\(- \nabla_w \LLreg(\v,\w)\)</span> that has been approximated with <a href="abbrev.html#abbrev">CD</a>,</p>
<span class="math display">\[\begin{equation}
  \w_{t+1} = \w_t - \alpha \cdot \nabla_w \LLreg(\v,\w) \; .
\end{equation}\]</span>
<p>In order to get a first intuition of the optimization problem, I tested initial learning rates <span class="math inline">\(\alpha_0 \in \{1\mathrm{e}{-4}, 5\mathrm{e}{-4}, 1\mathrm{e}{-3}, 5\mathrm{e}{-3}\}\)</span> with a standard learning rate annealing schedule, <span class="math inline">\(\alpha = \frac{\alpha_0}{1 + \gamma \cdot t}\)</span> with decay rate <span class="math inline">\(\gamma=0.01\)</span> and timestep <span class="math inline">\(t\)</span> <span class="citation">[<a href="#ref-Bottou2012">133</a>]</span>.</p>
<p>Figure <a href="methods-optimizing-full-likelihood.html#fig:performance-cd-alphaopt">8.3</a> shows the mean precision for top ranked contacts computed from pseudo-likelihood couplings and the <a href="abbrev.html#abbrev">CD</a> couplings optimized with stochastic gradient descent using the four different learning rates. Overall, mean precision for <a href="abbrev.html#abbrev">CD</a> contacts is smaller than for pseudo-likelihood contacts. Especially the smallest (<span class="math inline">\(1\mathrm{e}{-4}\)</span>) and biggest learning (<span class="math inline">\(5\mathrm{e}{-3}\)</span>) rate perform bad.</p>

<div class="figure" style="text-align: center"><span id="fig:performance-cd-alphaopt"></span>
<iframe src="img/full_likelihood/sgd/precision_vs_rank_learning_rates.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 8.3: Mean precision for top ranked contact predictions over 286 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. pseudo-likelihood: Contact scores computed from pseudo-likelihood. The other methods derive contact scores from couplings computed from <a href="abbrev.html#abbrev">CD</a> using stochastic gradient descent with different initial learning rates <span class="math inline">\(\alpha_0\)</span> as specified in the legend.
</p>
</div>
<p>Looking at individual proteins it turns out that the optimal learning rate depends on alignment size. Figure <a href="methods-optimizing-full-likelihood.html#fig:sgd-single-proteins-initial-learning-rate">8.4</a> shows convergence plots for two different proteins, where convergence is measured as the L2-norm of coupling parameters <span class="math inline">\(||\w||_2\)</span>. For an exemplary protein with a small alignment (left plot in Figure <a href="methods-optimizing-full-likelihood.html#fig:sgd-single-proteins-initial-learning-rate">8.4</a>), a small learning rate <span class="math inline">\(1\mathrm{e}{-4}\)</span> lets the optimization run very slowly and not reach convergence within 5000 iterations. A large learning rate of <span class="math inline">\(5\mathrm{e}{-3}\)</span> overshoots the optimum at the beginning of the optimization but as the learning rate decays over time the parameter estimates converge. In contrast, for a protein with a big alignment (right plot in Figure <a href="methods-optimizing-full-likelihood.html#fig:sgd-single-proteins-initial-learning-rate">8.4</a>) the learning rate choice has a more pronounced effect. With a small initial learning rate of <span class="math inline">\(1\mathrm{e}{-4}\)</span> the optimization runs slowly but almost converges within 5000 iterations. A large initial learning rate of <span class="math inline">\(5\mathrm{e}{-3}\)</span> lets the parameters diverge quickly and the optimum cannot be revovered. With learning rates <span class="math inline">\(5\mathrm{e}{-4}\)</span> and <span class="math inline">\(1\mathrm{e}{-3}\)</span>, the optimum is well overshot at the beginning of the optimization but the parameter estimates eventually converge as the learning rates decreases over time.</p>
<p>These observations are explained by the fact that the magnitude of the gradient scales with the number of sequences in the alignment. Because the gradient is computed from amino acid counts (see section <a href="full-likelihood-gradient.html#full-likelihood-gradient">5.5</a>), alignments with many sequences will generally produce larger gradients compared to alignments with few sequences, especially at the beginning of the optimization procedure when the difference in amino acid counts between sampled and observed sequences is largest. Following these observation,s I defined the initial learning rate <span class="math inline">\(\alpha_0\)</span> as a function of <a href="abbrev.html#abbrev">Neff</a>.<br />
Aiming to obtain values for <span class="math inline">\(\alpha_0\)</span> around 5e-3 for small <a href="abbrev.html#abbrev">Neff</a> and values for <span class="math inline">\(\alpha_0\)</span> around 1e-4 for large <a href="abbrev.html#abbrev">Neff</a>, the initial learning rate is defined as</p>
<span class="math display" id="eq:learning-rate-wrt-neff">\[\begin{equation}
  \alpha_0 = \frac{5\mathrm{e}{-2}}{\sqrt{N_{\text{eff}}}} \; .
  \tag{8.7}
\end{equation}\]</span>
<p>For small <a href="abbrev.html#abbrev">Neff</a> <span class="math inline">\(\approx 50\)</span> this yields <span class="math inline">\(\alpha_0 \approx 7\mathrm{e}{-3}\)</span> and for big <a href="abbrev.html#abbrev">Neff</a> <span class="math inline">\(\approx 20000\)</span> this yields <span class="math inline">\(\alpha_0 \approx 3.5\mathrm{e}{-4}\)</span>. Using this learning rate defined as a function of <a href="#bbrev">Neff</a>, precision improves over the previous fixed learning rates (see Figure <a href="methods-optimizing-full-likelihood.html#fig:performance-cd-alphaopt">8.3</a>). All following analyses are conducted using the <a href="abbrev.html#abbrev">Neff</a> dependent learning rate.</p>

<div class="figure" style="text-align: center"><span id="fig:sgd-single-proteins-initial-learning-rate"></span>
<img src="img/full_likelihood/sgd/parameter_norm_1mkca00_alphas_lindecay001.png" alt="L2-norm of the coupling parameters \(||\w||_2\) during stochastic gradient descent optimization with different learning rates. Linear learning rate annealing schedule has been used with decay rate \(\gamma=0.01\) and initial learning rates \(\alpha_0\) as stated in the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808). Figure is cut at the yaxis at \(||\w||_2=1500\), but learning rate of \(5\mathrm{e}{-3}\) reaches \(||\w||_2 \approx 13000\)." width="50%" /><img src="img/full_likelihood/sgd/parameter_norm_1c75a00_alphas_lindecay001.png" alt="L2-norm of the coupling parameters \(||\w||_2\) during stochastic gradient descent optimization with different learning rates. Linear learning rate annealing schedule has been used with decay rate \(\gamma=0.01\) and initial learning rates \(\alpha_0\) as stated in the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808). Figure is cut at the yaxis at \(||\w||_2=1500\), but learning rate of \(5\mathrm{e}{-3}\) reaches \(||\w||_2 \approx 13000\)." width="50%" />
<p class="caption">
Figure 8.4: L2-norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span> during stochastic gradient descent optimization with different learning rates. Linear learning rate annealing schedule has been used with decay rate <span class="math inline">\(\gamma=0.01\)</span> and initial learning rates <span class="math inline">\(\alpha_0\)</span> as stated in the legend. <strong>Left</strong> Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=96). <strong>Right</strong> Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=16808). Figure is cut at the yaxis at <span class="math inline">\(||\w||_2=1500\)</span>, but learning rate of <span class="math inline">\(5\mathrm{e}{-3}\)</span> reaches <span class="math inline">\(||\w||_2 \approx 13000\)</span>.
</p>
</div>
<p>In a next step, I tried to find an optimal learning rate annealing schedule and an optimal decay rate. I evaluated the following learning rate schedules and decay rates using the <a href="abbrev.html#abbrev">Neff</a> dependent definition of the initial learning given in eq. <a href="methods-optimizing-full-likelihood.html#eq:learning-rate-wrt-neff">(8.7)</a>:</p>
<ul>
<li>default linear learning rate schedule <span class="math inline">\(\alpha = \frac{\alpha_0}{1 + \gamma t}\)</span> with <span class="math inline">\(\gamma \in \{1\mathrm{e}{-3}, 1\mathrm{e}{-2}, 1\mathrm{e}{-1}, 1 \}\)</span></li>
<li>square root learning rate schedule <span class="math inline">\(\alpha = \frac{\alpha_0}{\sqrt{1 + \gamma t}}\)</span> with <span class="math inline">\(\gamma \in \{1\mathrm{e}{-2}, 1\mathrm{e}{-1}, 1 \}\)</span></li>
<li>sigmoidal learning rate schedule <span class="math inline">\(\alpha_{t+1} = \frac{\alpha_{t}}{1 + \gamma t}\)</span> with <span class="math inline">\(\gamma \in \{1\mathrm{e}{-6}, 1\mathrm{e}{-5}, 1\mathrm{e}{-4}, 1\mathrm{e}{-3}\}\)</span></li>
<li>exponential learning rate schedule <span class="math inline">\(\alpha_{t+1} = \alpha_0 \cdot\exp(- \gamma t)\)</span> with <span class="math inline">\(\gamma \in \{5\mathrm{e}{-4}, 1\mathrm{e}{-4}, 5\mathrm{e}{-3}\}\)</span></li>
</ul>
<p>The different learning rate annealing schedules and decay rates are visualized in Appendix Figure <a href="visualisation-of-learning-rate-schedules.html#fig:learning-rate-schedules">D.1</a> and the respective benchmark plots can be found in Appendix <a href="benchmark-learning-rate-annealing-schedules.html#benchmark-learning-rate-annealing-schedules">D.2</a>. None of the learning rate schedules achieves an average precision for the top ranked contacts that is comparable to the pseudo-likelihood score. The highest precision, being one to two percentage points below the pseudo-likelihood score, is obtained with a linear learning rate schedule and decay rate <span class="math inline">\(\gamma \eq 1\mathrm{e}{-2}\)</span> and with a sigmoidal learning rate schedule and decay rates <span class="math inline">\(\gamma \eq 1\mathrm{e}{-5}\)</span> and <span class="math inline">\(\gamma \eq 1\mathrm{e}{-6}\)</span>. The square root learning rate schedule gives ovarally bad results and does not lead to convergence because the learning rate decays slowly at later time steps. The exp decay rate TODOOOO.</p>
<p>In contrast to the findings regarding the initial learning rate earlier, an optimal decay rate is independent of the alignment size. Figure <a href="methods-optimizing-full-likelihood.html#fig:sgd-single-proteins-learning-rate-schedule">8.5</a> shows convergence plots for the same two exemplary proteins with small and big alignments as in Figure <a href="methods-optimizing-full-likelihood.html#fig:sgd-single-proteins-initial-learning-rate">8.4</a>. For proteins with small <a href="abbrev.html#abbrev">Neff</a>, a quickly decaying learning rate such as <span class="math inline">\(\gamma \eq 1\mathrm{e}{-1}\)</span> with a linear schedule or <span class="math inline">\(\gamma \eq 1\mathrm{e}{-4}\)</span> with a sigmoidal schedule guide the optimization closely to the presumed optimum at <span class="math inline">\(||w||_2 \approx 12.5\)</span>, as can be seen in the left plot in Figure <a href="methods-optimizing-full-likelihood.html#fig:sgd-single-proteins-learning-rate-schedule">8.5</a>. Proteins with big <a href="abbrev.html#abbrev">Neff</a> are generally stronger adversely affected by quickly decaying learning rates and the optimum at <span class="math inline">\(||w||_2 \approx 125\)</span> is not reached before the learning rate diminishes, effectively preventing further optimization progress. Learning rates decaying less quickly, such as <span class="math inline">\(\gamma \eq 1\mathrm{e}{-2}\)</span> with a linear schedule or <span class="math inline">\(\gamma \eq 1\mathrm{e}{-6}\)</span> with a sigmoidal schedule, guide the parameter estimates close to the expected optimum, both for proteins with small and big <a href="abbrev.html#abbrev">Neff</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:sgd-single-proteins-learning-rate-schedule"></span>
<img src="img/full_likelihood/sgd/parameter_norm_1mkca00_alpha0_different_schedules.png" alt="L2-norm of the coupling parameters \(||\w||_2\) during stochastic gradient descent optimization with different learning rates schedules. The initial learning rate \(\alpha_0\) is defined with respect to Neff as given in eq. (8.7). Learning rate schedules and decay rates are used according to the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808)." width="50%" /><img src="img/full_likelihood/sgd/parameter_norm_1c75a00_alpha0_different_schedules.png" alt="L2-norm of the coupling parameters \(||\w||_2\) during stochastic gradient descent optimization with different learning rates schedules. The initial learning rate \(\alpha_0\) is defined with respect to Neff as given in eq. (8.7). Learning rate schedules and decay rates are used according to the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808)." width="50%" />
<p class="caption">
Figure 8.5: L2-norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span> during stochastic gradient descent optimization with different learning rates schedules. The initial learning rate <span class="math inline">\(\alpha_0\)</span> is defined with respect to <a href="abbrev.html#abbrev">Neff</a> as given in eq. <a href="methods-optimizing-full-likelihood.html#eq:learning-rate-wrt-neff">(8.7)</a>. Learning rate schedules and decay rates are used according to the legend. <strong>Left</strong> Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=96). <strong>Right</strong> Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=16808).
</p>
</div>
<p>While the three learning rate choices, linear learning rate schedule with decay rate <span class="math inline">\(\gamma \eq 1\mathrm{e}{-2}\)</span> and sigmoidal learning rate schedule with decay rates <span class="math inline">\(\gamma \eq 1\mathrm{e}{-5}\)</span> and <span class="math inline">\(\gamma \eq 1\mathrm{e}{-6}\)</span>, give almost identical precision in the benchmarks shown in Appendix <a href="benchmark-learning-rate-annealing-schedules.html#benchmark-learning-rate-annealing-schedules">D.2</a>, they differ in their average rate until convergence. Figure <a href="methods-optimizing-full-likelihood.html#fig:distribution-num-iterations">8.6</a> shows the distribution over the number of iterations until convergence for all three choices of learning rate schedules. Optimization converges on average within 3584 iterations with a sigmoidal schedule and <span class="math inline">\(\gamma \eq 1\mathrm{e}{-6}\)</span> and within 1356 iterations when setting the decay rate <span class="math inline">\(\gamma \eq 1\mathrm{e}{-5}\)</span>. On the contrary, when using a linear learning rate schedule with <span class="math inline">\(\gamma \eq 1\mathrm{e}{-2}\)</span>, optimization converges on average within 4276 iterations but the distribution has a median of 5000. Under these considerations, I chose a sigmoidal learning rate schedule with <span class="math inline">\(\gamma \eq 5\mathrm{e}{-6}\)</span> for all further analysis.</p>

<div class="figure" style="text-align: center"><span id="fig:distribution-num-iterations"></span>
<iframe src="img/full_likelihood/sgd/distribution_numiterations_against_selected_learningrate_schedules.html" width="100%" height="500px">
</iframe>
<p class="caption">
Figure 8.6: Distribution of the number of iterations until convergence for <a href="abbrev.html#abbrev">SGD</a> optimizations of the full likelihood for different learning rate schedules. Convergence is reached when the relative difference of parameter norms <span class="math inline">\(||\w||_2\)</span> falls below <span class="math inline">\(\epsilon \eq 1e-8\)</span>. Initial learning rate <span class="math inline">\(\alpha_0\)</span> is defined with respect to <a href="abbrev.html#abbrev">Neff</a> as given in eq. <a href="methods-optimizing-full-likelihood.html#eq:learning-rate-wrt-neff">(8.7)</a> and maximum number of iterations is set to 5000. Learning rate schedules and decay rates are used according to the legend.
</p>
</div>
</div>
<div id="optimizing-regularization-coefficients-for-contrastive-divergence" class="section level3">
<h3><span class="header-section-number">8.4.3</span> Optimizing Regularization Coefficients for Contrastive Divergence</h3>
<p>Gaussian priors are put on the single potentials <span class="math inline">\(\v\)</span> and the couplings <span class="math inline">\(\w\)</span> when optimizing the full likelihood with <a href="abbrev.html#abbrev">CD</a> just as it is done for pseudo-likelihood optimization (compare section <a href="maxent.html#pseudo-likelihood">2.4.3.1</a>). A difference compared to pseudo-likelihood optimization that uses zero centered priors, is the centering of the Gaussian prior for the single potentials <span class="math inline">\(\v\)</span> at <span class="math inline">\(v^*\)</span> as described in section <a href="prior-v.html#prior-v">5.4</a>. The hyperparameter tuning for stochastic gradient descent described in the last section applied the default pseudo-likelihood regularization coefficients <span class="math inline">\(\lambda_v \eq 10\)</span> and <span class="math inline">\(\lambda_w \eq 0.2\cdot(L-1)\)</span>. The regularization coefficient <span class="math inline">\(\lambda_w\)</span> for couplings <span class="math inline">\(\w\)</span> is defined with respect to protein length <span class="math inline">\(L\)</span> owing to the fact that the number of possible contacts in a protein increases quadratically with <span class="math inline">\(L\)</span> whereas the number of observed contacts only increases linearly as can be seen in Figure <a href="methods-optimizing-full-likelihood.html#fig:number-contacts-against-L">8.7</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:number-contacts-against-L"></span>
<iframe src="img/full_likelihood/no_contacts_vs_protein_length_thr8.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 8.7: Number of contacts (<span class="math inline">\(\Cb &lt; 8 \angstrom\)</span>) with respect to protein length and sequence separation has a linear relationship.
</p>
</div>
<p>It is possible that <a href="abbrev.html#abbrev">CD</a> achieves optimal performance using stronger or weaker regularization coefficients compared to pseudo-likelihood. Therefore, I evaluated performance of contrastive divergence using different regularization coefficients <span class="math inline">\(\lambda_w \in \{ 1\mathrm{e}{-2}, 1\mathrm{e}{-1}, 0.2, 1\} \cdot(L-1)\)</span> while leaving the regularization for single potentials at the default value <span class="math inline">\(\lambda_v \eq 10\)</span>. Furthermore, I analysed whether precision is impacted by only optimizing the couplings <span class="math inline">\(\w\)</span> (with default regularization) while fixing the single potentials <span class="math inline">\(\vi\)</span> to their best estimates <span class="math inline">\(\vi^*\)</span> as described in section <a href="prior-v.html#prior-v">5.4</a>.</p>
<p>As can be seen in Figure <a href="methods-optimizing-full-likelihood.html#fig:precison-cd-regularization">8.8</a>, using strong regularization for the couplings <span class="math inline">\(\lambda_w \eq (L-1)\)</span> results in a drop of mean precision. Using weaker regularization or fixing the single potentials hardly has an impact on precision with using <span class="math inline">\(\lambda_w \eq 1\mathrm{e}{-2}(L-1)\)</span> yielding slightly improved precision for the top ranked contacts.</p>

<div class="figure" style="text-align: center"><span id="fig:precison-cd-regularization"></span>
<iframe src="img/full_likelihood/precision_vs_rank_notitle_cd_comparing_regularizers.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 8.8: Performance of contrastive divergence optimization of the full likelihood with different regularization settings compared to pseudo-likelihood (blue) for 280 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. Default regularization coefficients as used with pseudo-likelihood are <span class="math inline">\(\lambda_v \eq 10\)</span> and <span class="math inline">\(\lambda_w \eq 0.2(L-1)\)</span>. “fixed vi” (orange) uses <a href="abbrev.html#abbrev">CD</a> to optimize only couplings with default regularization while keeping the single potentials <span class="math inline">\(\vi\)</span> fixed at their <a href="abbrev.html#abbrev">MLE</a> optimum <span class="math inline">\(\vi^*\)</span>. The other optimization runs with <a href="abbrev.html#abbrev">CD</a> (green, red, purple, brown) use default regularization for the single potentials and a regularization coefficient for the couplings according to legend description.
</p>
</div>
</div>
<div id="optimizing-the-sampling-scheme-for-contrastive-divergence" class="section level3">
<h3><span class="header-section-number">8.4.4</span> Optimizing the Sampling Scheme for Contrastive Divergence</h3>
<p>I analysed whether choosing a different number of sequences for the approximation of the gradient via Gibbs sampling can improve performance. Randomly selecting only a subset <span class="math inline">\(N^{\prime}\)</span> of the <span class="math inline">\(N\)</span> observed sequences corresponds to the stochastic gradient descent idea of a minibatch and introduces additional stochasticity over the random Gibbs sampling process. Using <span class="math inline">\(N^{\prime} &lt; N\)</span> sequences for Gibbs sampling has the further advantage of decreasing the runtime at each iteration. Note, that the reference counts from the observed sequences <span class="math inline">\(N_{ij} q(x_i \eq a, x_j \eq b)\)</span> that are part of the gradient calculation will be kept constant. Therefore it is necessary, however to rescale the amino acid counts from the sampled sequences in a way such that the total sample counts match the total observed counts.</p>
<p>I evaluated two different schemes for randomly selecting <span class="math inline">\(N^{\prime} \eq xL\)</span> sequences from the <span class="math inline">\(N\)</span> given sequences of the alignment at every iteration:</p>
<ul>
<li><strong>without</strong> replacement (enforcing <span class="math inline">\(N^{\prime} \eq \min(N, xL)\)</span>)</li>
<li><strong>with</strong> replacement</li>
</ul>
<p>with <span class="math inline">\(x \in \{ 1, 5, 10, 50 \}\)</span>.</p>
<p>As can be seen in the Figure <a href="#fig:cd-sampling-size"><strong>??</strong></a>, the choice of minibatch size which corresponds to the number sequences that are selected to approximate the gradient, has no influence on precision.</p>
<p>PLOT PEFORMANCE SMAPLING SIZE</p>
<p>This results is somewhat unexpected, because using more samples to approximate the gradient should result in a better gradient approximation and thus in a better performance. Indeed, the magnitude of the gradient norms decreases when more sequences are used for sampling as can be seen in Figure <a href="#fig:cd-gradient-norm"><strong>??</strong></a>. However, this does apparantly not translate into better parameter values.</p>
<p>PLOT GRADIENT NORMS</p>
<p>The default <a href="abbrev.html#abbrev">CD</a> algorithm as described by Hinton in 2002 applies only one full step of Gibbs sampling on each data sample to generate a sampled data set that will be used to approximate the gradient <span class="citation">[<a href="#ref-Hinton2002">129</a>]</span>. One full step of Gibbs sampling corresponds to sampling each position in a protein sequence according to the conditional probabilities computed from the current model probabilties as described in <a href="#gibbs-sampling"><strong>??</strong></a>. The sampled sequences obtained after only one step of Gibbs sampling will be very similar to the input sequences. It has been shown that sampling with <span class="math inline">\(n&gt;1\)</span> steps gives more precise results but increases computational cost per gradient evaluation <span class="citation">[<a href="#ref-Bengio2009">153</a>,<a href="#ref-Tieleman2008">154</a>]</span>.</p>
<p>In the following I analysed the impact on performance when Gibbs sampling each sequence with 1, 5 and 10 full steps. As can be seen, there is hardly an impact on precision while having much longer runtimes (by a factor or 5 and 10).</p>

<div class="figure" style="text-align: center"><span id="fig:precision-cd-gibbs-steps"></span>
<iframe src="img/full_likelihood/precision_vs_rank_notitle_cd_comparing_nr_of_gibbs_sampling_steps.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 8.9: Performance of contrastive divergence optimization of the full likelihood with different number of Gibbs steps compared to pseudo-likelihood (blue) for 287 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. pseudo-likelihood: contact scores computed from pseudo-likelihood. The other methods derive contact scores from couplings computed from <a href="abbrev.html#abbrev">CD</a> with different number of Gibbs sampling steps.
</p>
</div>
<p>Another variant of <a href="abbrev.html#abbrev">CD</a> that has been suggested by Tieleman in 2008 is <a href="abbrev.html#abbrev"><em>PCD</em></a><span class="citation">[<a href="#ref-Tieleman2008">154</a>]</span> that does not reset the Markov Chains at every iteration. The reason being that when using small learning rates, the model changes only slightly between iterations and the true data distribution can be better approximated However, subsequent samples of <a href="abbrev.html#abbrev"><em>PCD</em></a> will be highly correlated creating a kind of momentum effect. Furthermore is has been found that <a href="abbrev.html#abbrev"><em>PCD</em></a> should be used with smaller learning rates and higher minibatch sizes.</p>
<p>As PCD might require samller update steps and larger minibatches, I analysed the performance of PCD for the default settings of CD and additionally for smaller learning and decay rates and larger minibatches. Note that one Markov chain is kept for every sequence of the input alignment. At each iteration a subset <span class="math inline">\(N^{\prime} &lt; N\)</span> of the Markov chains is randomly selected (without replacement) and used to for another round of Gibss sampling at the current iteration.</p>
<p>PLOT PCD for different LEARNIGN RATEWS and SAMPLE SIZES</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Carreira-Perpinan2005">
<p>149. Carreira-Perpiñán, M. a, and Hinton, G.E. (2005). On Contrastive Divergence Learning. Artif. Intell. Stat. <em>0</em>, 17. Available at: <a href="http://learning.cs.toronto.edu/{~}hinton/absps/cdmiguel.pdf" class="uri">http://learning.cs.toronto.edu/{~}hinton/absps/cdmiguel.pdf</a>.</p>
</div>
<div id="ref-Le2011">
<p>150. Le, Q.V., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng, A.Y. (2011). On optimization methods for deep learning ([International Machine Learning Society]) Available at: <a href="https://dl.acm.org/citation.cfm?id=3104516" class="uri">https://dl.acm.org/citation.cfm?id=3104516</a>.</p>
</div>
<div id="ref-Kingma2014">
<p>136. Kingma, D., and Ba, J. (2014). Adam: A Method for Stochastic Optimization. Available at: <a href="http://arxiv.org/abs/1412.6980" class="uri">http://arxiv.org/abs/1412.6980</a>.</p>
</div>
<div id="ref-Chollet2015">
<p>151. Chollet, F. and others (2015). Keras. Available at: <a href="https://github.com/fchollet/keras" class="uri">https://github.com/fchollet/keras</a>.</p>
</div>
<div id="ref-Dieleman2015">
<p>152. Dieleman, S., Schlüter, J., Raffel, C., Olson, E., Sønderby, S.K., Nouri, D., Maturana, D., Thoma, M., Battenberg, E., and Kelly, J. <em>et al.</em> (2015). Lasagne: First release. Available at: <a href="https://zenodo.org/record/27878" class="uri">https://zenodo.org/record/27878</a>.</p>
</div>
<div id="ref-Bottou2012">
<p>133. Bottou, L. (2012). Stochastic Gradient Descent Tricks. In Neural networks: Tricks of the trade (Springer, Berlin, Heidelberg), pp. 421–436. Available at: <a href="http://link.springer.com/10.1007/978-3-642-35289-8{\_}25" class="uri">http://link.springer.com/10.1007/978-3-642-35289-8{\_}25</a>.</p>
</div>
<div id="ref-Hinton2002">
<p>129. Hinton, G.E. (2002). Training Products of Experts by Minimizing Contrastive Divergence. Neural Comput. <em>14</em>, 1771–1800. Available at: <a href="http://www.gatsby.ucl.ac.uk/publications/tr/tr00-004.pdf" class="uri">http://www.gatsby.ucl.ac.uk/publications/tr/tr00-004.pdf</a>.</p>
</div>
<div id="ref-Bengio2009">
<p>153. Bengio, Y., and Delalleau, O. (2009). Justifying and Generalizing Contrastive Divergence. Neural Comput. <em>21</em>, 1601–21. Available at: <a href="http://www.iro.umontreal.ca/{~}lisa/publications2/index.php/attachments/single/105" class="uri">http://www.iro.umontreal.ca/{~}lisa/publications2/index.php/attachments/single/105</a>.</p>
</div>
<div id="ref-Tieleman2008">
<p>154. Tieleman, T. (2008). Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient. Proc. 25th Int. Conf. Mach. Learn. <em>307</em>, 7.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://contactpredictionthesis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="analysis-of-coupling-matrices.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-model-for-residue-resdiue-contact-prediction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/susannvorberg/phd_thesis/edit/master/04-methods.Rmd",
"text": "Edit"
},
"download": ["PhD_thesis_Susann_Vorberg.pdf"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
