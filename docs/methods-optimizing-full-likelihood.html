<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>PhD thesis: residue-residue contact prediction</title>
  <meta name="description" content="This is my PhD thesis on residue-residue contact prediction.">
  <meta name="generator" content="bookdown 0.3.9 and GitBook 2.6.7">

  <meta property="og:title" content="PhD thesis: residue-residue contact prediction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my PhD thesis on residue-residue contact prediction." />
  <meta name="github-repo" content="susannvorberg/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PhD thesis: residue-residue contact prediction" />
  
  <meta name="twitter:description" content="This is my PhD thesis on residue-residue contact prediction." />
  

<meta name="author" content="Susann Vorberg">


<meta name="date" content="2017-09-27">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="analysis-of-coupling-matrices.html">
<link rel="next" href="bayesian-model-for-residue-resdiue-contact-prediction.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
  tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
  TeX: { 
    extensions: ["mediawiki-texvc.js", "sinuitx.js"],
    Macros: {
      Cb: "C_\\beta",
      eq: "\\!=\\!",
      Gauss: "\\mathcal{N}",
      H: "\\mathbf{H}",
      Hij : "\\H_{ij}",
      I: "\\mathbf{I}",
      Lijk: "\\mathbf{\\Lambda}_{ij,k}",
      Lk: "\\mathbf{\\Lambda}_k",
      LLreg: "L\\!L_\\mathrm{reg}",
      muijk: "\\mathbf{\\mu}_{ij,k}",
      muk: "\\mathbf{\\mu}_k",
      neff: "N_\\mathrm{eff}",
      r: "\\mathbf{r}",
      rij: "r_{ij}",
      seq: "\\mathbf{x}",
      Qij: "\\mathbf{Q}_{ij}",
      q: "\\mathbf{q}",
      qij: "\\mathbf{q'}_{ij}",
      Sn: "\\mathcal{S}_n",
      v: "\\mathbf{v}",
      vi: "\\mathcal{v}_{i}",
      via: "\\mathcal{v}_{ia}",
      w: "\\mathbf{w}",
      wij: "\\mathbf{w}_{ij}",
      wijab: "\\mathcal{w}_{ijab}",
      wijcd: "\\mathcal{w}_{ijcd}",
      wklcd: "\\mathcal{w}_{klcd}",
      X: "\\mathbf{X}",
      angstrom: "\\AA \\;"
      }
  }
});
</script>


 
<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js", "[siunitx]/siunitx.js"],
  TeX: { TagSide: "left" }
});
MathJax.Ajax.config.path['siunitx']  = '../latex/MathJax-siunitx-master/';
</script>
//-->



<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD Thesis Susann Vorberg</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="general-intro.html"><a href="general-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="protein-structure.html"><a href="protein-structure.html"><i class="fa fa-check"></i><b>1.1</b> Protein Structure</a><ul>
<li class="chapter" data-level="1.1.1" data-path="protein-structure.html"><a href="protein-structure.html#amino-acid-interactions"><i class="fa fa-check"></i><b>1.1.1</b> Amino Acid Interactions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="structure-prediction.html"><a href="structure-prediction.html"><i class="fa fa-check"></i><b>1.2</b> Structure Prediction</a><ul>
<li class="chapter" data-level="1.2.1" data-path="structure-prediction.html"><a href="structure-prediction.html#template-based-methods"><i class="fa fa-check"></i><b>1.2.1</b> Template-based methods</a></li>
<li class="chapter" data-level="1.2.2" data-path="structure-prediction.html"><a href="structure-prediction.html#template-free-structure-prediction"><i class="fa fa-check"></i><b>1.2.2</b> Template-free structure prediction</a></li>
<li class="chapter" data-level="1.2.3" data-path="structure-prediction.html"><a href="structure-prediction.html#contact-assisted-str-pred"><i class="fa fa-check"></i><b>1.2.3</b> contact assisted de-novo predictions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="contact-prediction.html"><a href="contact-prediction.html"><i class="fa fa-check"></i><b>1.3</b> Contact Prediction</a><ul>
<li class="chapter" data-level="1.3.1" data-path="contact-prediction.html"><a href="contact-prediction.html#local-methods"><i class="fa fa-check"></i><b>1.3.1</b> Local Statistical Models</a></li>
<li class="chapter" data-level="1.3.2" data-path="contact-prediction.html"><a href="contact-prediction.html#global-methods"><i class="fa fa-check"></i><b>1.3.2</b> Global Statistical Models</a></li>
<li class="chapter" data-level="1.3.3" data-path="contact-prediction.html"><a href="contact-prediction.html#meta-predictors"><i class="fa fa-check"></i><b>1.3.3</b> Machine Learning Methods and Meta-Predictors</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="maxent.html"><a href="maxent.html"><i class="fa fa-check"></i><b>1.4</b> Modelling Protein Families with Potts Model</a><ul>
<li class="chapter" data-level="1.4.1" data-path="maxent.html"><a href="maxent.html#potts-model-properties"><i class="fa fa-check"></i><b>1.4.1</b> Model Properties</a></li>
<li class="chapter" data-level="1.4.2" data-path="maxent.html"><a href="maxent.html#potts-mle"><i class="fa fa-check"></i><b>1.4.2</b> Inferring Parameters for the Potts Model</a></li>
<li class="chapter" data-level="1.4.3" data-path="maxent.html"><a href="maxent.html#potts-model-solutions"><i class="fa fa-check"></i><b>1.4.3</b> Solving the Inverse Potts Problem</a></li>
<li class="chapter" data-level="1.4.4" data-path="maxent.html"><a href="maxent.html#post-processing-heuristics"><i class="fa fa-check"></i><b>1.4.4</b> Computing Contact Maps</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html"><i class="fa fa-check"></i><b>1.5</b> Evaluating Contact Prediction Methods</a><ul>
<li class="chapter" data-level="1.5.1" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#seq-sep"><i class="fa fa-check"></i><b>1.5.1</b> Sequence Separation</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>1.6</b> Challenges in Coevolutionary Inference</a><ul>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#phylogenetic-bias"><i class="fa fa-check"></i>Phylogenetic Bias</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#entropic-bias"><i class="fa fa-check"></i>Entropic bias</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#finite-sampling-effects"><i class="fa fa-check"></i>Finite Sampling Effects</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#transitive-effects"><i class="fa fa-check"></i>Transitive Effects</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#multiple-sequence-alignments"><i class="fa fa-check"></i>Multiple Sequence Alignments</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#evaluation-strategy"><i class="fa fa-check"></i>Evaluation Strategy</a></li>
<li class="chapter" data-level="" data-path="challenges.html"><a href="challenges.html#alternative-sources-of-coevolution"><i class="fa fa-check"></i>Alternative Sources of Coevolution</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="developing-a-bayesian-model-for-contact-prediction.html"><a href="developing-a-bayesian-model-for-contact-prediction.html"><i class="fa fa-check"></i><b>1.7</b> Developing a Bayesian Model for Contact Prediction</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpreting-coupling-matrices.html"><a href="interpreting-coupling-matrices.html"><i class="fa fa-check"></i><b>2</b> Interpretation of Coupling Matrices</a><ul>
<li class="chapter" data-level="2.1" data-path="correlation-between-couplings-and-class.html"><a href="correlation-between-couplings-and-class.html"><i class="fa fa-check"></i><b>2.1</b> Single Coupling Values Carry Evidence of Contacts</a></li>
<li class="chapter" data-level="2.2" data-path="physico-chemical-fingerprints-in-coupling-matrices.html"><a href="physico-chemical-fingerprints-in-coupling-matrices.html"><i class="fa fa-check"></i><b>2.2</b> Physico-Chemical Fingerprints in Coupling Matrices</a></li>
<li class="chapter" data-level="2.3" data-path="coupling-profiles-vary-with-distance.html"><a href="coupling-profiles-vary-with-distance.html"><i class="fa fa-check"></i><b>2.3</b> Coupling Profiles Vary with Distance</a></li>
<li class="chapter" data-level="2.4" data-path="higher-order-dependencies-between-couplings.html"><a href="higher-order-dependencies-between-couplings.html"><i class="fa fa-check"></i><b>2.4</b> Higher Order Dependencies Between Couplings</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="optimizing-full-likelihood.html"><a href="optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>3</b> Optimizing the Full-Likelihood</a><ul>
<li class="chapter" data-level="3.1" data-path="the-likelihood-of-the-sequences-as-a-potts-model.html"><a href="the-likelihood-of-the-sequences-as-a-potts-model.html"><i class="fa fa-check"></i><b>3.1</b> The Likelihood of the Sequences as a Potts Model</a></li>
<li class="chapter" data-level="3.2" data-path="gap-treatment.html"><a href="gap-treatment.html"><i class="fa fa-check"></i><b>3.2</b> Treating Gaps as Missing Information</a></li>
<li class="chapter" data-level="3.3" data-path="the-regularized-log-likelihood-and-its-gradient.html"><a href="the-regularized-log-likelihood-and-its-gradient.html"><i class="fa fa-check"></i><b>3.3</b> The Regularized Log Likelihood and its Gradient</a></li>
<li class="chapter" data-level="3.4" data-path="prior-v.html"><a href="prior-v.html"><i class="fa fa-check"></i><b>3.4</b> The prior on <span class="math inline">\(\v\)</span></a></li>
<li class="chapter" data-level="3.5" data-path="full-likelihood-gradient.html"><a href="full-likelihood-gradient.html"><i class="fa fa-check"></i><b>3.5</b> Optimizing the Full Likelihood with Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><a href="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><i class="fa fa-check"></i><b>4</b> A Bayesian Statistical Model for Residue-Residue Contact Prediction</a><ul>
<li class="chapter" data-level="4.1" data-path="overview-posterior-distances.html"><a href="overview-posterior-distances.html"><i class="fa fa-check"></i><b>4.1</b> Computing the Posterior Distribution of Distances <span class="math inline">\(p(\r | \X)\)</span></a></li>
<li class="chapter" data-level="4.2" data-path="coupling-prior.html"><a href="coupling-prior.html"><i class="fa fa-check"></i><b>4.2</b> Modelling the prior over couplings with dependence on <span class="math inline">\(\rij\)</span></a></li>
<li class="chapter" data-level="4.3" data-path="laplace-approx.html"><a href="laplace-approx.html"><i class="fa fa-check"></i><b>4.3</b> Gaussian approximation to the posterior of couplings</a><ul>
<li class="chapter" data-level="4.3.1" data-path="laplace-approx.html"><a href="laplace-approx.html#laplace-approx-improvement"><i class="fa fa-check"></i><b>4.3.1</b> Iterative improvement of Laplace approximation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="likelihood-fct-distances.html"><a href="likelihood-fct-distances.html"><i class="fa fa-check"></i><b>4.4</b> Computing the likelihood function of distances <span class="math inline">\(p(\X | \r)\)</span></a></li>
<li class="chapter" data-level="4.5" data-path="posterior-of-rij.html"><a href="posterior-of-rij.html"><i class="fa fa-check"></i><b>4.5</b> The posterior probability distribution for <span class="math inline">\(\rij\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="contact-prior.html"><a href="contact-prior.html"><i class="fa fa-check"></i><b>5</b> Contact Prior</a><ul>
<li class="chapter" data-level="5.1" data-path="random-forest-classifiers.html"><a href="random-forest-classifiers.html"><i class="fa fa-check"></i><b>5.1</b> Random Forest Classifiers</a></li>
<li class="chapter" data-level="5.2" data-path="evaluating-random-forest-model-as-contact-predictor.html"><a href="evaluating-random-forest-model-as-contact-predictor.html"><i class="fa fa-check"></i><b>5.2</b> Evaluating Random Forest Model as Contact Predictor</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>6</b> Methods</a><ul>
<li class="chapter" data-level="6.1" data-path="dataset.html"><a href="dataset.html"><i class="fa fa-check"></i><b>6.1</b> Dataset</a></li>
<li class="chapter" data-level="6.2" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html"><i class="fa fa-check"></i><b>6.2</b> Optimizing Pseudo-Likelihood</a><ul>
<li class="chapter" data-level="6.2.1" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#pseudo-likelihood-objective-function-and-its-gradients"><i class="fa fa-check"></i><b>6.2.1</b> Pseudo-Likelihood Objective Function and its Gradients</a></li>
<li class="chapter" data-level="6.2.2" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#diff-ccmpred-ccmpredpy"><i class="fa fa-check"></i><b>6.2.2</b> Differences between CCMpred and CCMpredpy</a></li>
<li class="chapter" data-level="6.2.3" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#seq-reweighting"><i class="fa fa-check"></i><b>6.2.3</b> Sequence Reweighting</a></li>
<li class="chapter" data-level="6.2.4" data-path="optimizing-pseudo-likelihood.html"><a href="optimizing-pseudo-likelihood.html#amino-acid-frequencies"><i class="fa fa-check"></i><b>6.2.4</b> Computing Amino Acid Frequencies</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html"><i class="fa fa-check"></i><b>6.3</b> Analysis of Coupling Matrices</a><ul>
<li class="chapter" data-level="6.3.1" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-correlation"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Couplings with Contact Class</a></li>
<li class="chapter" data-level="6.3.2" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-profile"><i class="fa fa-check"></i><b>6.3.2</b> Coupling Distribution Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>6.4</b> Optimizing the Full-Likelihood</a><ul>
<li class="chapter" data-level="6.4.1" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html#hyperparameter-optimization-for-stochastic-gradient-descent"><i class="fa fa-check"></i><b>6.4.1</b> Hyperparameter Optimization for Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="6.4.2" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html#optimizing-regularization-coefficients-for-contrastive-divergence"><i class="fa fa-check"></i><b>6.4.2</b> Optimizing Regularization Coefficients for Contrastive Divergence</a></li>
<li class="chapter" data-level="6.4.3" data-path="methods-optimizing-full-likelihood.html"><a href="methods-optimizing-full-likelihood.html#optimizing-the-sampling-scheme-for-contrastive-divergence"><i class="fa fa-check"></i><b>6.4.3</b> Optimizing the Sampling Scheme for Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html"><i class="fa fa-check"></i><b>6.5</b> Bayesian Model for Residue-Resdiue Contact Prediction</a><ul>
<li class="chapter" data-level="6.5.1" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#neg-Hessian-computation"><i class="fa fa-check"></i><b>6.5.1</b> Efficiently Computing the negative Hessian of the regularized log-likelihood</a></li>
<li class="chapter" data-level="6.5.2" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#inv-lambda-ij-k"><i class="fa fa-check"></i><b>6.5.2</b> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></a></li>
<li class="chapter" data-level="6.5.3" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#training-hyperparameters"><i class="fa fa-check"></i><b>6.5.3</b> Training the Hyperparameters <span class="math inline">\(\muk\)</span>, <span class="math inline">\(\Lk\)</span> and <span class="math inline">\(\gamma_k\)</span></a></li>
<li class="chapter" data-level="6.5.4" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#the-gradient-of-the-log-likelihood-with-respect-to-mathbfmu"><i class="fa fa-check"></i><b>6.5.4</b> The gradient of the log likelihood with respect to <span class="math inline">\(\mathbf{\mu}\)</span></a></li>
<li class="chapter" data-level="6.5.5" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#the-gradient-of-the-log-likelihood-with-respect-to-lk"><i class="fa fa-check"></i><b>6.5.5</b> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></a></li>
<li class="chapter" data-level="6.5.6" data-path="bayesian-model-for-residue-resdiue-contact-prediction.html"><a href="bayesian-model-for-residue-resdiue-contact-prediction.html#the-gradient-of-the-log-likelihood-with-respect-to-gamma_k"><i class="fa fa-check"></i><b>6.5.6</b> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><a href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><i class="fa fa-check"></i><b>6.6</b> Bayesian Statistical Model for Prediction of Protein Residue-Residue Distances</a><ul>
<li class="chapter" data-level="6.6.1" data-path="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><a href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html#modelling-the-dependence-of-wij-on-distance"><i class="fa fa-check"></i><b>6.6.1</b> Modelling the dependence of <span class="math inline">\(\wij\)</span> on distance</a></li>
<li class="chapter" data-level="6.6.2" data-path="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html"><a href="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html#training-the-hyperparameters-rho_k-and-alpha_k-for-distance-dependent-prior"><i class="fa fa-check"></i><b>6.6.2</b> Training the Hyperparameters <span class="math inline">\(\rho_k\)</span> and <span class="math inline">\(\alpha_k\)</span> for distance-dependent prior</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html"><i class="fa fa-check"></i><b>6.7</b> Training Random Forest Contat Prior</a><ul>
<li class="chapter" data-level="6.7.1" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#seq-features"><i class="fa fa-check"></i><b>6.7.1</b> Sequence Derived Features</a></li>
<li class="chapter" data-level="6.7.2" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#rf-hyperparameter-optimization"><i class="fa fa-check"></i><b>6.7.2</b> Hyperparameter Optimization for Random Forest Prior</a></li>
<li class="chapter" data-level="6.7.3" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#rf-feature-selection"><i class="fa fa-check"></i><b>6.7.3</b> Feature Selection</a></li>
<li class="chapter" data-level="6.7.4" data-path="training-random-forest-contat-prior.html"><a href="training-random-forest-contat-prior.html#rf-with-pll-score"><i class="fa fa-check"></i><b>6.7.4</b> Using Pseudo-likelihood Coevolution Score as Additional Feature</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="abbrev.html"><a href="abbrev.html"><i class="fa fa-check"></i><b>A</b> Abbreviations</a><ul>
<li class="chapter" data-level="A.1" data-path="amino-acids.html"><a href="amino-acids.html"><i class="fa fa-check"></i><b>A.1</b> Amino Acid Alphabet</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="dataset-properties.html"><a href="dataset-properties.html"><i class="fa fa-check"></i><b>B</b> Dataset Properties</a><ul>
<li class="chapter" data-level="B.1" data-path="alignment-diversity.html"><a href="alignment-diversity.html"><i class="fa fa-check"></i><b>B.1</b> Alignment Diversity</a></li>
<li class="chapter" data-level="B.2" data-path="proportion-of-gaps-in-alignment.html"><a href="proportion-of-gaps-in-alignment.html"><i class="fa fa-check"></i><b>B.2</b> Proportion of Gaps in Alignment</a></li>
<li class="chapter" data-level="B.3" data-path="alignment-size-number-of-sequences.html"><a href="alignment-size-number-of-sequences.html"><i class="fa fa-check"></i><b>B.3</b> Alignment Size (number of sequences)</a></li>
<li class="chapter" data-level="B.4" data-path="protein-length.html"><a href="protein-length.html"><i class="fa fa-check"></i><b>B.4</b> Protein Length</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><a href="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><i class="fa fa-check"></i><b>C</b> Amino Acid Interaction Preferences Reflected in Coupling Matrices</a><ul>
<li class="chapter" data-level="C.1" data-path="pi-cation.html"><a href="pi-cation.html"><i class="fa fa-check"></i><b>C.1</b> Pi-Cation interactions</a></li>
<li class="chapter" data-level="C.2" data-path="disulfide.html"><a href="disulfide.html"><i class="fa fa-check"></i><b>C.2</b> Disulfide Bonds</a></li>
<li class="chapter" data-level="C.3" data-path="aromatic-proline.html"><a href="aromatic-proline.html"><i class="fa fa-check"></i><b>C.3</b> Aromatic-Proline Interactions</a></li>
<li class="chapter" data-level="C.4" data-path="aromatic-network.html"><a href="aromatic-network.html"><i class="fa fa-check"></i><b>C.4</b> Network-like structure of aromatic residues</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="optimizing-full-likelihood-with-gradient-descent.html"><a href="optimizing-full-likelihood-with-gradient-descent.html"><i class="fa fa-check"></i><b>D</b> Optimizing Full Likelihood with Gradient Descent</a><ul>
<li class="chapter" data-level="D.1" data-path="cd-large-alpha-large-neff.html"><a href="cd-large-alpha-large-neff.html"><i class="fa fa-check"></i><b>D.1</b> Divergence of objective function for big learning rates and Neff values</a></li>
<li class="chapter" data-level="D.2" data-path="number-of-iterations-for-different-learning-rates.html"><a href="number-of-iterations-for-different-learning-rates.html"><i class="fa fa-check"></i><b>D.2</b> Number of iterations for different learning rates</a></li>
<li class="chapter" data-level="D.3" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html"><i class="fa fa-check"></i><b>D.3</b> Number of iterations for different learning rate schedules and fixed initial learning rate <span class="math inline">\(\alpha_0 =\)</span> 1e-4</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="training-of-the-random-forest-contact-prior.html"><a href="training-of-the-random-forest-contact-prior.html"><i class="fa fa-check"></i><b>E</b> Training of the Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="E.1" data-path="rf-window-size.html"><a href="rf-window-size.html"><i class="fa fa-check"></i><b>E.1</b> Evaluating window size with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="E.2" data-path="rf-noncontact-threshold.html"><a href="rf-noncontact-threshold.html"><i class="fa fa-check"></i><b>E.2</b> Evaluating non-contact threshold with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="E.3" data-path="rf-ratio-noncontacts.html"><a href="rf-ratio-noncontacts.html"><i class="fa fa-check"></i><b>E.3</b> Evaluating ratio of non-contacts and contacts in the training set with 5-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PhD thesis: residue-residue contact prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="methods-optimizing-full-likelihood" class="section level2">
<h2><span class="header-section-number">6.4</span> Optimizing the Full-Likelihood</h2>
<p>Given the likelihood gradient estimates obtained with <a href="abbrev.html#abbrev">CD</a>, the full likelihood can now be minimized using a gradient descent optimization algorithm. Second order optimization algorithms that make use of the (approximate) partial second derivates cannot be applied here, as the computation of the second derivatives of the full likelihood is too complex.</p>
<p>The following sections will describe the hyperparameter tuning for the stochastic gradient descent optimization as well as tuning different aspects of the Gibbs Sampler used to approximate the gradient with <a href="abbrev.html#abbrev">CD</a>. The performance will be evaluated as the mean precision of the top ranked contact predictions over a benchmark set of 300 proteins, that is a subset of the data set described in methods section <a href="dataset.html#dataset">6.1</a>. The reference method for all new developments is the pseudo-likelihood method that uses the <a href="abbrev.html#abbrev">APC</a> corrected L2norm as a contact score as explained in section <a href="maxent.html#post-processing-heuristics">1.4.4</a>. Pseudo-likelihood couplings are computed with the tool CCMpredPy that is introduced in methods section <a href="optimizing-pseudo-likelihood.html#diff-ccmpred-ccmpredpy">6.2.2</a>. Contact scores for couplings optimized with <a href="abbrev.html#abbrev">CD</a> are computed in the same way as for the pseudo-likelihood.</p>
<div id="hyperparameter-optimization-for-stochastic-gradient-descent" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Hyperparameter Optimization for Stochastic Gradient Descent</h3>
<p>Gradient descent algorithms in general minimize an objective function by iteratively updating the function parameters in the opposite direction of the gradient of the objective function with respect to the parameters. Stochastic gradient descent <a href="abbrev.html#abbrev">SGD</a> is a variant thereof that uses only a subsample of the data at each step of the optimization procedure to estimate the gradient. Consequently, the gradient estimates are noisy resulting in parameter updates with high variance and strong fluctuations of the objective function. These fluctuation enable stochastic gradient descent to escape local minima but also complicate finding the exact minimum of the objective function. By slowly decreasing the step size of the parameter updates at every iteration, stochastic gradient descent most likely will converge to the global minimum for convex objective functions <span class="citation">[<a href="#ref-Ruder2017">129</a>,<a href="#ref-Bottou2012">130</a>]</span>. However, choosing an optimal step size for parameter updates, referred to as learning rate, as well as finding the optimal rate of decay offers a challenge and needs manual tuning <span class="citation">[<a href="#ref-Schaul2013">131</a>]</span>.</p>
<p>When optimizing <a href="abbrev.html#abbrev">CD</a> with <a href="abbrev.html#abbrev">SGD</a>, the stochasticity is not so much introduced by looking only at subsamples of the data but rather by the stochastic nature of the Gibbs sampling process to approximate the gradient. The coupling parameters <span class="math inline">\(\w\)</span> will be updated in each iteration <span class="math inline">\(t\)</span> by taking a step of size <span class="math inline">\(\alpha\)</span> along the direction of the negative gradient of the full log likelihood <span class="math inline">\(- \nabla_w \LLreg(\v,\w)\)</span> that has been approximated with contrastive divergence,</p>
<span class="math display">\[\begin{equation}
  \w_{t+1} = \w_t - \alpha \cdot \nabla_w \LLreg(\v,\w) \; .
\end{equation}\]</span>
<p>In order to get a first intuition of the optimization problem, I used an initial learning rate <span class="math inline">\(\alpha_0 \in \{1\mathrm{e}{-4}, 5\mathrm{e}{-4}, 1\mathrm{e}{-3}, 5\mathrm{e}{-3}\}\)</span> with a standard learning rate annealing schedule <span class="math inline">\(\alpha = \frac{\alpha_0}{1 + \gamma \cdot t}\)</span>, with <span class="math inline">\(\gamma=0.1\)</span> being the decay rate and <span class="math inline">\(t\)</span> the current iteration <span class="citation">[<a href="#ref-Bottou2012">130</a>]</span>. Typically, the algorithm has converged and is stopped whenever the difference of subsequent function evaluations is less than a small value <span class="math inline">\(\epsilon\)</span>. However, as it is not feasible to evaluate the full likelihood function at each iteration, because of the exponential complexity of the partition function, it is necessary to define a different stopping criteron. Therefore the optimization will stop whenever the gradient norm changes less than a small <span class="math inline">\(\epsilon=1\mathrm{e}{-8}\)</span> as in <span class="citation">[<a href="#ref-Carreira-Perpinan2005">132</a>]</span> or when a maximum of 5000 iterations has been reached. Furthermore, parameters are initialized with the pseudo-likelihood optimum, assuming that it is already close to the full likelihood optimum.</p>
<p>Figure <a href="methods-optimizing-full-likelihood.html#fig:performance-cd-alphaopt">6.3</a> shows the mean precision against top ranked contacts computed from pseudo-likelihood couplings and <a href="abbrev.html#abbrev">CD</a> couplings optimized with four different learning rates. Overall, mean precision for <a href="abbrev.html#abbrev">CD</a> contacts is comparable to <span style="color:blue">pseudo-likelihood</span> contacts. Using smaller learning rates with the stochastic gradient descent optimizer results in higher mean precision.</p>

<div class="figure" style="text-align: center"><span id="fig:performance-cd-alphaopt"></span>
<iframe src="img/full_likelihood/alpha_opt_precision_vs_rank_notitle.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 6.3: Mean precision for top ranked contact predictions over 286 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. <span style="color:blue;">pseudo-likelihood</span>: Contact scores computed from pseudo-likelihood. The other methods derive contact scores from couplings computed from <a href="abbrev.html#abbrev">CD</a> using stochastic gradient descent with different initial learning rates <span class="math inline">\(\alpha_0\)</span> as specified in the legend.
</p>
</div>
<p>When evaluating the learning rate settings with respect to alignment size (see Figure <a href="methods-optimizing-full-likelihood.html#fig:performance-cd-alphaopt-neff">6.4</a>) it seems that higher learning rates do not work well for proteins with large alignments. The magnitude of the gradient scales with the number of sequences in the alignment because the gradient is computed as a difference of amino acid counts between observed and sampled sequences (see section <a href="full-likelihood-gradient.html#full-likelihood-gradient">3.5</a>).<br />
And since alignments with many sequences produce high amino acid counts, their gradients will generally be higher compared to alignments with few sequences, especially at the beginning of the optimization procedure when the difference in amino acid counts between sampled and observed sequences is still big. Analysis of individual proteins with large alignments that perform bad in the benchmark reveals that high learning rates lead the parameters to diverge as can be seen in appendix <a href="cd-large-alpha-large-neff.html#cd-large-alpha-large-neff">D.1</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:performance-cd-alphaopt-neff"></span>
<iframe src="img/full_likelihood/alpha_opt_precision_vs_rank_facetted_by_neff_notitle.html" width="90%" height="600px">
</iframe>
<p class="caption">
Figure 6.4: Mean precision for top ranked contact predictions over 286 proteins splitted into four equally sized subsets according to <a href="abbrev.html#abbrev">Neff</a>. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. Subsets are defined according to quantiles of <a href="abbrev.html#abbrev">Neff</a> values. Upper left: Subset of proteins with <a href="abbrev.html#abbrev">Neff</a> &lt; Q1. Upper right: Subset of proteins with Q1 &lt;= <a href="abbrev.html#abbrev">Neff</a> &lt; Q2. Lower left: Subset of proteins with Q2 &lt;= <a href="abbrev.html#abbrev">Neff</a> &lt; Q3. Lower right: Subset of proteins with Q3 &lt;= <a href="abbrev.html#abbrev">Neff</a> &lt; Q4. Methods are the same as in Figure <a href="methods-optimizing-full-likelihood.html#fig:performance-cd-alphaopt">6.3</a>.
</p>
</div>
<p>Furthermore, the smaller the learning rate, the more iterations are necessary for the optimization ot converge. With a learning rate of <span class="math inline">\(1\mathrm{e}{-4}\)</span> no protein converged with less than the maximum number of 5000 iterations (see Figure in Appendix <a href="number-of-iterations-for-different-learning-rates.html#fig:full-likelihood-opt-different-alphas-numit">D.1</a>). It is therefore necessary to find a more appropriate learning rate schedule and decay rate.</p>
<p>I evaluated the following learning rate schedules and decay rates for an initial learning rate <span class="math inline">\(\alpha_0 = 1\mathrm{e}{-4}\)</span> which gave highest precision in the previous benchmark but slow convergence:</p>
<ul>
<li>default learning rate schedule <span class="math inline">\(\alpha = \frac{\alpha_0}{1 + \gamma \cdot t}\)</span> with <span class="math inline">\(\gamma \in \{1\mathrm{e}{-2}, 1\mathrm{e}{-1}, 1\}\)</span></li>
<li>square root learning rate schedule <span class="math inline">\(\alpha = \frac{\alpha_0}{\sqrt{1 + \gamma \cdot t}}\)</span> with <span class="math inline">\(\gamma \in \{1\mathrm{e}{-2}, 1\mathrm{e}{-1}, 1, 10\}\)</span></li>
<li>sigmoidal learning rate schedule <span class="math inline">\(\alpha_{t+1} = \frac{\alpha_{t}}{1 + \gamma \cdot t}\)</span> with <span class="math inline">\(\gamma \in \{1\mathrm{e}{-5}, 1\mathrm{e}{-4}, 1\mathrm{e}{-3}, 1\mathrm{e}{-2}\}\)</span></li>
</ul>
<p>The decay schedules with different decay rates are visualized in Figure <a href="methods-optimizing-full-likelihood.html#fig:learning-rate-schedules">6.5</a></p>

<div class="figure" style="text-align: center"><span id="fig:learning-rate-schedules"></span>
<iframe src="img/full_likelihood/learning_rate_schedules_alpha0_00001_notitle.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 6.5: Value of learning rate against the number of iterations for different learning rate schedules. Red legend group represents the default learning rate schedule <span class="math inline">\(\alpha = \alpha_0 / (1 + \gamma \cdot t)\)</span>. Blue legend represents the sigmoidal learning rate schedule <span class="math inline">\(\alpha_{t+1} = \alpha_{t} / (1 + \gamma \cdot t)\)</span> with <span class="math inline">\(\gamma\)</span>. Green legend represents the square root learning rate schedule <span class="math inline">\(\alpha = \alpha_0 / \sqrt{1 + \gamma \cdot t}\)</span>. The iteration number is given by <span class="math inline">\(t\)</span>. Initial learning rate <span class="math inline">\(\alpha_0\)</span> is set to 1e-4 and <span class="math inline">\(\gamma\)</span> is the decay rate and its value is given in brackets in the legend.
</p>
</div>
<p>Only the sigmoidal learning rate schedule achieves precision comparable to the pseudo-likelihood score while improving convergence speed measured by the number of iterations. Appendix <a href="learning-rate-schedules-distribution-iterations.html#learning-rate-schedules-distribution-iterations">D.3</a> shows benchmarks as well as the distribution over the number of iterations until convergence for all learning rate schedules that have been evaluated. Whereas none of the proteins except one did converge within 5000 iterations using the default learning rate schedule with decay rate <span class="math inline">\(\gamma = 1\mathrm{e}{-1}\)</span> (blue box plot in Figure <a href="methods-optimizing-full-likelihood.html#fig:distribution-num-it-for-best-learning-rate-schedules">6.6</a>), all proteins converged within 2000 or 1000 iterations using the sigmoidal learning rate schedule with decay rates <span class="math inline">\(\gamma = 1\mathrm{e}{-5}\)</span> and <span class="math inline">\(\gamma = 1\mathrm{e}{-4}\)</span>, respectively (orange and green box plot respectively in Figure <a href="methods-optimizing-full-likelihood.html#fig:distribution-num-it-for-best-learning-rate-schedules">6.6</a>).</p>
<p>Because the gradient of the full likelihood scales with number of sequences in the <a href="abbrev.html#abbrev">MSA</a>, I defined the initial learning rate <span class="math inline">\(\alpha_0\)</span> and the decay rate <span class="math inline">\(\gamma\)</span> for the sigmoidal learning rate schedule as functions of <a href="abbrev.html#abbrev">Neff</a>. Aiming to obtain values for the initial learning rate <span class="math inline">\(\alpha_0\)</span> and the decay rate <span class="math inline">\(\gamma\)</span> around 1e-4, as these values yield good performance and fast convergence, I defined <span class="math inline">\(\alpha_0 = \gamma = \frac{3\mathrm{e}{-4}}{\log N_{\text{eff}}}\)</span>. Assuming small <a href="abbrev.html#abbrev">Neff</a> <span class="math inline">\(\approx 50\)</span> this yields <span class="math inline">\(\alpha_0 = \gamma \approx 1.7\mathrm{e}{-4}\)</span> and for big <a href="abbrev.html#abbrev">Neff</a> <span class="math inline">\(\approx 20000\)</span> this yields <span class="math inline">\(\alpha_0 = \gamma \approx 7\mathrm{e}{-5}\)</span>. The distribution of the number of iterations until convergence is displayed as red box plot in Figure <a href="methods-optimizing-full-likelihood.html#fig:distribution-num-it-for-best-learning-rate-schedules">6.6</a>, and is in the range of the sigmoidal learning rate schedule with decay rates <span class="math inline">\(\gamma = 1\mathrm{e}{-5}\)</span> and <span class="math inline">\(\gamma = 1\mathrm{e}{-4}\)</span>, as expected.</p>
<p>All following analyses are conducted using the sigmoidal learning rate schedule with initial learning rate and decay rate defined as functions of <a href="abbrev.html#abbrev">Neff</a> as <span class="math inline">\(\alpha_0 = \gamma = \frac{3\mathrm{e}{-4}}{\log N_{\text{eff}}}\)</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:distribution-num-it-for-best-learning-rate-schedules"></span>
<iframe src="img/full_likelihood/distribution_numiterations_against_different_learning_rate_schedules.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 6.6: Distribution of the number of iterations until convergence for gradient descent optimizations of the full likelihood for different learning rate schedules. Initial learning rate <span class="math inline">\(\alpha_0\)</span> is fixed to 1e-4 and maximum number of iterations is set to 5000. Blue box plot displays default learning rate schedule with decay rate <span class="math inline">\(\gamma = 1\mathrm{e}{-1}\)</span>. Orange and green boxplot represent sigmoidal learning rate schedule with decay rates <span class="math inline">\(\gamma = 1\mathrm{e}{-5}\)</span> and <span class="math inline">\(\gamma = 1\mathrm{e}{-4}\)</span>, respectively. Red boxplot displays sigmoidal learning rate schedule with <span class="math inline">\(\alpha_0\)</span> and decay rate <span class="math inline">\(\gamma\)</span> defined as functions of <a href="abbrev.html#abbrev">Neff</a>.
</p>
</div>
<p>Interestingly, and in contradiction to the benchmark results, using higher initial learning rates usually leads the parameters to diverge further from the pseudo-likelihood initializations than using smaller and faster decaying learning rates. Figure <a href="#fig:cd-parameter-optimum-for-1mkca00-different-learning-rates"><strong>??</strong></a> illustrates this effect for protein <em>1mkc</em>. Determining the learning and decay rate with respect to <a href="abbrev.html#abbrev">Neff</a>, as just described, yields <span class="math inline">\(\alpha_0 = \gamma \approx 6.6e-5\)</span>. Optimization converges after <span class="math inline">\(\approx 600\)</span> iterations with an L2-norm over couplings <span class="math inline">\(||\w||_2 = 15.28\)</span>. When using a larger learning rate <span class="math inline">\(\alpha_0 = 1e-3\)</span> the optimization has converged after <span class="math inline">\(\approx 560\)</span> iterations with an L2-norm over couplings <span class="math inline">\(||\w||_2 = 12.66\)</span>.</p>
</div>
<div id="optimizing-regularization-coefficients-for-contrastive-divergence" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Optimizing Regularization Coefficients for Contrastive Divergence</h3>
<p>Gaussian priors are put on the single potentials <span class="math inline">\(\v\)</span> and the couplings <span class="math inline">\(\w\)</span> when optimizing the full likelihood with <a href="abbrev.html#abbrev">CD</a> just as it is done for pseudo-likelihood optimization (compare section <a href="maxent.html#pseudo-likelihood">1.4.3.1</a>). A difference compared to pseudo-likelihood optimization that uses zero centered priors, is the centering of the Gaussian prior for the single potentials <span class="math inline">\(\v\)</span> at <span class="math inline">\(v^*\)</span> as described in section <a href="prior-v.html#prior-v">3.4</a>. The hyperparameter tuning for stochastic gradient descent described in the last section applied the default pseudo-likelihood regularization coefficients <span class="math inline">\(\lambda_v \eq 10\)</span> and <span class="math inline">\(\lambda_w \eq 0.2\cdot(L-1)\)</span>. The regularization coefficient <span class="math inline">\(\lambda_w\)</span> for couplings <span class="math inline">\(\w\)</span> is defined with respect to protein length <span class="math inline">\(L\)</span> owing to the fact that the number of possible contacts in a protein increases quadratically with <span class="math inline">\(L\)</span> whereas the number of observed contacts only increases linearly as can be seen in Figure <a href="methods-optimizing-full-likelihood.html#fig:number-contacts-against-L">6.7</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:number-contacts-against-L"></span>
<iframe src="img/full_likelihood/no_contacts_vs_protein_length_thr8.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 6.7: Number of contacts (<span class="math inline">\(\Cb &lt; 8 \angstrom\)</span>) with respect to protein length and sequence separation has a linear relationship.
</p>
</div>
<p>It is possible that <a href="abbrev.html#abbrev">CD</a> achieves optimal performance using stronger or weaker regularization coefficients compared to pseudo-likelihood. Therefore, I evaluated performance of contrastive divergence using different regularization coefficients <span class="math inline">\(\lambda_w \in \{ 1\mathrm{e}{-2}, 1\mathrm{e}{-1}, 0.2, 1\} \cdot(L-1)\)</span> while leaving the regularization for single potentials at the default value <span class="math inline">\(\lambda_v \eq 10\)</span>. Furthermore, I analysed whether precision is impacted by only optimizing the couplings <span class="math inline">\(\w\)</span> (with default regularization) while fixing the single potentials <span class="math inline">\(\vi\)</span> to their best estimates <span class="math inline">\(\vi^*\)</span> as described in section <a href="prior-v.html#prior-v">3.4</a>.</p>
<p>As can be seen in Figure <a href="methods-optimizing-full-likelihood.html#fig:precison-cd-regularization">6.8</a>, using strong regularization for the couplings <span class="math inline">\(\lambda_w \eq (L-1)\)</span> results in a drop of mean precision. Using weaker regularization or fixing the single potentials hardly has an impact on precision with using <span class="math inline">\(\lambda_w \eq 1\mathrm{e}{-2}(L-1)\)</span> yielding slightly improved precision for the top ranked contacts.</p>

<div class="figure" style="text-align: center"><span id="fig:precison-cd-regularization"></span>
<iframe src="img/full_likelihood/precision_vs_rank_notitle_cd_comparing_regularizers.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 6.8: Performance of contrastive divergence optimization of the full likelihood with different regularization settings compared to pseudo-likelihood (blue) for 280 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. Default regularization coefficients as used with pseudo-likelihood are <span class="math inline">\(\lambda_v \eq 10\)</span> and <span class="math inline">\(\lambda_w \eq 0.2(L-1)\)</span>. “fixed vi” (orange) uses <a href="abbrev.html#abbrev">CD</a> to optimize only couplings with default regularization while keeping the single potentials <span class="math inline">\(\vi\)</span> fixed at their <a href="abbrev.html#abbrev">MLE</a> optimum <span class="math inline">\(\vi^*\)</span>. The other optimization runs with <a href="abbrev.html#abbrev">CD</a> (green, red, purple, brown) use default regularization for the single potentials and a regularization coefficient for the couplings according to legend description.
</p>
</div>
</div>
<div id="optimizing-the-sampling-scheme-for-contrastive-divergence" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Optimizing the Sampling Scheme for Contrastive Divergence</h3>
<p>I analysed whether choosing a different number of sequences for the approximation of the gradient via Gibbs sampling can improve performance. Randomly selecting only a subset <span class="math inline">\(N^{\prime}\)</span> of the <span class="math inline">\(N\)</span> observed sequences corresponds to the stochastic gradient descent idea of a minibatch and introduces additional stochasticity over the random Gibbs sampling process. Using <span class="math inline">\(N^{\prime} &lt; N\)</span> sequences for Gibbs sampling has the further advantage of decreasing the runtime at each iteration. Note, that the reference counts from the observed sequences <span class="math inline">\(N_{ij} q(x_i \eq a, x_j \eq b)\)</span> that are part of the gradient calculation will be kept constant. Therefore it is necessary, however to rescale the amino acid counts from the sampled sequences in a way such that the total sample counts match the total observed counts.</p>
<p>I evaluated two different schemes for randomly selecting <span class="math inline">\(N^{\prime} \eq xL\)</span> sequences from the <span class="math inline">\(N\)</span> given sequences of the alignment at every iteration:</p>
<ul>
<li><strong>without</strong> replacement (enforcing <span class="math inline">\(N^{\prime} \eq \min(N, xL)\)</span>)</li>
<li><strong>with</strong> replacement</li>
</ul>
<p>with <span class="math inline">\(x \in \{ 1, 5, 10, 50 \}\)</span>.</p>
<p>As can be seen in the Figure <a href="#fig:cd-sampling-size"><strong>??</strong></a>, the choice of minibatch size which corresponds to the number sequences that are selected to approximate the gradient, has no influence on precision.</p>
<p>PLOT PEFORMANCE SMAPLING SIZE</p>
<p>This results is somewhat unexpected, because using more samples to approximate the gradient should result in a better gradient approximation and thus in a better performance. Indeed, the magnitude of the gradient norms decreases when more sequences are used for sampling as can be seen in Figure <a href="#fig:cd-gradient-norm"><strong>??</strong></a>. However, this does apparantly not translate into better parameter values.</p>
<p>PLOT GRADIENT NORMS</p>
<p>The default <a href="abbrev.html#abbrev">CD</a> algorithm as described by Hinton in 2002 applies only one full step of Gibbs sampling on each data sample to generate a sampled data set that will be used to approximate the gradient <span class="citation">[<a href="#ref-Hinton2002">117</a>]</span>. One full step of Gibbs sampling corresponds to sampling each position in a protein sequence according to the conditional probabilities computed from the current model probabilties as described in <a href="#gibbs-sampling"><strong>??</strong></a>. The sampled sequences obtained after only one step of Gibbs sampling will be very similar to the input sequences. It has been shown that sampling with <span class="math inline">\(n&gt;1\)</span> steps gives more precise results but increases computational cost per gradient evaluation <span class="citation">[<a href="#ref-Bengio2009">133</a>,<a href="#ref-Tieleman2008">134</a>]</span>.</p>
<p>In the following I analysed the impact on performance when Gibbs sampling each sequence with 1, 5 and 10 full steps. As can be seen, there is hardly an impact on precision while having much longer runtimes (by a factor or 5 and 10).</p>

<div class="figure" style="text-align: center"><span id="fig:precision-cd-gibbs-steps"></span>
<iframe src="img/full_likelihood/precision_vs_rank_notitle_cd_comparing_nr_of_gibbs_sampling_steps.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 6.9: Performance of contrastive divergence optimization of the full likelihood with different number of Gibbs steps compared to pseudo-likelihood (blue) for 287 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. pseudo-likelihood: contact scores computed from pseudo-likelihood. The other methods derive contact scores from couplings computed from <a href="abbrev.html#abbrev">CD</a> with different number of Gibbs sampling steps.
</p>
</div>
<p>Another variant of <a href="abbrev.html#abbrev">CD</a> that has been suggested by Tieleman in 2008 is <a href="abbrev.html#abbrev"><em>PCD</em></a><span class="citation">[<a href="#ref-Tieleman2008">134</a>]</span> that does not reset the Markov Chains at every iteration. The reason being that when using small learning rates, the model changes only slightly between iterations and the true data distribution can be better approximated However, subsequent samples of <a href="abbrev.html#abbrev"><em>PCD</em></a> will be highly correlated creating a kind of momentum effect. Furthermore is has been found that <a href="abbrev.html#abbrev"><em>PCD</em></a> should be used with smaller learning rates and higher minibatch sizes.</p>
<p>As PCD might require samller update steps and larger minibatches, I analysed the performance of PCD for the default settings of CD and additionally for smaller learning and decay rates and larger minibatches. Note that one Markov chain is kept for every sequence of the input alignment. At each iteration a subset <span class="math inline">\(N^{\prime} &lt; N\)</span> of the Markov chains is randomly selected (without replacement) and used to for another round of Gibss sampling at the current iteration.</p>
<p>PLOT PCD for different LEARNIGN RATEWS and SAMPLE SIZES</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Ruder2017">
<p>129. Ruder, S. (2017). An overview of gradient descent optimization algorithms. arXiv. Available at: <a href="http://arxiv.org/abs/1609.04747" class="uri">http://arxiv.org/abs/1609.04747</a>.</p>
</div>
<div id="ref-Bottou2012">
<p>130. Bottou, L. (2012). Stochastic Gradient Descent Tricks. In Neural networks: Tricks of the trade (Springer, Berlin, Heidelberg), pp. 421–436. Available at: <a href="http://link.springer.com/10.1007/978-3-642-35289-8{\_}25" class="uri">http://link.springer.com/10.1007/978-3-642-35289-8{\_}25</a>.</p>
</div>
<div id="ref-Schaul2013">
<p>131. Schaul, T., Zhang, S., and Lecun, Y. (2013). No More Pesky Learning Rates. arXiv. Available at: <a href="https://arxiv.org/pdf/1206.1106.pdf" class="uri">https://arxiv.org/pdf/1206.1106.pdf</a>.</p>
</div>
<div id="ref-Carreira-Perpinan2005">
<p>132. Carreira-Perpiñán, M. a, and Hinton, G.E. (2005). On Contrastive Divergence Learning. Artif. Intell. Stat. <em>0</em>, 17. Available at: <a href="http://learning.cs.toronto.edu/{~}hinton/absps/cdmiguel.pdf" class="uri">http://learning.cs.toronto.edu/{~}hinton/absps/cdmiguel.pdf</a>.</p>
</div>
<div id="ref-Hinton2002">
<p>117. Hinton, G.E. (2002). Training Products of Experts by Minimizing Contrastive Divergence. Neural Comput. <em>14</em>, 1771–1800. Available at: <a href="http://www.gatsby.ucl.ac.uk/publications/tr/tr00-004.pdf" class="uri">http://www.gatsby.ucl.ac.uk/publications/tr/tr00-004.pdf</a>.</p>
</div>
<div id="ref-Bengio2009">
<p>133. Bengio, Y., and Delalleau, O. (2009). Justifying and Generalizing Contrastive Divergence. Neural Comput. <em>21</em>, 1601–21. Available at: <a href="http://www.iro.umontreal.ca/{~}lisa/publications2/index.php/attachments/single/105" class="uri">http://www.iro.umontreal.ca/{~}lisa/publications2/index.php/attachments/single/105</a>.</p>
</div>
<div id="ref-Tieleman2008">
<p>134. Tieleman, T. (2008). Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient. Proc. 25th Int. Conf. Mach. Learn. <em>307</em>, 7.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://contactpredictionthesis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="analysis-of-coupling-matrices.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-model-for-residue-resdiue-contact-prediction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/susannvorberg/phd_thesis/edit/master/04-methods.Rmd",
"text": "Edit"
},
"download": ["PhD_thesis_Susann_Vorberg.pdf"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
