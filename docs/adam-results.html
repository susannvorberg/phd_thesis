<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>PhD thesis: residue-residue contact prediction</title>
  <meta name="description" content="This is my PhD thesis on residue-residue contact prediction.">
  <meta name="generator" content="bookdown 0.3.9 and GitBook 2.6.7">

  <meta property="og:title" content="PhD thesis: residue-residue contact prediction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my PhD thesis on residue-residue contact prediction." />
  <meta name="github-repo" content="susannvorberg/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PhD thesis: residue-residue contact prediction" />
  
  <meta name="twitter:description" content="This is my PhD thesis on residue-residue contact prediction." />
  

<meta name="author" content="Susann Vorberg">


<meta name="date" content="2017-10-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="cd-sampling-optimization.html">
<link rel="next" href="comparing-pll-cd.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
  tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
  TeX: { 
    extensions: ["mediawiki-texvc.js", "sinuitx.js"],
    Macros: {
      Cb: "C_\\beta",
      eq: "\\!=\\!",
      Gauss: "\\mathcal{N}",
      H: "\\mathbf{H}",
      Hij : "\\H_{ij}",
      I: "\\mathbf{I}",
      Lijk: "\\mathbf{\\Lambda}_{ij,k}",
      Lk: "\\mathbf{\\Lambda}_k",
      LL: "L\\!L(\\mathbf{v}, \\mathbf{w})",
      LLreg: "L\\!L_\\mathrm{reg}",
      muijk: "\\mathbf{\\mu}_{ij,k}",
      muk: "\\mathbf{\\mu}_k",
      neff: "N_\\mathrm{eff}",
      r: "\\mathbf{r}",
      rij: "r_{ij}",
      c: "\\mathbf{c}",
      cij: "c_{ij}",
      seq: "\\mathbf{x}",
      Qij: "\\mathbf{Q}_{ij}",
      q: "\\mathbf{q}",
      qij: "\\mathbf{q'}_{ij}",
      Sn: "\\mathcal{S}_n",
      v: "\\mathbf{v}",
      vi: "\\mathcal{v}_{i}",
      vj: "\\mathcal{v}_{j}",
      via: "\\mathcal{v}_{ia}",
      vja: "\\mathcal{v}_{ja}",
      w: "\\mathbf{w}",
      wij: "\\mathbf{w}_{ij}",
      wijab: "\\mathcal{w}_{ijab}",
      wijcd: "\\mathcal{w}_{ijcd}",
      wklcd: "\\mathcal{w}_{klcd}",
      X: "\\mathbf{X}",
      angstrom: "\\AA \\; \\;"
      }
  }
});
</script>


 
<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js", "[siunitx]/siunitx.js"],
  TeX: { TagSide: "left" }
});
MathJax.Ajax.config.path['siunitx']  = '../latex/MathJax-siunitx-master/';
</script>
//-->



<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD Thesis Susann Vorberg</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>1</b> Background</a><ul>
<li class="chapter" data-level="1.1" data-path="general-intro.html"><a href="general-intro.html"><i class="fa fa-check"></i><b>1.1</b> Biological Background</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html"><i class="fa fa-check"></i><b>1.2</b> Introduction to Contact Prediction</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html#local-methods"><i class="fa fa-check"></i><b>1.2.1</b> Local Statistical Models</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html#global-methods"><i class="fa fa-check"></i><b>1.2.2</b> Global Statistical Models</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html#meta-predictors"><i class="fa fa-check"></i><b>1.2.3</b> Machine Learning Methods and Meta-Predictors</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html#maxent"><i class="fa fa-check"></i><b>1.2.4</b> Modelling Protein Families with Potts Model</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="application-contact-prediction.html"><a href="application-contact-prediction.html"><i class="fa fa-check"></i><b>1.3</b> Applications</a></li>
<li class="chapter" data-level="1.4" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html"><i class="fa fa-check"></i><b>1.4</b> Evaluating Contact Prediction Methods</a><ul>
<li class="chapter" data-level="1.4.1" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#seq-sep"><i class="fa fa-check"></i><b>1.4.1</b> Sequence Separation</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#interpretation-of-evaluation-results"><i class="fa fa-check"></i><b>1.4.2</b> Interpretation of Evaluation Results</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>1.5</b> Challenges for Coevolutionary Inference</a><ul>
<li class="chapter" data-level="1.5.1" data-path="challenges.html"><a href="challenges.html#phylogenetic-noise"><i class="fa fa-check"></i><b>1.5.1</b> Phylogenetic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="1.5.2" data-path="challenges.html"><a href="challenges.html#entropic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>1.5.2</b> Entropic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="1.5.3" data-path="challenges.html"><a href="challenges.html#finite-sampling-effects"><i class="fa fa-check"></i><b>1.5.3</b> Finite Sampling Effects</a></li>
<li class="chapter" data-level="1.5.4" data-path="challenges.html"><a href="challenges.html#multiple-sequence-alignments"><i class="fa fa-check"></i><b>1.5.4</b> Multiple Sequence Alignments</a></li>
<li class="chapter" data-level="1.5.5" data-path="challenges.html"><a href="challenges.html#alternative-sources-of-coevolution"><i class="fa fa-check"></i><b>1.5.5</b> Alternative Sources of Coevolution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpreting-coupling-matrices.html"><a href="interpreting-coupling-matrices.html"><i class="fa fa-check"></i><b>2</b> Interpretation of Coupling Matrices</a><ul>
<li class="chapter" data-level="2.1" data-path="correlation-between-couplings-and-class.html"><a href="correlation-between-couplings-and-class.html"><i class="fa fa-check"></i><b>2.1</b> Single Coupling Values Carry Evidence of Contacts</a></li>
<li class="chapter" data-level="2.2" data-path="coupling-profiles.html"><a href="coupling-profiles.html"><i class="fa fa-check"></i><b>2.2</b> Coupling Profiles Vary with Distance</a></li>
<li class="chapter" data-level="2.3" data-path="physico-chemical-fingerprints-in-coupling-matrices.html"><a href="physico-chemical-fingerprints-in-coupling-matrices.html"><i class="fa fa-check"></i><b>2.3</b> Physico-Chemical Fingerprints in Coupling Matrices</a></li>
<li class="chapter" data-level="2.4" data-path="higher-order-coupling-profiles.html"><a href="higher-order-coupling-profiles.html"><i class="fa fa-check"></i><b>2.4</b> Higher Order Dependencies Between Couplings</a></li>
<li class="chapter" data-level="2.5" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
<li class="chapter" data-level="2.6" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2.6</b> Methods</a><ul>
<li class="chapter" data-level="2.6.1" data-path="methods.html"><a href="methods.html#dataset"><i class="fa fa-check"></i><b>2.6.1</b> Dataset</a></li>
<li class="chapter" data-level="2.6.2" data-path="methods.html"><a href="methods.html#diff-ccmpred-ccmpredpy"><i class="fa fa-check"></i><b>2.6.2</b> Computing Pseudo-Likelihood Couplings</a></li>
<li class="chapter" data-level="2.6.3" data-path="methods.html"><a href="methods.html#seq-reweighting"><i class="fa fa-check"></i><b>2.6.3</b> Sequence Reweighting</a></li>
<li class="chapter" data-level="2.6.4" data-path="methods.html"><a href="methods.html#amino-acid-frequencies"><i class="fa fa-check"></i><b>2.6.4</b> Computing Amino Acid Frequencies</a></li>
<li class="chapter" data-level="2.6.5" data-path="methods.html"><a href="methods.html#methods-regularization"><i class="fa fa-check"></i><b>2.6.5</b> Regularization</a></li>
<li class="chapter" data-level="2.6.6" data-path="methods.html"><a href="methods.html#method-coupling-correlation"><i class="fa fa-check"></i><b>2.6.6</b> Correlation of Couplings with Contact Class</a></li>
<li class="chapter" data-level="2.6.7" data-path="methods.html"><a href="methods.html#method-coupling-profile"><i class="fa fa-check"></i><b>2.6.7</b> Coupling Distribution Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="optimizing-full-likelihood.html"><a href="optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>3</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="3.1" data-path="full-likelihood-gradient.html"><a href="full-likelihood-gradient.html"><i class="fa fa-check"></i><b>3.1</b> Approximating the Gradient of the Full Likelihood with Contrastive Divergence</a></li>
<li class="chapter" data-level="3.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html"><i class="fa fa-check"></i><b>3.2</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="3.2.1" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#convergence-criteria-sgd"><i class="fa fa-check"></i><b>3.2.1</b> Convergence Criterion for Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="3.2.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#sgd-hyperparameter-tuning"><i class="fa fa-check"></i><b>3.2.2</b> Tuning Hyperparameters of Stochastic Gradient Descent Optimizer</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html"><i class="fa fa-check"></i><b>3.3</b> Tuning the Gibbs Sampling Scheme for Contrastive Divergence</a><ul>
<li class="chapter" data-level="3.3.1" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#regularization-for-cd-with-sgd"><i class="fa fa-check"></i><b>3.3.1</b> Tuning Regularization Coefficients for Contrastive Divergence</a></li>
<li class="chapter" data-level="3.3.2" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-sampling-size"><i class="fa fa-check"></i><b>3.3.2</b> Varying the Sample Size</a></li>
<li class="chapter" data-level="3.3.3" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>3.3.3</b> Varying the number of Gibbs Steps</a></li>
<li class="chapter" data-level="3.3.4" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>3.3.4</b> Persistent Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="adam-results.html"><a href="adam-results.html"><i class="fa fa-check"></i><b>3.4</b> Using ADAM to Optimize Contrastive Divergence</a><ul>
<li class="chapter" data-level="3.4.1" data-path="adam-results.html"><a href="adam-results.html#adam-violates-sum-wij"><i class="fa fa-check"></i><b>3.4.1</b> A <em>Potts</em> model specific convergence criterion</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="comparing-pll-cd.html"><a href="comparing-pll-cd.html"><i class="fa fa-check"></i><b>3.5</b> Comparing CD couplings to pLL couplings</a><ul>
<li class="chapter" data-level="3.5.1" data-path="comparing-pll-cd.html"><a href="comparing-pll-cd.html#protein-1c75a00"><i class="fa fa-check"></i><b>3.5.1</b> Protein 1c75A00</a></li>
<li class="chapter" data-level="3.5.2" data-path="comparing-pll-cd.html"><a href="comparing-pll-cd.html#protein-1ss3a00-and-1c55a00"><i class="fa fa-check"></i><b>3.5.2</b> Protein 1ss3A00 and 1c55A00</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="discussion-1.html"><a href="discussion-1.html"><i class="fa fa-check"></i><b>3.6</b> Discussion</a></li>
<li class="chapter" data-level="3.7" data-path="methods-1.html"><a href="methods-1.html"><i class="fa fa-check"></i><b>3.7</b> Methods</a><ul>
<li class="chapter" data-level="3.7.1" data-path="methods-1.html"><a href="methods-1.html#potts-full-likelihood"><i class="fa fa-check"></i><b>3.7.1</b> The Potts Model</a></li>
<li class="chapter" data-level="3.7.2" data-path="methods-1.html"><a href="methods-1.html#gap-treatment"><i class="fa fa-check"></i><b>3.7.2</b> Treating Gaps as Missing Information</a></li>
<li class="chapter" data-level="3.7.3" data-path="methods-1.html"><a href="methods-1.html#the-regularized-full-log-likelihood-and-its-gradient-with-gap-treatment"><i class="fa fa-check"></i><b>3.7.3</b> The Regularized Full Log Likelihood and its Gradient With Gap Treatment</a></li>
<li class="chapter" data-level="3.7.4" data-path="methods-1.html"><a href="methods-1.html#prior-v"><i class="fa fa-check"></i><b>3.7.4</b> The prior on single potentials</a></li>
<li class="chapter" data-level="3.7.5" data-path="methods-1.html"><a href="methods-1.html#methods-sgd"><i class="fa fa-check"></i><b>3.7.5</b> Stochastic Gradien Descent</a></li>
<li class="chapter" data-level="3.7.6" data-path="methods-1.html"><a href="methods-1.html#methods-cd-sampling"><i class="fa fa-check"></i><b>3.7.6</b> Computing the Gradient with Contrastive Divergence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="contact-prior.html"><a href="contact-prior.html"><i class="fa fa-check"></i><b>4</b> Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="4.1" data-path="random-forest-classifiers.html"><a href="random-forest-classifiers.html"><i class="fa fa-check"></i><b>4.1</b> Random Forest Classifiers</a></li>
<li class="chapter" data-level="4.2" data-path="hyperparameter-optimization-for-random-forest.html"><a href="hyperparameter-optimization-for-random-forest.html"><i class="fa fa-check"></i><b>4.2</b> Hyperparameter Optimization for Random Forest</a></li>
<li class="chapter" data-level="4.3" data-path="evaluating-random-forest-model-as-contact-predictor.html"><a href="evaluating-random-forest-model-as-contact-predictor.html"><i class="fa fa-check"></i><b>4.3</b> Evaluating Random Forest Model as Contact Predictor</a></li>
<li class="chapter" data-level="4.4" data-path="using-contact-scores-as-additional-features.html"><a href="using-contact-scores-as-additional-features.html"><i class="fa fa-check"></i><b>4.4</b> Using Contact Scores as Additional Features</a></li>
<li class="chapter" data-level="4.5" data-path="discussion-2.html"><a href="discussion-2.html"><i class="fa fa-check"></i><b>4.5</b> Discussion</a></li>
<li class="chapter" data-level="4.6" data-path="methods-2.html"><a href="methods-2.html"><i class="fa fa-check"></i><b>4.6</b> Methods</a><ul>
<li class="chapter" data-level="4.6.1" data-path="methods-2.html"><a href="methods-2.html#seq-features"><i class="fa fa-check"></i><b>4.6.1</b> Features used to train Random Forest Model</a></li>
<li class="chapter" data-level="4.6.2" data-path="methods-2.html"><a href="methods-2.html#simple-contact-prior-with-respect-to-protein-length"><i class="fa fa-check"></i><b>4.6.2</b> Simple Contact Prior with Respect to Protein Length</a></li>
<li class="chapter" data-level="4.6.3" data-path="methods-2.html"><a href="methods-2.html#rf-training"><i class="fa fa-check"></i><b>4.6.3</b> Cross-validation for Random Forest Training</a></li>
<li class="chapter" data-level="4.6.4" data-path="methods-2.html"><a href="methods-2.html#rf-feature-selection"><i class="fa fa-check"></i><b>4.6.4</b> Feature Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayesian-approach.html"><a href="bayesian-approach.html"><i class="fa fa-check"></i><b>5</b> A Bayesian Statistical Model for Residue-Residue Contact Prediction</a><ul>
<li class="chapter" data-level="5.1" data-path="overview-posterior-distances.html"><a href="overview-posterior-distances.html"><i class="fa fa-check"></i><b>5.1</b> Computing the Posterior Probabiilty of a Contact</a></li>
<li class="chapter" data-level="5.2" data-path="coupling-prior.html"><a href="coupling-prior.html"><i class="fa fa-check"></i><b>5.2</b> Modelling the Prior Over Couplings Depending on Contact States</a></li>
<li class="chapter" data-level="5.3" data-path="bayesian-model-training-hyperparameters.html"><a href="bayesian-model-training-hyperparameters.html"><i class="fa fa-check"></i><b>5.3</b> Training the Hyperparameters in the Likelihood Function of Contact States</a></li>
<li class="chapter" data-level="5.4" data-path="training-hyperparameters-for-a-gaussian-mixture-with-three-components.html"><a href="training-hyperparameters-for-a-gaussian-mixture-with-three-components.html"><i class="fa fa-check"></i><b>5.4</b> Training Hyperparameters for a Gaussian Mixture with Three Components</a></li>
<li class="chapter" data-level="5.5" data-path="training-hyperparameters-for-a-gaussian-mixture-with-five-and-ten-components.html"><a href="training-hyperparameters-for-a-gaussian-mixture-with-five-and-ten-components.html"><i class="fa fa-check"></i><b>5.5</b> Training Hyperparameters for a Gaussian Mixture with Five and Ten Components</a></li>
<li class="chapter" data-level="5.6" data-path="posterior-of-rij.html"><a href="posterior-of-rij.html"><i class="fa fa-check"></i><b>5.6</b> Computing The Posterior Probability of Contacts</a><ul>
<li class="chapter" data-level="5.6.1" data-path="posterior-of-rij.html"><a href="posterior-of-rij.html#evaluation-of-the-bayesian-models-for-contact-prediction"><i class="fa fa-check"></i><b>5.6.1</b> Evaluation of the Bayesian models for contact-prediction</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="analysing-contact-maps.html"><a href="analysing-contact-maps.html"><i class="fa fa-check"></i><b>5.7</b> Analysing Contact Maps</a></li>
<li class="chapter" data-level="5.8" data-path="discussion-3.html"><a href="discussion-3.html"><i class="fa fa-check"></i><b>5.8</b> Discussion</a></li>
<li class="chapter" data-level="5.9" data-path="methods-3.html"><a href="methods-3.html"><i class="fa fa-check"></i><b>5.9</b> Methods</a><ul>
<li class="chapter" data-level="5.9.1" data-path="methods-3.html"><a href="methods-3.html#methods-coupling-prior"><i class="fa fa-check"></i><b>5.9.1</b> Modelling the Prior Over Couplings Depending on Contact States</a></li>
<li class="chapter" data-level="5.9.2" data-path="methods-3.html"><a href="methods-3.html#laplace-approx"><i class="fa fa-check"></i><b>5.9.2</b> Gaussian Approximation to the Posterior of Couplings</a></li>
<li class="chapter" data-level="5.9.3" data-path="methods-3.html"><a href="methods-3.html#likelihood-fct-distances"><i class="fa fa-check"></i><b>5.9.3</b> Integrating out the Hidden Variables to Obtain the Likelihood Function of the Contact States</a></li>
<li class="chapter" data-level="5.9.4" data-path="methods-3.html"><a href="methods-3.html#Hessian-offdiagonal"><i class="fa fa-check"></i><b>5.9.4</b> The Hessian off-diagonal Elements Carry a Negligible Signal</a></li>
<li class="chapter" data-level="5.9.5" data-path="methods-3.html"><a href="methods-3.html#neg-Hessian-computation"><i class="fa fa-check"></i><b>5.9.5</b> Efficiently Computing the negative Hessian of the regularized log-likelihood</a></li>
<li class="chapter" data-level="5.9.6" data-path="methods-3.html"><a href="methods-3.html#inv-lambda-ij-k"><i class="fa fa-check"></i><b>5.9.6</b> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></a></li>
<li class="chapter" data-level="5.9.7" data-path="methods-3.html"><a href="methods-3.html#gradient-muk"><i class="fa fa-check"></i><b>5.9.7</b> The gradient of the log likelihood with respect to <span class="math inline">\(\muk\)</span></a></li>
<li class="chapter" data-level="5.9.8" data-path="methods-3.html"><a href="methods-3.html#gradient-lambdak"><i class="fa fa-check"></i><b>5.9.8</b> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></a></li>
<li class="chapter" data-level="5.9.9" data-path="methods-3.html"><a href="methods-3.html#the-gradient-of-the-log-likelihood-with-respect-to-gamma_k"><i class="fa fa-check"></i><b>5.9.9</b> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></a></li>
<li class="chapter" data-level="5.9.10" data-path="methods-3.html"><a href="methods-3.html#bayesian-model-distances"><i class="fa fa-check"></i><b>5.9.10</b> Extending the Bayesian Statistical Model for the Prediction of Protein Residue-Residue Distances</a></li>
<li class="chapter" data-level="5.9.11" data-path="methods-3.html"><a href="methods-3.html#training-hyperparameters-bayesian-model"><i class="fa fa-check"></i><b>5.9.11</b> Training the Hyperparameters in the Likelihood Function of Contact States</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="conclusion-and-outlook.html"><a href="conclusion-and-outlook.html"><i class="fa fa-check"></i><b>6</b> Conclusion and Outlook</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="abbrev.html"><a href="abbrev.html"><i class="fa fa-check"></i><b>A</b> Abbreviations</a></li>
<li class="chapter" data-level="B" data-path="amino-acids.html"><a href="amino-acids.html"><i class="fa fa-check"></i><b>B</b> Amino Acid Alphabet</a></li>
<li class="chapter" data-level="C" data-path="dataset-properties.html"><a href="dataset-properties.html"><i class="fa fa-check"></i><b>C</b> Dataset Properties</a></li>
<li class="chapter" data-level="D" data-path="interpretation-of-coupling-matrices.html"><a href="interpretation-of-coupling-matrices.html"><i class="fa fa-check"></i><b>D</b> Interpretation of Coupling Matrices</a></li>
<li class="chapter" data-level="E" data-path="optimizing-full-likelihood-with-gradient-descent.html"><a href="optimizing-full-likelihood-with-gradient-descent.html"><i class="fa fa-check"></i><b>E</b> Optimizing Full Likelihood with Gradient Descent</a></li>
<li class="chapter" data-level="F" data-path="training-of-the-random-forest-contact-prior.html"><a href="training-of-the-random-forest-contact-prior.html"><i class="fa fa-check"></i><b>F</b> Training of the Random Forest Contact Prior</a></li>
<li class="chapter" data-level="G" data-path="bayesian-statistical-model-for-contact-prediction.html"><a href="bayesian-statistical-model-for-contact-prediction.html"><i class="fa fa-check"></i><b>G</b> Bayesian statistical model for contact prediction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PhD thesis: residue-residue contact prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="adam-results" class="section level2">
<h2><span class="header-section-number">3.4</span> Using ADAM to Optimize Contrastive Divergence</h2>
<p><em>ADAM</em> computes per-parameter adaptive learning rates including momentum. The default values have been found to work well in practice so that little parameter tuning is required (see methods section <a href="methods-1.html#methods-full-likelihood-adam">3.7.5.1</a> for details) <span class="citation">[<a href="#ref-Ruder2017">196</a>,<a href="#ref-Kingma2014">211</a>]</span>. However, I tested <em>ADAM</em> with different learning rates for the optimization with <a href="abbrev.html#abbrev">CD</a>-1 for protein 1mkcA00 (number of sequences = 142) and 1c75A00 (number of sequences = 28078) and found that both proteins are sensitive to the choice of learning rate. In contrast to plain stochastic gradient descent, with <em>ADAM</em> it is possible to use larger learning rates for proteins having large alignments, because the learning rate will be adapted to the magnitude of the gradient for every parameter individually. For protein 1mkcA00, with <a href="abbrev.html#abbrev">Neff</a>=96, a learning rate of 5e-3 quickly leads to convergence whereas for protein 1c75A00, having <a href="abbrev.html#abbrev">Neff</a>=16808, an even larger learning rate can be chosen to obtain quick convergence (see Appendix Figure <a href="optimizing-full-likelihood-with-gradient-descent.html#fig:adam-learning-rate">E.16</a>). Therefore, I again specified the learning rate as a function of <a href="abbrev.html#abbrev">Neff</a>, <span class="math inline">\(\alpha = 2\mathrm{e}{-3}\log(\text{N}_{\text{eff}})\)</span>, such that for small <a href="abbrev.html#abbrev">Neff</a>, e.g. 5th percentile of the distribution in the dataset <span class="math inline">\(\approx 50\)</span>, this definition of the learning rate yields <span class="math inline">\(\alpha_0 \approx 8\mathrm{e}{-3}\)</span> and for large <a href="abbrev.html#abbrev">Neff</a>, e.g. 95th percentile <span class="math inline">\(\approx 15000\)</span>, this yields <span class="math inline">\(\alpha_0 \approx 2\mathrm{e}{-2}\)</span>.</p>
<p>It is interesting to note, that the norm of the coupling parameters, <span class="math inline">\(||\w||_2\)</span>, converges towards different values depending on the choice of the learning rate (see Appendix Figure <a href="optimizing-full-likelihood-with-gradient-descent.html#fig:adam-learning-rate">E.16</a>. By default, <em>ADAM</em> uses a constant learning rate, because the algorithm performs a kind of step size annealing by nature. However, popular implementations of <em>ADAM</em> in the <a href="https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L385">Keras</a> <span class="citation">[<a href="#ref-Chollet2015">212</a>]</span> and <a href="https://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py#L565-L629">Lasagne</a> <span class="citation">[<a href="#ref-Dieleman2015">213</a>]</span> packages allow the use of an annealing schedule. I therefore tested <em>ADAM</em> with a sigmoidal learning rate annealing schedule which already gave good results for <a href="abbrev.html#abbrev">SGD</a> (see section <a href="full-likelihood-optimization.html#sgd-hyperparameter-tuning">3.2.2</a>). Indeed, as can be seen in Appendix Figure <a href="optimizing-full-likelihood-with-gradient-descent.html#fig:adam-learning-rate-annealing">E.17</a>, when <em>ADAM</em> is used with a sigmoidal decay of the learning rate, the L2-norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span> converges roughly towards the same value. For the following analysis I used <em>ADAM</em> with a learning rate defined as a function of <a href="abbrev.html#abbrev">Neff</a> and a sigmoidal learning rate annealing schedule with decay rate <span class="math inline">\(\gamma \eq 5e-6\)</span>.</p>

<p>I evaluated <a href="abbrev.html#abbrev">CD</a>-1, <a href="abbrev.html#abbrev">CD</a>-10 and persistent contrastive divergence. As before, I will also evaluate a combination of both, such that <a href="abbrev.html#abbrev">PCD</a> is switched on, when the relative change of the norm of coupling parameters, <span class="math inline">\(||\w||_2\)</span>, falls below a small threshold. Figure <a href="adam-results.html#fig:precision-cd-adam">3.18</a> shows the benchmark for training the various modified <a href="abbrev.html#abbrev">CD</a> models with the <em>ADAM</em> optimizer. Overall, the predictive performance for <a href="abbrev.html#abbrev">CD</a> and <a href="abbrev.html#abbrev">PCD</a> did not improve by using the <em>ADAM</em> optimzier instead of the manually tuned stochastic gradient descent optimizer. Therefore it can be concluded that adaptive learning rates and momentum do not provide an essential advantage for inferring <em>Potts</em> model parameters with <a href="abbrev.html#abbrev">CD</a> and <a href="abbrev.html#abbrev">PCD</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:precision-cd-adam"></span>
<iframe src="img/full_likelihood/adam/precision_vs_rank_adam_gibbs_pcd.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 3.18: Mean precision for top ranked contact predictions over 300 proteins. Contact score is computed as <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings. <strong>pseudo-likelihood</strong>: couplings computed from pseudo-likelihood. <strong>CD ADAM</strong>: couplings computed from contrastive divergence using <em>ADAM</em> optimizer. <strong>CD ADAM #Gibbs steps = 10</strong>: couplings computed from contrastive divergence using <em>ADAM</em> optimizer and 10 Gibbs steps for sampling sequences. <strong>pCD ADAM</strong>: couplings computed from persistent contrastive divergence usign <em>ADAM</em> optimizer. <strong>pCD ADAM start = 1e-3</strong>: <em>ADAM</em> starts by optimizing the full likelihood using the <a href="abbrev.html#abbrev">CD</a> gradient estimate and switches to the <a href="abbrev.html#abbrev">PCD</a> gradient estimate once the relative change of L2 norm of parameters has fallen below <span class="math inline">\(\epsilon \eq \mathrm{1e}{-3}\)</span> evaluated over the last 10 iterations. <strong>pCD ADAM start = 1e-5</strong>: same as “pCD ADAM start = 1e-3” but <a href="abbrev.html#abbrev">PCD</a> is switched on for <span class="math inline">\(\epsilon \eq \mathrm{1e}{-5}\)</span>
</p>
</div>
<p>The convergence analysis for the two example proteins 1ahoA00 and 1c75A00 reveals, that optimization with <em>ADAM</em> converges towards similar offsets as optimization with plain <a href="abbrev.html#abbrev">SGD</a> with respect to the L2 norm of coupling parameters. For 1ahoA00, with low <a href="abbrev.html#abbrev">Neff</a>=229, the L2 norm of the parameters converges towards <span class="math inline">\(||\w||_2 \approx 21.6\)</span> when using <a href="abbrev.html#abbrev">CD</a>-1 and towards <span class="math inline">\(||\w||_2 \approx 24\)</span> when using <a href="abbrev.html#abbrev">PCD</a> or <a href="abbrev.html#abbrev">CD</a>-k with k&gt;1 (compare left plots in Figures <a href="adam-results.html#fig:adam-gibbs-pcd">3.19</a> and <a href="cd-sampling-optimization.html#fig:pcd-single-proteins">3.17</a>). For protein 1c75A00, with high <a href="abbrev.html#abbrev">Neff</a>=16808, <em>ADAM</em> seems to find distinct optima that are clearly separated in contrast to using plain <a href="#abrev">SGD</a>. When using <em>ADAM</em> with <a href="abbrev.html#abbrev">CD</a>-1 the algorithm converges towards <span class="math inline">\(||\w||_2 \approx 120\)</span>, <em>ADAM</em> with <a href="abbrev.html#abbrev">CD</a>-5 converges towards <span class="math inline">\(||\w||_2 \approx 130\)</span> and with <a href="abbrev.html#abbrev">CD</a>-10 towards <span class="math inline">\(||\w||_2 \approx 131\)</span>. And using <em>ADAM</em> with <a href="#abrrev">PCD</a>, regardless of whether the <a href="#abrrev">PCD</a> gradient estimate is used right from the start of optimization or only later, the algorithm converges towards <span class="math inline">\(||\w||_2 \approx 134\)</span>. Therefore, <em>ADAM</em> establishes the clear trend that longer sampling, or sampling with persistent chaisn results in larger parameter estimates.</p>

<div class="figure" style="text-align: center"><span id="fig:adam-gibbs-pcd"></span>
<img src="img/full_likelihood/adam/1ahoA00_parameter_norm_pcd_gibbs.png" alt="Monitoring the L2 norm of coupling parameters,\(||\w||_2\), for protein 1ahoA00 and 1c75A00 during optimization of CD and PCD with the ADAM optimizer. Contrastive Divergence (CD in legend) is optimized employing a different number of Gibbs steps that are specified in the legend. Persistent contrastive divergence (pCD in legend) uses one Gibbs step. “pCD start= X” indicates that optimization starts by using the CD gradient estimate and switches to the PCD gradient estimate once the relative change of L2 norm of parameters has fallen below a small threshold over the last 10 iterations. The threshold is given in the legend. Left Protein 1ahoA00 has length L=64 and 378 sequences in the alignment (Neff=229) Right Protein 1c75A00 has length L=71 and 28078 sequences in the alignment (Neff=16808)." width="49%" /><img src="img/full_likelihood/adam/1c75A00_parameter_norm_pcd_gibbs.png" alt="Monitoring the L2 norm of coupling parameters,\(||\w||_2\), for protein 1ahoA00 and 1c75A00 during optimization of CD and PCD with the ADAM optimizer. Contrastive Divergence (CD in legend) is optimized employing a different number of Gibbs steps that are specified in the legend. Persistent contrastive divergence (pCD in legend) uses one Gibbs step. “pCD start= X” indicates that optimization starts by using the CD gradient estimate and switches to the PCD gradient estimate once the relative change of L2 norm of parameters has fallen below a small threshold over the last 10 iterations. The threshold is given in the legend. Left Protein 1ahoA00 has length L=64 and 378 sequences in the alignment (Neff=229) Right Protein 1c75A00 has length L=71 and 28078 sequences in the alignment (Neff=16808)." width="49%" />
<p class="caption">
Figure 3.19: Monitoring the L2 norm of coupling parameters,<span class="math inline">\(||\w||_2\)</span>, for protein 1ahoA00 and 1c75A00 during optimization of <a href="abbrev.html#abbrev">CD</a> and <a href="abbrev.html#abbrev">PCD</a> with the <em>ADAM</em> optimizer. Contrastive Divergence (CD in legend) is optimized employing a different number of Gibbs steps that are specified in the legend. Persistent contrastive divergence (pCD in legend) uses one Gibbs step. “pCD start= X” indicates that optimization starts by using the <a href="abbrev.html#abbrev">CD</a> gradient estimate and switches to the <a href="abbrev.html#abbrev">PCD</a> gradient estimate once the relative change of L2 norm of parameters has fallen below a small threshold over the last 10 iterations. The threshold is given in the legend. <strong>Left</strong> Protein 1ahoA00 has length L=64 and 378 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=229) <strong>Right</strong> Protein 1c75A00 has length L=71 and 28078 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=16808).
</p>
</div>
<div id="adam-violates-sum-wij" class="section level3">
<h3><span class="header-section-number">3.4.1</span> A <em>Potts</em> model specific convergence criterion</h3>
<p>For the <em>Potts</em> model there exists a necessary but not sufficient condition that is satisfied at the optimum when the gradient is zero (derived in method section <a href="methods-1.html#prior-v">3.7.4</a>) and that is given by, <span class="math inline">\(\sum_{a,b=1}^{20} \wijab = 0\)</span>. This condition is never violated, as long as parameters satisfy this criterion at initialization and the same step size is used to update all parameters. To understand why, note that the 400 partial derivatives <span class="math inline">\(\frac{\partial L\!L(\v^*, \w)}{\partial \wijab}\)</span> for a residue pair <span class="math inline">\((i,j)\)</span> and for <span class="math inline">\(a,b \in \{1, \ldots, 20\}\)</span> are not independent. The sum over the 400 pairwise amino acid counts at positions <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is identical for the observed and the sampled alignment and amounts to, <span class="math inline">\(\sum_{a,b=1}^{20} N_{ij} q(x_i \eq a, q_j \eq b) = N_{ij}\)</span>.</p>
<p>Considering a residue pair <span class="math inline">\((i,j)\)</span> and assuming amino acid pair <span class="math inline">\((a,b)\)</span> has higher counts in the sampled alignment than in the observed input alignment, then this difference in counts must be compensated by other amino acid pairs <span class="math inline">\((c,d)\)</span> having less counts in the sampled alignment compared to the true alignment (see Figure <a href="adam-results.html#fig:visualisation-wijab-constraint">3.20</a>). Therefore, it holds <span class="math inline">\(\sum_{a,b=1}^{20} \frac{\partial L\!L(\v^*, \w)}{\partial \wijab} = 0\)</span>. This symmetry is translated into parameter updates as long as the same step size is used to update all parameters. However, when using adaptive learning rates, e.g. with the <em>ADAM</em> optimizer, this symmetry is broken and the condition <span class="math inline">\(\sum_{a,b=1}^{20} \wijab = 0\)</span> can be violated during the optimization processs.</p>

<div class="figure" style="text-align: center"><span id="fig:visualisation-wijab-constraint"></span>
<img src="img/full_likelihood/constraint_wijab.png" alt="The 400 partial derivatives \(\frac{\partial \LLreg(\v^*,\w)}{\partial \wijab}\) at position \((i,j)\) for \(a,b \in \{1, \ldots, 20 \}\) are not independent. Red bars represent pairwise amino acid counts at position \((i,j)\) for the empirical alignment. Blue bars represent pairwise amino acid counts at position \((i,j)\) for the sampled alignment. The sum over pairwise amino acid counts at position \((i,j)\) for both alignments is \(N_{ij}\), which is the number of ungapped sequences. The partial derivative for \(\wijab\) is computed as the difference of pairwise amino acid counts for amino acids \(a\) and \(b\) at position \((i,j)\). The sum over the partial derivatives \(\frac{\partial \LLreg(\v^*,\w)}{\partial \wijab}\) at position \((i,j)\) for all \(a,b \in \{1, \ldots, 20 \}\) is zero." width="70%" />
<p class="caption">
Figure 3.20: The 400 partial derivatives <span class="math inline">\(\frac{\partial \LLreg(\v^*,\w)}{\partial \wijab}\)</span> at position <span class="math inline">\((i,j)\)</span> for <span class="math inline">\(a,b \in \{1, \ldots, 20 \}\)</span> are not independent. Red bars represent pairwise amino acid counts at position <span class="math inline">\((i,j)\)</span> for the empirical alignment. Blue bars represent pairwise amino acid counts at position <span class="math inline">\((i,j)\)</span> for the sampled alignment. The sum over pairwise amino acid counts at position <span class="math inline">\((i,j)\)</span> for both alignments is <span class="math inline">\(N_{ij}\)</span>, which is the number of ungapped sequences. The partial derivative for <span class="math inline">\(\wijab\)</span> is computed as the difference of pairwise amino acid counts for amino acids <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> at position <span class="math inline">\((i,j)\)</span>. The sum over the partial derivatives <span class="math inline">\(\frac{\partial \LLreg(\v^*,\w)}{\partial \wijab}\)</span> at position <span class="math inline">\((i,j)\)</span> for all <span class="math inline">\(a,b \in \{1, \ldots, 20 \}\)</span> is zero.
</p>
</div>
<p>For proteins 1ahoA00 and 1c75A00, Figure <a href="adam-results.html#fig:adam-gibbs-pcd-sumwijab">3.21</a> shows the number of residue pairs for which this condition is violated according to <span class="math inline">\(|\sum_{a,b=1}^{20} \wijab| &gt; \mathrm{1e}{-2}\)</span>, during optimization with <em>ADAM</em>. For about half out of the 2016 residue pairs in protein 1ahoA00 the condition is violated at the end of optimization. For protein 1c75A00 it is about 2300 out of the 2485 residue pairs. Whereas this is not a problem when computing the contact score based on the Frobenius norm of the coupling matrix, it is problematic when utilizing the couplings in the Bayesian framework presented in section <a href="bayesian-approach.html#bayesian-approach">5</a>, which requires the condition <span class="math inline">\(\sum_{a,b=1}^{20} \wijab = 0\)</span> to hold.</p>

<div class="figure" style="text-align: center"><span id="fig:adam-gibbs-pcd-sumwijab"></span>
<img src="img/full_likelihood/adam/1ahoA00_number_ij_sumwijuneqzero.png" alt="Monitoring the number of residue pairs for which \(|\sum_{a,b=1}^{20} \wijab| &gt; \mathrm{1e}{-2}\). Legend is the same as in Figure 3.19. Left Protein 1ahoA00 has length L=64 and 378 sequences in the alignment (Neff=229) Right Protein 1c75A00 has length L=71 and 28078 sequences in the alignment (Neff=16808)." width="49%" /><img src="img/full_likelihood/adam/1c75A00_number_ij_sumwijuneqzero.png" alt="Monitoring the number of residue pairs for which \(|\sum_{a,b=1}^{20} \wijab| &gt; \mathrm{1e}{-2}\). Legend is the same as in Figure 3.19. Left Protein 1ahoA00 has length L=64 and 378 sequences in the alignment (Neff=229) Right Protein 1c75A00 has length L=71 and 28078 sequences in the alignment (Neff=16808)." width="49%" />
<p class="caption">
Figure 3.21: Monitoring the number of residue pairs for which <span class="math inline">\(|\sum_{a,b=1}^{20} \wijab| &gt; \mathrm{1e}{-2}\)</span>. Legend is the same as in Figure <a href="adam-results.html#fig:adam-gibbs-pcd">3.19</a>. <strong>Left</strong> Protein 1ahoA00 has length L=64 and 378 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=229) <strong>Right</strong> Protein 1c75A00 has length L=71 and 28078 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=16808).
</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Ruder2017">
<p>196. Ruder, S. (2017). An overview of gradient descent optimization algorithms. arXiv.</p>
</div>
<div id="ref-Kingma2014">
<p>211. Kingma, D., and Ba, J. (2014). Adam: A Method for Stochastic Optimization.</p>
</div>
<div id="ref-Chollet2015">
<p>212. Chollet, F. others (2015). Keras.</p>
</div>
<div id="ref-Dieleman2015">
<p>213. Dieleman, S., Schlüter, J., Raffel, C., Olson, E., Sønderby, S.K., Nouri, D., Maturana, D., Thoma, M., Battenberg, E., and Kelly, J. <em>et al.</em> (2015). Lasagne: First release., doi: <a href="https://doi.org/10.5281/ZENODO.27878">10.5281/ZENODO.27878</a>.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://contactpredictionthesis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="cd-sampling-optimization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="comparing-pll-cd.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/susannvorberg/phd_thesis/edit/master/06-optimizing_full_likelihood.Rmd",
"text": "Edit"
},
"download": ["PhD_thesis_Susann_Vorberg.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
