<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>PhD thesis: residue-residue contact prediction</title>
  <meta name="description" content="This is my PhD thesis on residue-residue contact prediction.">
  <meta name="generator" content="bookdown 0.3.9 and GitBook 2.6.7">

  <meta property="og:title" content="PhD thesis: residue-residue contact prediction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my PhD thesis on residue-residue contact prediction." />
  <meta name="github-repo" content="susannvorberg/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PhD thesis: residue-residue contact prediction" />
  
  <meta name="twitter:description" content="This is my PhD thesis on residue-residue contact prediction." />
  

<meta name="author" content="Susann Vorberg">


<meta name="date" content="2017-10-20">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regularization-for-cd-with-sgd.html">
<link rel="next" href="adam-results.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
  tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
  TeX: { 
    extensions: ["mediawiki-texvc.js", "sinuitx.js"],
    Macros: {
      Cb: "C_\\beta",
      eq: "\\!=\\!",
      Gauss: "\\mathcal{N}",
      H: "\\mathbf{H}",
      Hij : "\\H_{ij}",
      I: "\\mathbf{I}",
      Lijk: "\\mathbf{\\Lambda}_{ij,k}",
      Lk: "\\mathbf{\\Lambda}_k",
      LL: "L\\!L(\\mathbf{v}, \\mathbf{w})",
      LLreg: "L\\!L_\\mathrm{reg}",
      muijk: "\\mathbf{\\mu}_{ij,k}",
      muk: "\\mathbf{\\mu}_k",
      neff: "N_\\mathrm{eff}",
      r: "\\mathbf{r}",
      rij: "r_{ij}",
      c: "\\mathbf{c}",
      cij: "c_{ij}",
      seq: "\\mathbf{x}",
      Qij: "\\mathbf{Q}_{ij}",
      q: "\\mathbf{q}",
      qij: "\\mathbf{q'}_{ij}",
      Sn: "\\mathcal{S}_n",
      v: "\\mathbf{v}",
      vi: "\\mathcal{v}_{i}",
      vj: "\\mathcal{v}_{j}",
      via: "\\mathcal{v}_{ia}",
      vja: "\\mathcal{v}_{ja}",
      w: "\\mathbf{w}",
      wij: "\\mathbf{w}_{ij}",
      wijab: "\\mathcal{w}_{ijab}",
      wijcd: "\\mathcal{w}_{ijcd}",
      wklcd: "\\mathcal{w}_{klcd}",
      X: "\\mathbf{X}",
      angstrom: "\\AA \\; \\;"
      }
  }
});
</script>


 
<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js", "[siunitx]/siunitx.js"],
  TeX: { TagSide: "left" }
});
MathJax.Ajax.config.path['siunitx']  = '../latex/MathJax-siunitx-master/';
</script>
//-->



<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD Thesis Susann Vorberg</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="general-intro.html"><a href="general-intro.html"><i class="fa fa-check"></i><b>1</b> Biological Background</a></li>
<li class="chapter" data-level="2" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html"><i class="fa fa-check"></i><b>2</b> Introduction to Contact Prediction</a><ul>
<li class="chapter" data-level="2.1" data-path="local-methods.html"><a href="local-methods.html"><i class="fa fa-check"></i><b>2.1</b> Local Statistical Models</a></li>
<li class="chapter" data-level="2.2" data-path="global-methods.html"><a href="global-methods.html"><i class="fa fa-check"></i><b>2.2</b> Global Statistical Models</a></li>
<li class="chapter" data-level="2.3" data-path="meta-predictors.html"><a href="meta-predictors.html"><i class="fa fa-check"></i><b>2.3</b> Machine Learning Methods and Meta-Predictors</a></li>
<li class="chapter" data-level="2.4" data-path="maxent.html"><a href="maxent.html"><i class="fa fa-check"></i><b>2.4</b> Modelling Protein Families with Potts Model</a><ul>
<li class="chapter" data-level="2.4.1" data-path="maxent.html"><a href="maxent.html#potts-model-properties"><i class="fa fa-check"></i><b>2.4.1</b> Model Properties</a></li>
<li class="chapter" data-level="2.4.2" data-path="maxent.html"><a href="maxent.html#potts-mle"><i class="fa fa-check"></i><b>2.4.2</b> Inferring Parameters for the Potts Model</a></li>
<li class="chapter" data-level="2.4.3" data-path="maxent.html"><a href="maxent.html#potts-model-solutions"><i class="fa fa-check"></i><b>2.4.3</b> Solving the Inverse Potts Problem</a></li>
<li class="chapter" data-level="2.4.4" data-path="maxent.html"><a href="maxent.html#post-processing-heuristics"><i class="fa fa-check"></i><b>2.4.4</b> Computing Contact Maps</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="application-contact-prediction.html"><a href="application-contact-prediction.html"><i class="fa fa-check"></i><b>2.5</b> Applications</a></li>
<li class="chapter" data-level="2.6" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html"><i class="fa fa-check"></i><b>2.6</b> Evaluating Contact Prediction Methods</a><ul>
<li class="chapter" data-level="2.6.1" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#seq-sep"><i class="fa fa-check"></i><b>2.6.1</b> Sequence Separation</a></li>
<li class="chapter" data-level="2.6.2" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#interpretation-of-evaluation-results"><i class="fa fa-check"></i><b>2.6.2</b> Interpretation of Evaluation Results</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>2.7</b> Challenges for Coevolutionary Inference</a><ul>
<li class="chapter" data-level="2.7.1" data-path="challenges.html"><a href="challenges.html#phylogenetic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>2.7.1</b> Phylogenetic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="2.7.2" data-path="challenges.html"><a href="challenges.html#entropic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>2.7.2</b> Entropic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="2.7.3" data-path="challenges.html"><a href="challenges.html#finite-sampling-effects"><i class="fa fa-check"></i><b>2.7.3</b> Finite Sampling Effects</a></li>
<li class="chapter" data-level="2.7.4" data-path="challenges.html"><a href="challenges.html#multiple-sequence-alignments"><i class="fa fa-check"></i><b>2.7.4</b> Multiple Sequence Alignments</a></li>
<li class="chapter" data-level="2.7.5" data-path="challenges.html"><a href="challenges.html#alternative-sources-of-coevolution"><i class="fa fa-check"></i><b>2.7.5</b> Alternative Sources of Coevolution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpreting-coupling-matrices.html"><a href="interpreting-coupling-matrices.html"><i class="fa fa-check"></i><b>3</b> Interpretation of Coupling Matrices</a><ul>
<li class="chapter" data-level="3.1" data-path="correlation-between-couplings-and-class.html"><a href="correlation-between-couplings-and-class.html"><i class="fa fa-check"></i><b>3.1</b> Single Coupling Values Carry Evidence of Contacts</a></li>
<li class="chapter" data-level="3.2" data-path="physico-chemical-fingerprints-in-coupling-matrices.html"><a href="physico-chemical-fingerprints-in-coupling-matrices.html"><i class="fa fa-check"></i><b>3.2</b> Physico-Chemical Fingerprints in Coupling Matrices</a></li>
<li class="chapter" data-level="3.3" data-path="coupling-profiles.html"><a href="coupling-profiles.html"><i class="fa fa-check"></i><b>3.3</b> Coupling Profiles Vary with Distance</a></li>
<li class="chapter" data-level="3.4" data-path="higher-order-coupling-profiles.html"><a href="higher-order-coupling-profiles.html"><i class="fa fa-check"></i><b>3.4</b> Higher Order Dependencies Between Couplings</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="optimizing-full-likelihood.html"><a href="optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>4</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="4.1" data-path="full-likelihood-gradient.html"><a href="full-likelihood-gradient.html"><i class="fa fa-check"></i><b>4.1</b> Approximating the Gradient of the Full Likelihood with Contrastive Divergence</a></li>
<li class="chapter" data-level="4.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html"><i class="fa fa-check"></i><b>4.2</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="4.2.1" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#convergence-criteria-sgd"><i class="fa fa-check"></i><b>4.2.1</b> Convergence Criterion for Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="4.2.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#sgd-hyperparameter-tuning"><i class="fa fa-check"></i><b>4.2.2</b> Tuning Hyperparameters of Stochastic Gradient Descent Optimizer</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="regularization-for-cd-with-sgd.html"><a href="regularization-for-cd-with-sgd.html"><i class="fa fa-check"></i><b>4.3</b> Tuning Regularization Coefficients for Contrastive Divergence</a></li>
<li class="chapter" data-level="4.4" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html"><i class="fa fa-check"></i><b>4.4</b> Tuning the Gibbs Sampling Scheme for Contrastive Divergence</a><ul>
<li class="chapter" data-level="4.4.1" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-sampling-size"><i class="fa fa-check"></i><b>4.4.1</b> Varying the Sample Size</a></li>
<li class="chapter" data-level="4.4.2" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>4.4.2</b> Varying the number of Gibbs Steps</a></li>
<li class="chapter" data-level="4.4.3" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>4.4.3</b> Persistent Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="adam-results.html"><a href="adam-results.html"><i class="fa fa-check"></i><b>4.5</b> Using ADAM to optimize Contrastive Divergence</a></li>
<li class="chapter" data-level="4.6" data-path="comparing-cd-couplings-to-pll-couplings.html"><a href="comparing-cd-couplings-to-pll-couplings.html"><i class="fa fa-check"></i><b>4.6</b> Comparing CD couplings to pLL couplings</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><a href="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><i class="fa fa-check"></i><b>5</b> A Bayesian Statistical Model for Residue-Residue Contact Prediction</a><ul>
<li class="chapter" data-level="5.1" data-path="overview-posterior-distances.html"><a href="overview-posterior-distances.html"><i class="fa fa-check"></i><b>5.1</b> Computing the Posterior Probabiilty of a Contact <span class="math inline">\(p(\c \eq 1 | \X)\)</span></a></li>
<li class="chapter" data-level="5.2" data-path="laplace-approx.html"><a href="laplace-approx.html"><i class="fa fa-check"></i><b>5.2</b> Gaussian approximation to the posterior of couplings</a><ul>
<li class="chapter" data-level="5.2.1" data-path="laplace-approx.html"><a href="laplace-approx.html#laplace-approx-improvement"><i class="fa fa-check"></i><b>5.2.1</b> Iterative improvement of Laplace approximation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="coupling-prior.html"><a href="coupling-prior.html"><i class="fa fa-check"></i><b>5.3</b> Modelling the prior over couplings with dependence on <span class="math inline">\(\cij\)</span></a></li>
<li class="chapter" data-level="5.4" data-path="likelihood-fct-distances.html"><a href="likelihood-fct-distances.html"><i class="fa fa-check"></i><b>5.4</b> Computing the likelihood function of contact states <span class="math inline">\(p(\X | \c)\)</span></a></li>
<li class="chapter" data-level="5.5" data-path="bayesian-model-training-hyperparameters.html"><a href="bayesian-model-training-hyperparameters.html"><i class="fa fa-check"></i><b>5.5</b> Training the Hyperparameters in the Likelihood Function of Contact States</a></li>
<li class="chapter" data-level="5.6" data-path="posterior-of-rij.html"><a href="posterior-of-rij.html"><i class="fa fa-check"></i><b>5.6</b> The posterior probability distribution for contact states <span class="math inline">\(\cij\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="contact-prior.html"><a href="contact-prior.html"><i class="fa fa-check"></i><b>6</b> Contact Prior</a><ul>
<li class="chapter" data-level="6.1" data-path="random-forest-classifiers.html"><a href="random-forest-classifiers.html"><i class="fa fa-check"></i><b>6.1</b> Random Forest Classifiers</a></li>
<li class="chapter" data-level="6.2" data-path="hyperparameter-optimization-for-random-forest.html"><a href="hyperparameter-optimization-for-random-forest.html"><i class="fa fa-check"></i><b>6.2</b> Hyperparameter Optimization for Random Forest</a></li>
<li class="chapter" data-level="6.3" data-path="evaluating-random-forest-model-as-contact-predictor.html"><a href="evaluating-random-forest-model-as-contact-predictor.html"><i class="fa fa-check"></i><b>6.3</b> Evaluating Random Forest Model as Contact Predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>7</b> Methods</a><ul>
<li class="chapter" data-level="7.1" data-path="dataset.html"><a href="dataset.html"><i class="fa fa-check"></i><b>7.1</b> Dataset</a></li>
<li class="chapter" data-level="7.2" data-path="computing-pseudo-likelihood-couplings.html"><a href="computing-pseudo-likelihood-couplings.html"><i class="fa fa-check"></i><b>7.2</b> Computing Pseudo-Likelihood Couplings</a><ul>
<li class="chapter" data-level="7.2.1" data-path="computing-pseudo-likelihood-couplings.html"><a href="computing-pseudo-likelihood-couplings.html#diff-ccmpred-ccmpredpy"><i class="fa fa-check"></i><b>7.2.1</b> Differences between CCMpred and CCMpredpy</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="seq-reweighting.html"><a href="seq-reweighting.html"><i class="fa fa-check"></i><b>7.3</b> Sequence Reweighting</a></li>
<li class="chapter" data-level="7.4" data-path="amino-acid-frequencies.html"><a href="amino-acid-frequencies.html"><i class="fa fa-check"></i><b>7.4</b> Computing Amino Acid Frequencies</a></li>
<li class="chapter" data-level="7.5" data-path="methods-regularization.html"><a href="methods-regularization.html"><i class="fa fa-check"></i><b>7.5</b> Regularization</a></li>
<li class="chapter" data-level="7.6" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html"><i class="fa fa-check"></i><b>7.6</b> The Potts Model</a><ul>
<li class="chapter" data-level="7.6.1" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html#gap-treatment"><i class="fa fa-check"></i><b>7.6.1</b> Treating Gaps as Missing Information</a></li>
<li class="chapter" data-level="7.6.2" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html#the-regularized-full-log-likelihood-and-its-gradient-with-gap-treatment"><i class="fa fa-check"></i><b>7.6.2</b> The Regularized Full Log Likelihood and its Gradient With Gap Treatment</a></li>
<li class="chapter" data-level="7.6.3" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html#prior-v"><i class="fa fa-check"></i><b>7.6.3</b> The prior on <span class="math inline">\(\v\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html"><i class="fa fa-check"></i><b>7.7</b> Analysis of Coupling Matrices</a><ul>
<li class="chapter" data-level="7.7.1" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-correlation"><i class="fa fa-check"></i><b>7.7.1</b> Correlation of Couplings with Contact Class</a></li>
<li class="chapter" data-level="7.7.2" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-profile"><i class="fa fa-check"></i><b>7.7.2</b> Coupling Distribution Plots</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="methods-sgd.html"><a href="methods-sgd.html"><i class="fa fa-check"></i><b>7.8</b> Optimizing Contrastive Divergence with Stochastic Gradient Descent</a><ul>
<li class="chapter" data-level="7.8.1" data-path="methods-sgd.html"><a href="methods-sgd.html#methods-full-likelihood-adam"><i class="fa fa-check"></i><b>7.8.1</b> Tuning Hyperparameters of <em>ADAM</em> Optimizer</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="methods-cd-sampling.html"><a href="methods-cd-sampling.html"><i class="fa fa-check"></i><b>7.9</b> Computing the Gradient with Contrastive Divergence</a></li>
<li class="chapter" data-level="7.10" data-path="Hessian-offdiagonal.html"><a href="Hessian-offdiagonal.html"><i class="fa fa-check"></i><b>7.10</b> The Hessian off-diagonal Elements Carry a Negligible Signal</a></li>
<li class="chapter" data-level="7.11" data-path="neg-Hessian-computation.html"><a href="neg-Hessian-computation.html"><i class="fa fa-check"></i><b>7.11</b> Efficiently Computing the negative Hessian of the regularized log-likelihood</a></li>
<li class="chapter" data-level="7.12" data-path="inv-lambda-ij-k.html"><a href="inv-lambda-ij-k.html"><i class="fa fa-check"></i><b>7.12</b> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></a></li>
<li class="chapter" data-level="7.13" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html"><i class="fa fa-check"></i><b>7.13</b> Training the Hyperparameters in the Likelihood Function of Contact States</a><ul>
<li class="chapter" data-level="7.13.1" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html#dataset-training-hyperparmeters"><i class="fa fa-check"></i><b>7.13.1</b> Dataset Specifications</a></li>
<li class="chapter" data-level="7.13.2" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html#model-specifications-training-hyperparmeters"><i class="fa fa-check"></i><b>7.13.2</b> Model Specifications</a></li>
<li class="chapter" data-level="7.13.3" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html#gradient-muk"><i class="fa fa-check"></i><b>7.13.3</b> The gradient of the log likelihood with respect to <span class="math inline">\(\muk\)</span></a></li>
<li class="chapter" data-level="7.13.4" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html#gradient-lambdak"><i class="fa fa-check"></i><b>7.13.4</b> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></a></li>
<li class="chapter" data-level="7.13.5" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html#the-gradient-of-the-log-likelihood-with-respect-to-gamma_k"><i class="fa fa-check"></i><b>7.13.5</b> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.14" data-path="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html"><a href="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html"><i class="fa fa-check"></i><b>7.14</b> Extending the Bayesian Statistical Model for the Prediction of Protein Residue-Residue Distances</a><ul>
<li class="chapter" data-level="7.14.1" data-path="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html"><a href="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html#the-derivative-of-the-log-likelihood-with-respect-to-rho_k"><i class="fa fa-check"></i><b>7.14.1</b> The derivative of the log likelihood with respect to <span class="math inline">\(\rho_k\)</span></a></li>
<li class="chapter" data-level="7.14.2" data-path="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html"><a href="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html#the-derivative-of-the-log-likelihood-with-respect-to-alpha_k"><i class="fa fa-check"></i><b>7.14.2</b> The derivative of the log likelihood with respect to <span class="math inline">\(\alpha_k\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.15" data-path="seq-features.html"><a href="seq-features.html"><i class="fa fa-check"></i><b>7.15</b> Features used to train Random Forest Model</a><ul>
<li class="chapter" data-level="7.15.1" data-path="seq-features.html"><a href="seq-features.html#seq-features-global"><i class="fa fa-check"></i><b>7.15.1</b> Global Features</a></li>
<li class="chapter" data-level="7.15.2" data-path="seq-features.html"><a href="seq-features.html#seq-features-single"><i class="fa fa-check"></i><b>7.15.2</b> Single Position Features</a></li>
<li class="chapter" data-level="7.15.3" data-path="seq-features.html"><a href="seq-features.html#seq-features-pairwise"><i class="fa fa-check"></i><b>7.15.3</b> Pairwise Features</a></li>
</ul></li>
<li class="chapter" data-level="7.16" data-path="rf-training.html"><a href="rf-training.html"><i class="fa fa-check"></i><b>7.16</b> Training Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="7.16.1" data-path="rf-training.html"><a href="rf-training.html#rf-feature-selection"><i class="fa fa-check"></i><b>7.16.1</b> Feature Selection</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="abbrev.html"><a href="abbrev.html"><i class="fa fa-check"></i><b>A</b> Abbreviations</a></li>
<li class="chapter" data-level="B" data-path="amino-acids.html"><a href="amino-acids.html"><i class="fa fa-check"></i><b>B</b> Amino Acid Alphabet</a></li>
<li class="chapter" data-level="C" data-path="dataset-properties.html"><a href="dataset-properties.html"><i class="fa fa-check"></i><b>C</b> Dataset Properties</a></li>
<li class="chapter" data-level="D" data-path="standard-deviation-of-couplings-for-noncontacts.html"><a href="standard-deviation-of-couplings-for-noncontacts.html"><i class="fa fa-check"></i><b>D</b> Standard Deviation of Couplings for Noncontacts</a></li>
<li class="chapter" data-level="E" data-path="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><a href="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><i class="fa fa-check"></i><b>E</b> Amino Acid Interaction Preferences Reflected in Coupling Matrices</a><ul>
<li class="chapter" data-level="E.1" data-path="pi-cation.html"><a href="pi-cation.html"><i class="fa fa-check"></i><b>E.1</b> Pi-Cation interactions</a></li>
<li class="chapter" data-level="E.2" data-path="disulfide.html"><a href="disulfide.html"><i class="fa fa-check"></i><b>E.2</b> Disulfide Bonds</a></li>
<li class="chapter" data-level="E.3" data-path="aromatic-proline.html"><a href="aromatic-proline.html"><i class="fa fa-check"></i><b>E.3</b> Aromatic-Proline Interactions</a></li>
<li class="chapter" data-level="E.4" data-path="aromatic-network.html"><a href="aromatic-network.html"><i class="fa fa-check"></i><b>E.4</b> Network-like structure of aromatic residues</a></li>
<li class="chapter" data-level="E.5" data-path="aromatic-small-distances.html"><a href="aromatic-small-distances.html"><i class="fa fa-check"></i><b>E.5</b> Aromatic Sidechains at small <span class="math inline">\(Cb\)</span>-<span class="math inline">\(\Cb\)</span> distances</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="optimizing-full-likelihood-with-gradient-descent.html"><a href="optimizing-full-likelihood-with-gradient-descent.html"><i class="fa fa-check"></i><b>F</b> Optimizing Full Likelihood with Gradient Descent</a><ul>
<li class="chapter" data-level="F.1" data-path="visualisation-of-learning-rate-schedules.html"><a href="visualisation-of-learning-rate-schedules.html"><i class="fa fa-check"></i><b>F.1</b> Visualisation of learning rate schedules</a></li>
<li class="chapter" data-level="F.2" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html"><i class="fa fa-check"></i><b>F.2</b> Benchmarking learning rate schedules</a><ul>
<li class="chapter" data-level="F.2.1" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#linear-learning-rate-schedule"><i class="fa fa-check"></i><b>F.2.1</b> Linear learning rate schedule</a></li>
<li class="chapter" data-level="F.2.2" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#sigmoidal-learning-rate-schedule"><i class="fa fa-check"></i><b>F.2.2</b> Sigmoidal learning rate schedule</a></li>
<li class="chapter" data-level="F.2.3" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#square-root-learning-rate-schedule"><i class="fa fa-check"></i><b>F.2.3</b> Square root learning rate schedule</a></li>
<li class="chapter" data-level="F.2.4" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#exponential-learning-rate-schedule"><i class="fa fa-check"></i><b>F.2.4</b> Exponential learning rate schedule</a></li>
</ul></li>
<li class="chapter" data-level="F.3" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html"><i class="fa fa-check"></i><b>F.3</b> Number of iterations until convergence for different learning rate schedules</a><ul>
<li class="chapter" data-level="F.3.1" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#linear-learning-rate-schedule-1"><i class="fa fa-check"></i><b>F.3.1</b> Linear learning rate schedule</a></li>
<li class="chapter" data-level="F.3.2" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#sigmoidal-learning-rate-schedule-1"><i class="fa fa-check"></i><b>F.3.2</b> Sigmoidal learning rate schedule</a></li>
<li class="chapter" data-level="F.3.3" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#square-root-learning-rate-schedule-1"><i class="fa fa-check"></i><b>F.3.3</b> Square root learning rate schedule</a></li>
<li class="chapter" data-level="F.3.4" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#exponential-learning-rate-schedule-1"><i class="fa fa-check"></i><b>F.3.4</b> Exponential learning rate schedule</a></li>
</ul></li>
<li class="chapter" data-level="F.4" data-path="modifying-number-of-iterations-over-which-relative-change-of-coupling-norm-is-evaluated.html"><a href="modifying-number-of-iterations-over-which-relative-change-of-coupling-norm-is-evaluated.html"><i class="fa fa-check"></i><b>F.4</b> Modifying Number of Iterations over which Relative Change of Coupling Norm is Evaluated</a></li>
<li class="chapter" data-level="F.5" data-path="number-of-gibbs-steps-with-respect-to-neff.html"><a href="number-of-gibbs-steps-with-respect-to-neff.html"><i class="fa fa-check"></i><b>F.5</b> Number of Gibbs steps with respect to Neff</a></li>
<li class="chapter" data-level="F.6" data-path="fix-single-potentials-at-maximum-likelihood-estimate-v.html"><a href="fix-single-potentials-at-maximum-likelihood-estimate-v.html"><i class="fa fa-check"></i><b>F.6</b> Fix single potentials at maximum-likelihood estimate v*</a></li>
<li class="chapter" data-level="F.7" data-path="monitoring-optimization-for-different-sample-sizes.html"><a href="monitoring-optimization-for-different-sample-sizes.html"><i class="fa fa-check"></i><b>F.7</b> Monitoring Optimization for different Sample Sizes</a></li>
<li class="chapter" data-level="F.8" data-path="statistics-for-comparing-couplings-computed-with-pseudo-likelihood-and-contrastive-divergence.html"><a href="statistics-for-comparing-couplings-computed-with-pseudo-likelihood-and-contrastive-divergence.html"><i class="fa fa-check"></i><b>F.8</b> Statistics for Comparing Couplings computed with Pseudo-likelihood and Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="training-of-the-random-forest-contact-prior.html"><a href="training-of-the-random-forest-contact-prior.html"><i class="fa fa-check"></i><b>G</b> Training of the Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="G.1" data-path="training-random-forest-model-with-pseudo-likelihood-feature.html"><a href="training-random-forest-model-with-pseudo-likelihood-feature.html"><i class="fa fa-check"></i><b>G.1</b> Training Random Forest Model with pseudo-likelihood Feature</a></li>
<li class="chapter" data-level="G.2" data-path="rf-window-size.html"><a href="rf-window-size.html"><i class="fa fa-check"></i><b>G.2</b> Evaluating window size with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="G.3" data-path="rf-noncontact-threshold.html"><a href="rf-noncontact-threshold.html"><i class="fa fa-check"></i><b>G.3</b> Evaluating non-contact threshold with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="G.4" data-path="rf-ratio-noncontacts.html"><a href="rf-ratio-noncontacts.html"><i class="fa fa-check"></i><b>G.4</b> Evaluating ratio of non-contacts and contacts in the training set with 5-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PhD thesis: residue-residue contact prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="cd-sampling-optimization" class="section level2">
<h2><span class="header-section-number">4.4</span> Tuning the Gibbs Sampling Scheme for Contrastive Divergence</h2>
<p>The original <a href="abbrev.html#abbrev">CD</a>-k algorithm described by Hinton in 2002 evolves the Markov chains by k=1 Gibbs steps <span class="citation">[<a href="#ref-Hinton2002">188</a>]</span>. As described earlier, <a href="abbrev.html#abbrev">CD</a>-1 provides a biased estimate of the true gradient because the Markov chains have not reached the stationary distribution <span class="citation">[<a href="#ref-Fischer2012">190</a>]</span>. Bengio and Delalleau show that the bias for <a href="abbrev.html#abbrev">CD</a>-k kan be understood as a residual term when expressing the log likelihood gradient as an expansion that involves the k-th sample of the Gibbs chain <span class="citation">[<a href="#ref-Bengio2009">191</a>]</span>. As k goes to infinity the residual term and hence the bias converges to zero and the <a href="abbrev.html#abbrev">CD</a> gradient estimate converges to a stochastic estimation of the true likelihood gradient. Indeed, eventhough surprising results have been obtained by evolving the Markov chains for only one Gibbs step, typically <a href="abbrev.html#abbrev">CD</a>-k for k&gt;&gt;1 gives more precise results <span class="citation">[<a href="#ref-Bengio2009">191</a>]</span>. Furthermore it has been shown, that bias also depends on the mixing rate or the rate of convergence of the chains whereby the mixing rate decreases when model parameters increase <span class="citation">[<a href="#ref-Tieleman2008">200</a>]</span>. This can lead to divergence of the <a href="abbrev.html#abbrev">CD</a>-k solution from the maximum-likelihood solution in a sense the model systematically gets worse as optimization progresses <span class="citation">[<a href="#ref-Fischer2010">201</a>]</span>. Regularization of the parameters offers a solution to this problem, constraining the magnitude of the parameters. A different solution suggested by Bengio and Delalleau is to dynamically increase k when the model parameters increase <span class="citation">[<a href="#ref-Bengio2009">191</a>]</span>. These studies analysing the convergence properties and the expected approximation error for <a href="abbrev.html#abbrev">CD</a>-k have mainly been conducted for Restricted Boltzmann Machines. It is therefore not clear, whether and to what extent these findings apply to the <em>Potts</em> model.</p>
<p>Several connections of <a href="abbrev.html#abbrev">CD</a> to other well known approximation algorithms have been drawn. For example, it can be shown that <a href="abbrev.html#abbrev">CD</a> by sampling only one random variable according to the conditional probability is exactly equivalent to optimising the pseudo-likelihood <span class="citation">[<a href="#ref-Hyvarinen2006">202</a>,<a href="#ref-Hyvarinen2007">203</a>]</span>. Asuncion and colleagues showed further that an arbitrary good approximation to the full likelihood can be reached by applying blocked-Gibbs sampling <span class="citation">[<a href="#ref-Asuncion2010">204</a>]</span>. Thereby, <a href="abbrev.html#abbrev">CD</a> by varying the number variables that is randomly samples, has an equivalent composite likelihoods, which is a higher-order generalization of the pseudo-likelihood.</p>
<p><a href="abbrev.html#abbrev">PCD</a> is a variation of <a href="abbrev.html#abbrev">CD</a> such that the Markov chain is not reinitialized at a data sample every time a new gradient is computed <span class="citation">[<a href="#ref-Tieleman2008">200</a>]</span>. Instead, the Markov chains are kept <em>persistent</em> that is, they are evolved between successive gradient computations. The fundamental idea behind <a href="abbrev.html#abbrev">PCD</a> is that the model changes only slowly between parameter updates given a sufficiently small learning rate and the Markov chains will not be pushed too far from equilibrium after each update but rather stay close to the stationary distribution <span class="citation">[<a href="#ref-Murphy2012">94</a>,<a href="#ref-Fischer2012">190</a>,<a href="#ref-Tieleman2008">200</a>]</span>. Tieleman and others observed that <a href="abbrev.html#abbrev">PCD</a> often works better than <a href="abbrev.html#abbrev">CD</a>, eventhough <a href="abbrev.html#abbrev">CD</a> can be faster in the early stages of learning and thus should be preferred when runtime is the limiting factor <span class="citation">[<a href="#ref-Murphy2012">94</a>,<a href="#ref-Tieleman2008">200</a>]</span>.</p>
<p>persistent CD performs better than CD in all practical cases testet and it performs similarly to CD10, a version of contrastive divergence using 10 Gibbs sampling steps instead of just one [johannes]</p>
<p>The next sections discuss various modifications of the <a href="abbrev.html#abbrev">CD</a> algorithm, such as varying the regularizatio strength <span class="math inline">\(\lambda_w\)</span> for constraining the coupling parameters <span class="math inline">\(\w\)</span>, increasing the number of Gibbs sampling steps and varying the number of Markov chains used for sampling. Persistent contrastive divergence is analysed for various combinations of the above mentioned settings and eventully combined with <a href="abbrev.html#abbrev">CD</a>-k. Unless noted otherwise, all optimizations will be performed using stochastic gradient descent with the tuned hyperparameters described in the last sections.</p>
<div id="cd-sampling-size" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Varying the Sample Size</h3>
<p>The default Gibbs sampling scheme explained in method section <a href="methods-cd-sampling.html#methods-cd-sampling">7.9</a> involves the random selection of <span class="math inline">\(10L\)</span> sequences from the input alignment, with <span class="math inline">\(L\)</span> being protein length, at every iteration of the optimization procedure. These sequences are used to initialize the Markov chains for Gibbs sampling new sequences to estimate the gradient with <a href="abbrev.html#abbrev">CD</a>. The particular choice of <span class="math inline">\(10L\)</span> sequences was motivated by the fact that there is a relationship between the precision of contacts predicted from pseudo-likelihood and protein length as long as the alignment has less than <span class="math inline">\(10^3\)</span> diverse sequences <span class="citation">[<a href="#ref-Anishchenko2017">174</a>]</span>. It has been argued that roughly <span class="math inline">\(5L\)</span> nonredundant sequences are required to obtain confident predictions that can bet used for protein structure prediction <span class="citation">[<a href="#ref-Kamisetty2013">102</a>]</span>.</p>
<p>I analysed whether varying the number of sequences used for the approximation of the gradient via Gibbs sampling affects performance. Randomly selecting only a subset of sequences <span class="math inline">\(S\)</span> from the <span class="math inline">\(N\)</span> sequences of the input alignment corresponds to the stochastic gradient descent idea of a minibatch and introduces additional stochasticity over the <a href="abbrev.html#abbrev">CD</a> Gibbs sampling process. Using <span class="math inline">\(S &lt; N\)</span> sequences for Gibbs sampling has the further advantage of decreasing the runtime at each iteration. I evaluated different schemes for randomly selecting sequences for Gibbs sampling at every iteration of the optimization:</p>
<ul>
<li>sampling <span class="math inline">\(x \cdot L\)</span> sequences with <span class="math inline">\(x \in \{ 1, 5, 10, 50 \}\)</span> without replacement enforcing <span class="math inline">\(S \eq \min(N, xL)\)</span></li>
<li>sampling <span class="math inline">\(x \cdot N_{\textrm{eff}}\)</span> sequences with <span class="math inline">\(x \in \{ 0.2, 0.3, 0.4 \}\)</span> without replacement</li>
</ul>
<p>As can be seen in Figure <a href="cd-sampling-optimization.html#fig:cd-performance-samplesize">4.12</a>, randomly selecting <span class="math inline">\(L\)</span> sequences for sampling, results in a visible drop in performance. Using <span class="math inline">\(5L\)</span> sequences for sampling results in slighlty decreased performance over using <span class="math inline">\(10L\)</span> or <span class="math inline">\(50L\)</span> sequences. There is no benefit in using more than <span class="math inline">\(10L\)</span> sequences, especially as sampling more sequences increases runtime per iteration. Specifying the number of sequences for sampling as fractions of <a href="abbrev.html#abbrev">Neff</a> generally improves precision slightly over selecting <span class="math inline">\(10L\)</span> or <span class="math inline">\(50L\)</span> sequences for sampling. And by sampling <span class="math inline">\(0.3N_{\textrm{eff}}\)</span> and <span class="math inline">\(0.4N_{\textrm{eff}}\)</span> sequences, <a href="abbrev.html#abbrev">CD</a> does even slighty improve over pseudo-likelihood.</p>

<div class="figure" style="text-align: center"><span id="fig:cd-performance-samplesize"></span>
<iframe src="img/full_likelihood/gibbs_sampling/precision_vs_rank_samplesize.html" width="100%" height="500px">
</iframe>
<p class="caption">
Figure 4.12: Mean precision for top ranked contact predictions over 288 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. <strong>pseudo-likelihood</strong>: couplings computed with pseudo-likelihood. <strong>CD sample size = X </strong>: contact scores computed from <a href="abbrev.html#abbrev">CD</a> with <a href="abbrev.html#abbrev">SGD</a> and varying number of sample size as specified in the legend. Sample size refers to the number of randomly selected sequences for Gibbs sampling. It is defined either as multiples of protein length <span class="math inline">\(L\)</span> or as fraction of the effective number of sequences <a href="abbrev.html#abbrev">Neff</a>.
</p>
</div>
<p>When evaluating performance with respect to the number of effective sequences <a href="abbrev.html#abbrev">Neff</a>, it can clearly be noted that the optimal samplings size must depend on <a href="abbrev.html#abbrev">Neff</a>. Selecting too many sequences, e.g. <span class="math inline">\(50L\)</span> for small alignments (upper left plot in Figure <a href="cd-sampling-optimization.html#fig:cd-precision-sampling-size-neff">4.13</a>), or selecting too few sequences, e.g <span class="math inline">\(1L\)</span> for big alignments (lower right plot in Figure <a href="cd-sampling-optimization.html#fig:cd-precision-sampling-size-neff">4.13</a>), results in a decrease in precision compared to defining sampling size as fractions of <a href="abbrev.html#abbrev">Neff</a>. Especially small alignments benefit from sample sizes defined as a fraction of <a href="abbrev.html#abbrev">Neff</a> with improvements of about three percentage points in precision over pseudo-likelihood.</p>

<div class="figure" style="text-align: center"><span id="fig:cd-precision-sampling-size-neff"></span>
<iframe src="img/full_likelihood/gibbs_sampling/precision_vs_rank_facetted_by_neff_samplesize.html" width="100%" height="600px">
</iframe>
<p class="caption">
Figure 4.13: Mean precision for top ranked contact predictions over 300 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. <strong>pseudo-likelihood</strong>: contact scores computed from pseudo-likelihood. <strong>CD sample size = X</strong>: contact scores computed from <a href="abbrev.html#abbrev">CD</a> optimized with <a href="abbrev.html#abbrev">SGD</a> and varying number of sample size as specified in the legend. Sample size refers to the number of randomly selected sequences for Gibbs sampling. It is defined either as multiples of protein length <span class="math inline">\(L\)</span> or as fraction of the effective number of sequences <a href="abbrev.html#abbrev">Neff</a>.
</p>
</div>
<p>To understand the effect of different choices of sample size it is necessary to look at single proteins. The left plot in Figure <a href="cd-sampling-optimization.html#fig:cd-samplesize-protein1c75a00">4.14</a> shows the development of the L2 norm of the gradient for couplings <span class="math inline">\(||\nabla_{\w}||_2\)</span> for protein chain 1c75_A_00 that is of length 71 and has <a href="abbrev.html#abbrev">Neff</a> = 16808. The norm of the gradient decreases during optimization and saturates at decreasing levels for increasing choices of sample size. Increasing the sample size by a factor 100 (from <span class="math inline">\(L\)</span> to <span class="math inline">\(100L\)</span>) leads to an approximately 10-fold reduction of the norm of gradient (<span class="math inline">\(\mathrm{1.4e}{+5}\)</span> compared to <span class="math inline">\(\mathrm{1.45e}{+4}\)</span>) at convergence, which corresponds to a typical reduction of statistical noise as the square root of the number of samples. It is not feasible to sample the number of sequences at each iteration that would be necessary to reduce the norm of the gradient to near zero. <!--
Of course, the magnitude of the norm of the gradient for couplings depends on the size of the protein, because the number of coupling parameters $\wijab$ scales with protein length $L$ as $\frac{L(L-1)}{2} \cdot 400$.
--></p>
<p>In any case, precision of the top ranked contacts does not improve to the same amount as the norm of the gradient decreases when using larger sample sizes as could be seen in the previous benchmark. Probably, the improved gradient when using a larger sample size helps to finetune the parameters, which only has a negligible effect on the contact score computed as <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. This hypothesis is confirmed when assessing the development of the L2 norm of coupling parameters <span class="math inline">\(||\w||_2\)</span> over optimization shown in right plot in Figure <a href="cd-sampling-optimization.html#fig:cd-samplesize-protein1c75a00">4.14</a>. The norm of the coupling parameters is almost indistinguishable after optimization when using a sample size of <span class="math inline">\(50L\)</span>, <span class="math inline">\(100L\)</span> or <span class="math inline">\(0.2 - 0.4\)</span>Neff.</p>
<p>It is not clear, why an improved gradient estimate due to sampling more sequences results in weaker performance for proteins with small alignments as could be seen in the previous benchmark in Figure <a href="cd-sampling-optimization.html#fig:cd-precision-sampling-size-neff">4.13</a>. For protein chain 1aho_A_00 of length 64 and with 378 sequences (<a href="abbrev.html#abbrev">Neff</a>=229), setting <span class="math inline">\(S=10L\)</span> or <span class="math inline">\(S=50L\)</span> which corresponds to using all <span class="math inline">\(N \eq 378\)</span> sequences for the sampling procedure to approximate the gradient, results in a mean precision over the top <span class="math inline">\(0.1L\)</span> - <span class="math inline">\(L\)</span> contacts of 0.44, whereas using only <span class="math inline">\(0.3N_{\textrm{eff}} \eq 69\)</span> sequences gives a mean precision of 0.62. The left plot in Appendix Figure <a href="monitoring-optimization-for-different-sample-sizes.html#fig:cd-samplesize-protein1ahoa00">F.13</a> shows the development of the norm of the gradient and the norm of coupling parameters for this protein. As before, the gradient estimate improves and the norm of the gradient saturates at smaller values when more sequences are used in the Gibbs sampling process and therefore should lead to a better approximation of the likelihood. One explanation could be that this is some effect of overfitting, eventhough a regularizer is used and the norm of coupling parameters actually is smaller when using higher sampling sizes (see the right plot in Appendix Figure <a href="monitoring-optimization-for-different-sample-sizes.html#fig:cd-samplesize-protein1ahoa00">F.13</a>).</p>

<div class="figure" style="text-align: center"><span id="fig:cd-samplesize-protein1c75a00"></span>
<img src="img/full_likelihood/gibbs_sampling/1c75A00_gradient_norm_for_samplesizes.png" alt="Monitoring parameter norm and gradient norm for protein 1c75_A_00 during SGD using different sample sizes. Protein 1c75_A_00 has length L=71 and 28078 sequences in the alignment (Neff=16808) Left L2-norm of the gradients for coupling parameters \(||\w||_2\) (without contrbution of regularizer). The number of sequences, that is used for Gibbs sampling to approximate the gradient, is given in the legend. Right L2-norm of the coupling parameters \(||\w||_2\). The number of sequences, that is used for Gibbs sampling to approximate the gradient, is given in the legend." width="48%" /><img src="img/full_likelihood/gibbs_sampling/1c75A00_parameter_norm_for_samplesizes.png" alt="Monitoring parameter norm and gradient norm for protein 1c75_A_00 during SGD using different sample sizes. Protein 1c75_A_00 has length L=71 and 28078 sequences in the alignment (Neff=16808) Left L2-norm of the gradients for coupling parameters \(||\w||_2\) (without contrbution of regularizer). The number of sequences, that is used for Gibbs sampling to approximate the gradient, is given in the legend. Right L2-norm of the coupling parameters \(||\w||_2\). The number of sequences, that is used for Gibbs sampling to approximate the gradient, is given in the legend." width="48%" />
<p class="caption">
Figure 4.14: Monitoring parameter norm and gradient norm for protein 1c75_A_00 during <a href="abbrev.html#abbrev">SGD</a> using different sample sizes. Protein 1c75_A_00 has length L=71 and 28078 sequences in the alignment (<a href="abbrev.html#abbrev">Neff</a>=16808) <strong>Left</strong> L2-norm of the gradients for coupling parameters <span class="math inline">\(||\w||_2\)</span> (without contrbution of regularizer). The number of sequences, that is used for Gibbs sampling to approximate the gradient, is given in the legend. <strong>Right</strong> L2-norm of the coupling parameters <span class="math inline">\(||\w||_2\)</span>. The number of sequences, that is used for Gibbs sampling to approximate the gradient, is given in the legend.
</p>
</div>
</div>
<div id="cd-gibbs-steps" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Varying the number of Gibbs Steps</h3>
<p>The default <a href="abbrev.html#abbrev">CD</a>-k algorithm as described by Hinton in 2002 evolves the gibbs chain by only <span class="math inline">\(k \eq 1\)</span> full step <span class="citation">[<a href="#ref-Hinton2002">188</a>]</span>. As it has been pointed out in the literature, the CD-1 sampler represents a biased estimator because samples are not obtained from the stationary distribution of the Markov chain. Furthermore, it was found that sampling <span class="math inline">\(k&gt;1\)</span> steps gives more precise results at the cost of longer runtimes per gradient evaluation <span class="citation">[<a href="#ref-Bengio2009">191</a>,<a href="#ref-Tieleman2008">200</a>]</span>. I analysed the impact on performance when the number of Gibbs steps is increased to 5 and 10.</p>
<p>As can be seen in Figure <a href="cd-sampling-optimization.html#fig:precision-cd-gibbs-steps">4.15</a>, increasing the number of Gibbs steps does result in a slight drop of performance. When evaluating precision with respect to <a href="abbrev.html#abbrev">Neff</a> it can be found that using more Gibbs sampling steps is especially disadvantageous for large alignments (see Appendix Figure <a href="number-of-gibbs-steps-with-respect-to-neff.html#fig:cd-precision-gibbssteps-neff">F.11</a>). By checking single proteins it becomes obvious</p>

<div class="figure" style="text-align: center"><span id="fig:precision-cd-gibbs-steps"></span>
<iframe src="img/full_likelihood/gibbs_sampling/precision_vs_rank_gibbssteps.html" width="90%" height="500px">
</iframe>
<p class="caption">
Figure 4.15: Mean precision for top ranked contact predictions over 300 proteins. Contact scores are computed as the <a href="abbrev.html#abbrev">APC</a> corrected Frobenius norm of the couplings <span class="math inline">\(\wij\)</span>. <strong>pseudo-likelihood</strong>: contact scores computed from pseudo-likelihood. <strong>CD #Gibbs steps = X</strong>: contact scores computed from <a href="abbrev.html#abbrev">CD</a> optimized with <a href="abbrev.html#abbrev">SGD</a> and evolving each Markov chain using the number of Gibbs steps specified in the legend.
</p>
</div>
</div>
<div id="cd-gibbs-steps" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Persistent Contrastive Divergence</h3>
<p>Finally I analysed <a href="abbrev.html#abbrev">PCD</a> since it has often been found to be a better estimator of the likelihood gradient than <a href="abbrev.html#abbrev">CD</a>. It has been suggested to use <a href="abbrev.html#abbrev">PCD</a> with small learning rates and larger mini-batches because the fundamental assumption is that the model does not change to quickly over iterations and therefore the Markov chains will stay close to the equilibrium distribition. However, using smaller learning rates and larger minibatches not only increases runtime but also requires tuning of the learning rate and learning rate schedule. Since it has been found, that CD is faster in learning at the beginning of the optimization, I tested a compromise, that uses CD-1 at the beginning of the optimization and when learning slows down, measured as the relative change of the norm of coupling parameters, I start PCD. This also has the advantage that the model has already approached the optimum and the parameters <span class="math inline">\(\w\)</span> will almost be constant over many updates.</p>
<p>As PCD might require samller update steps and larger minibatches, I analysed the performance of PCD for the default settings of CD and additionally for smaller learning and decay rates and larger minibatches. Note that one Markov chain is kept for every sequence of the input alignment. At each iteration a subset <span class="math inline">\(N^{\prime} &lt; N\)</span> of the Markov chains is randomly selected (without replacement) and used to for another round of Gibss sampling at the current iteration.</p>
<p>PLOT PCD for different LEARNIGN RATEWS and SAMPLE SIZES</p>
<p>Discussion: - as could be seen: improved gradients and different solutions do not translate into improved precision of top ranked contacts - APC corrected l2norm might not be an appropriate measure for CD couplings: look at correlation plot of couplings (pll vs cd) and l2norm (pll vs cd) and apc l2norm (pll vs cd) -&gt; differences vanish - ranking of residues might not be influenced by subtle changes in parameters when crude l2norm is computed -&gt; rank plot: merged list of top ranked contacts from both methods what can we see: - generally pll has stronger scores (see also boxplot over all proteins? statistic?) - ranking is very similar, especially for top ranked contacts (thats why benchmark plots so similar)</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Hinton2002">
<p>188. Hinton, G.E. (2002). Training Products of Experts by Minimizing Contrastive Divergence. Neural Comput. <em>14</em>, 1771–1800. Available at: <a href="http://www.gatsby.ucl.ac.uk/publications/tr/tr00-004.pdf" class="uri">http://www.gatsby.ucl.ac.uk/publications/tr/tr00-004.pdf</a>.</p>
</div>
<div id="ref-Fischer2012">
<p>190. Fischer, A., and Igel, C. (2012). An Introduction to Restricted Boltzmann Machines. Lect. Notes Comput. Sci. Prog. Pattern Recognition, Image Anal. Comput. Vision, Appl. <em>7441</em>, 14–36. Available at: <a href="http://link.springer.com/chapter/10.1007/978-3-642-33275-3{\_}2" class="uri">http://link.springer.com/chapter/10.1007/978-3-642-33275-3{\_}2</a>.</p>
</div>
<div id="ref-Bengio2009">
<p>191. Bengio, Y., and Delalleau, O. (2009). Justifying and Generalizing Contrastive Divergence. Neural Comput. <em>21</em>, 1601–21. Available at: <a href="http://www.iro.umontreal.ca/{~}lisa/publications2/index.php/attachments/single/105" class="uri">http://www.iro.umontreal.ca/{~}lisa/publications2/index.php/attachments/single/105</a>.</p>
</div>
<div id="ref-Tieleman2008">
<p>200. Tieleman, T. (2008). Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient. Proc. 25th Int. Conf. Mach. Learn. <em>307</em>, 7.</p>
</div>
<div id="ref-Fischer2010">
<p>201. Fischer, A., and Igel, C. (2010). Empirical Analysis of the Divergence of Gibbs Sampling Based Learning Algorithms for Restricted Boltzmann Machines. In Artif. neural networks – icann 2010 (Springer, Berlin, Heidelberg), pp. 208–217. Available at: <a href="http://link.springer.com/10.1007/978-3-642-15825-4{\_}26" class="uri">http://link.springer.com/10.1007/978-3-642-15825-4{\_}26</a>.</p>
</div>
<div id="ref-Hyvarinen2006">
<p>202. Hyvärinen, A. (2006). Consistency of pseudolikelihood estimation of fully visible Boltzmann machines. Available at: <a href="https://www.cs.helsinki.fi/u/ahyvarin/papers/NC06.pdf" class="uri">https://www.cs.helsinki.fi/u/ahyvarin/papers/NC06.pdf</a>.</p>
</div>
<div id="ref-Hyvarinen2007">
<p>203. Hyvarinen, A. (2007). Connections Between Score Matching, Contrastive Divergence, and Pseudolikelihood for Continuous-Valued Variables. IEEE Trans. Neural Networks <em>18</em>, 1529–1531. Available at: <a href="http://ieeexplore.ieee.org/document/4298117/" class="uri">http://ieeexplore.ieee.org/document/4298117/</a>.</p>
</div>
<div id="ref-Asuncion2010">
<p>204. Asuncion, A.U., Liu, Q., Ihler, A.T., and Smyth, P. (2010). Learning with Blocks: Composite Likelihood and Contrastive Divergence. Proc. Mach. Learn. Res. <em>9</em>, 33–40. Available at: <a href="http://www.ics.uci.edu/{~}asuncion/pubs/AISTATS{\_}10.pdf" class="uri">http://www.ics.uci.edu/{~}asuncion/pubs/AISTATS{\_}10.pdf</a>.</p>
</div>
<div id="ref-Murphy2012">
<p>94. Murphy, K.P. (2012). Machine Learning: A Probabilistic Perspective (MIT Press).</p>
</div>
<div id="ref-Anishchenko2017">
<p>174. Anishchenko, I., Ovchinnikov, S., Kamisetty, H., and Baker, D. (2017). Origins of coevolution between residues distant in protein 3D structures. Proc. Natl. Acad. Sci., 201702664. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/28784799 http://www.pnas.org/lookup/doi/10.1073/pnas.1702664114" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/28784799 http://www.pnas.org/lookup/doi/10.1073/pnas.1702664114</a>.</p>
</div>
<div id="ref-Kamisetty2013">
<p>102. Kamisetty, H., Ovchinnikov, S., and Baker, D. (2013). Assessing the utility of coevolution-based residue-residue contact predictions in a sequence- and structure-rich era. Proc. Natl. Acad. Sci. U. S. A. <em>110</em>, 15674–9. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3785744{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract" class="uri">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3785744{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract</a>.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://contactpredictionthesis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="regularization-for-cd-with-sgd.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="adam-results.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/susannvorberg/phd_thesis/edit/master/06-optimizing_full_likelihood.Rmd",
"text": "Edit"
},
"download": ["PhD_thesis_Susann_Vorberg.pdf"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
