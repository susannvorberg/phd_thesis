<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>PhD thesis: residue-residue contact prediction</title>
  <meta name="description" content="This is my PhD thesis on residue-residue contact prediction.">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="PhD thesis: residue-residue contact prediction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my PhD thesis on residue-residue contact prediction." />
  <meta name="github-repo" content="susannvorberg/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PhD thesis: residue-residue contact prediction" />
  
  <meta name="twitter:description" content="This is my PhD thesis on residue-residue contact prediction." />
  

<meta name="author" content="Susann Vorberg">


<meta name="date" content="2017-10-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="meta-predictors.html">
<link rel="next" href="application-contact-prediction.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
  tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
  TeX: { 
    extensions: ["mediawiki-texvc.js", "sinuitx.js"],
    Macros: {
      Cb: "C_\\beta",
      eq: "\\!=\\!",
      Gauss: "\\mathcal{N}",
      H: "\\mathbf{H}",
      Hij : "\\H_{ij}",
      I: "\\mathbf{I}",
      Lijk: "\\mathbf{\\Lambda}_{ij,k}",
      Lk: "\\mathbf{\\Lambda}_k",
      LL: "L\\!L(\\mathbf{v}, \\mathbf{w})",
      LLreg: "L\\!L_\\mathrm{reg}",
      muijk: "\\mathbf{\\mu}_{ij,k}",
      muk: "\\mathbf{\\mu}_k",
      neff: "N_\\mathrm{eff}",
      r: "\\mathbf{r}",
      rij: "r_{ij}",
      c: "\\mathbf{c}",
      cij: "c_{ij}",
      seq: "\\mathbf{x}",
      Qij: "\\mathbf{Q}_{ij}",
      q: "\\mathbf{q}",
      qij: "\\mathbf{q'}_{ij}",
      Sn: "\\mathcal{S}_n",
      v: "\\mathbf{v}",
      vi: "\\mathcal{v}_{i}",
      vj: "\\mathcal{v}_{j}",
      via: "\\mathcal{v}_{ia}",
      vja: "\\mathcal{v}_{ja}",
      w: "\\mathbf{w}",
      wij: "\\mathbf{w}_{ij}",
      wijab: "\\mathcal{w}_{ijab}",
      wijcd: "\\mathcal{w}_{ijcd}",
      wklcd: "\\mathcal{w}_{klcd}",
      X: "\\mathbf{X}",
      angstrom: "\\AA \\; \\;"
      }
  }
});
</script>


 
<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js", "[siunitx]/siunitx.js"],
  TeX: { TagSide: "left" }
});
MathJax.Ajax.config.path['siunitx']  = '../latex/MathJax-siunitx-master/';
</script>
//-->



<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD Thesis Susann Vorberg</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="general-intro.html"><a href="general-intro.html"><i class="fa fa-check"></i><b>1</b> Biological Background</a></li>
<li class="chapter" data-level="2" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html"><i class="fa fa-check"></i><b>2</b> Introduction to Contact Prediction</a><ul>
<li class="chapter" data-level="2.1" data-path="local-methods.html"><a href="local-methods.html"><i class="fa fa-check"></i><b>2.1</b> Local Statistical Models</a></li>
<li class="chapter" data-level="2.2" data-path="global-methods.html"><a href="global-methods.html"><i class="fa fa-check"></i><b>2.2</b> Global Statistical Models</a></li>
<li class="chapter" data-level="2.3" data-path="meta-predictors.html"><a href="meta-predictors.html"><i class="fa fa-check"></i><b>2.3</b> Machine Learning Methods and Meta-Predictors</a></li>
<li class="chapter" data-level="2.4" data-path="maxent.html"><a href="maxent.html"><i class="fa fa-check"></i><b>2.4</b> Modelling Protein Families with Potts Model</a><ul>
<li class="chapter" data-level="2.4.1" data-path="maxent.html"><a href="maxent.html#potts-model-properties"><i class="fa fa-check"></i><b>2.4.1</b> Model Properties</a></li>
<li class="chapter" data-level="2.4.2" data-path="maxent.html"><a href="maxent.html#potts-mle"><i class="fa fa-check"></i><b>2.4.2</b> Inferring Parameters for the Potts Model</a></li>
<li class="chapter" data-level="2.4.3" data-path="maxent.html"><a href="maxent.html#potts-model-solutions"><i class="fa fa-check"></i><b>2.4.3</b> Solving the Inverse Potts Problem</a></li>
<li class="chapter" data-level="2.4.4" data-path="maxent.html"><a href="maxent.html#post-processing-heuristics"><i class="fa fa-check"></i><b>2.4.4</b> Computing Contact Maps</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="application-contact-prediction.html"><a href="application-contact-prediction.html"><i class="fa fa-check"></i><b>2.5</b> Applications</a></li>
<li class="chapter" data-level="2.6" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html"><i class="fa fa-check"></i><b>2.6</b> Evaluating Contact Prediction Methods</a><ul>
<li class="chapter" data-level="2.6.1" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#seq-sep"><i class="fa fa-check"></i><b>2.6.1</b> Sequence Separation</a></li>
<li class="chapter" data-level="2.6.2" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#interpretation-of-evaluation-results"><i class="fa fa-check"></i><b>2.6.2</b> Interpretation of Evaluation Results</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>2.7</b> Challenges for Coevolutionary Inference</a><ul>
<li class="chapter" data-level="2.7.1" data-path="challenges.html"><a href="challenges.html#phylogenetic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>2.7.1</b> Phylogenetic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="2.7.2" data-path="challenges.html"><a href="challenges.html#entropic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>2.7.2</b> Entropic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="2.7.3" data-path="challenges.html"><a href="challenges.html#finite-sampling-effects"><i class="fa fa-check"></i><b>2.7.3</b> Finite Sampling Effects</a></li>
<li class="chapter" data-level="2.7.4" data-path="challenges.html"><a href="challenges.html#multiple-sequence-alignments"><i class="fa fa-check"></i><b>2.7.4</b> Multiple Sequence Alignments</a></li>
<li class="chapter" data-level="2.7.5" data-path="challenges.html"><a href="challenges.html#alternative-sources-of-coevolution"><i class="fa fa-check"></i><b>2.7.5</b> Alternative Sources of Coevolution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpreting-coupling-matrices.html"><a href="interpreting-coupling-matrices.html"><i class="fa fa-check"></i><b>3</b> Interpretation of Coupling Matrices</a><ul>
<li class="chapter" data-level="3.1" data-path="correlation-between-couplings-and-class.html"><a href="correlation-between-couplings-and-class.html"><i class="fa fa-check"></i><b>3.1</b> Single Coupling Values Carry Evidence of Contacts</a></li>
<li class="chapter" data-level="3.2" data-path="physico-chemical-fingerprints-in-coupling-matrices.html"><a href="physico-chemical-fingerprints-in-coupling-matrices.html"><i class="fa fa-check"></i><b>3.2</b> Physico-Chemical Fingerprints in Coupling Matrices</a></li>
<li class="chapter" data-level="3.3" data-path="coupling-profiles.html"><a href="coupling-profiles.html"><i class="fa fa-check"></i><b>3.3</b> Coupling Profiles Vary with Distance</a></li>
<li class="chapter" data-level="3.4" data-path="higher-order-coupling-profiles.html"><a href="higher-order-coupling-profiles.html"><i class="fa fa-check"></i><b>3.4</b> Higher Order Dependencies Between Couplings</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="optimizing-full-likelihood.html"><a href="optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>4</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="4.1" data-path="full-likelihood-gradient.html"><a href="full-likelihood-gradient.html"><i class="fa fa-check"></i><b>4.1</b> Approximating the Gradient of the Full Likelihood with Contrastive Divergence</a></li>
<li class="chapter" data-level="4.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html"><i class="fa fa-check"></i><b>4.2</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="4.2.1" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#convergence-criteria-sgd"><i class="fa fa-check"></i><b>4.2.1</b> Convergence Criterion for Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="4.2.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#sgd-hyperparameter-tuning"><i class="fa fa-check"></i><b>4.2.2</b> Tuning Hyperparameters of Stochastic Gradient Descent Optimizer</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="regularization-for-cd-with-sgd.html"><a href="regularization-for-cd-with-sgd.html"><i class="fa fa-check"></i><b>4.3</b> Tuning Regularization Coefficients for Contrastive Divergence</a></li>
<li class="chapter" data-level="4.4" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html"><i class="fa fa-check"></i><b>4.4</b> Tuning the Gibbs Sampling Scheme for Contrastive Divergence</a><ul>
<li class="chapter" data-level="4.4.1" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-sampling-size"><i class="fa fa-check"></i><b>4.4.1</b> Varying the Sample Size</a></li>
<li class="chapter" data-level="4.4.2" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>4.4.2</b> Varying the number of Gibbs Steps</a></li>
<li class="chapter" data-level="4.4.3" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>4.4.3</b> Persistent Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="adam-results.html"><a href="adam-results.html"><i class="fa fa-check"></i><b>4.5</b> Using ADAM to optimize Contrastive Divergence</a></li>
<li class="chapter" data-level="4.6" data-path="comparing-pll-cd.html"><a href="comparing-pll-cd.html"><i class="fa fa-check"></i><b>4.6</b> Comparing CD couplings to pLL couplings</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="contact-prior.html"><a href="contact-prior.html"><i class="fa fa-check"></i><b>5</b> Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="5.1" data-path="random-forest-classifiers.html"><a href="random-forest-classifiers.html"><i class="fa fa-check"></i><b>5.1</b> Random Forest Classifiers</a></li>
<li class="chapter" data-level="5.2" data-path="hyperparameter-optimization-for-random-forest.html"><a href="hyperparameter-optimization-for-random-forest.html"><i class="fa fa-check"></i><b>5.2</b> Hyperparameter Optimization for Random Forest</a></li>
<li class="chapter" data-level="5.3" data-path="evaluating-random-forest-model-as-contact-predictor.html"><a href="evaluating-random-forest-model-as-contact-predictor.html"><i class="fa fa-check"></i><b>5.3</b> Evaluating Random Forest Model as Contact Predictor</a></li>
<li class="chapter" data-level="5.4" data-path="using-contact-scores-as-additional-features.html"><a href="using-contact-scores-as-additional-features.html"><i class="fa fa-check"></i><b>5.4</b> Using Contact Scores as Additional Features</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><a href="a-bayesian-statistical-model-for-residue-residue-contact-prediction.html"><i class="fa fa-check"></i><b>6</b> A Bayesian Statistical Model for Residue-Residue Contact Prediction</a><ul>
<li class="chapter" data-level="6.1" data-path="overview-posterior-distances.html"><a href="overview-posterior-distances.html"><i class="fa fa-check"></i><b>6.1</b> Computing the Posterior Probabiilty of a Contact <span class="math inline">\(p(\c \eq 1 | \X)\)</span></a></li>
<li class="chapter" data-level="6.2" data-path="laplace-approx.html"><a href="laplace-approx.html"><i class="fa fa-check"></i><b>6.2</b> Gaussian approximation to the posterior of couplings</a><ul>
<li class="chapter" data-level="6.2.1" data-path="laplace-approx.html"><a href="laplace-approx.html#laplace-approx-improvement"><i class="fa fa-check"></i><b>6.2.1</b> Iterative improvement of Laplace approximation</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="coupling-prior.html"><a href="coupling-prior.html"><i class="fa fa-check"></i><b>6.3</b> Modelling the prior over couplings with dependence on <span class="math inline">\(\cij\)</span></a></li>
<li class="chapter" data-level="6.4" data-path="likelihood-fct-distances.html"><a href="likelihood-fct-distances.html"><i class="fa fa-check"></i><b>6.4</b> Computing the likelihood function of contact states <span class="math inline">\(p(\X | \c)\)</span></a></li>
<li class="chapter" data-level="6.5" data-path="bayesian-model-training-hyperparameters.html"><a href="bayesian-model-training-hyperparameters.html"><i class="fa fa-check"></i><b>6.5</b> Training the Hyperparameters in the Likelihood Function of Contact States</a></li>
<li class="chapter" data-level="6.6" data-path="posterior-of-rij.html"><a href="posterior-of-rij.html"><i class="fa fa-check"></i><b>6.6</b> The posterior probability distribution for contact states <span class="math inline">\(\cij\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>7</b> Methods</a><ul>
<li class="chapter" data-level="7.1" data-path="dataset.html"><a href="dataset.html"><i class="fa fa-check"></i><b>7.1</b> Dataset</a></li>
<li class="chapter" data-level="7.2" data-path="computing-pseudo-likelihood-couplings.html"><a href="computing-pseudo-likelihood-couplings.html"><i class="fa fa-check"></i><b>7.2</b> Computing Pseudo-Likelihood Couplings</a><ul>
<li class="chapter" data-level="7.2.1" data-path="computing-pseudo-likelihood-couplings.html"><a href="computing-pseudo-likelihood-couplings.html#diff-ccmpred-ccmpredpy"><i class="fa fa-check"></i><b>7.2.1</b> Differences between CCMpred and CCMpredpy</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="seq-reweighting.html"><a href="seq-reweighting.html"><i class="fa fa-check"></i><b>7.3</b> Sequence Reweighting</a></li>
<li class="chapter" data-level="7.4" data-path="amino-acid-frequencies.html"><a href="amino-acid-frequencies.html"><i class="fa fa-check"></i><b>7.4</b> Computing Amino Acid Frequencies</a></li>
<li class="chapter" data-level="7.5" data-path="methods-regularization.html"><a href="methods-regularization.html"><i class="fa fa-check"></i><b>7.5</b> Regularization</a></li>
<li class="chapter" data-level="7.6" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html"><i class="fa fa-check"></i><b>7.6</b> The Potts Model</a><ul>
<li class="chapter" data-level="7.6.1" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html#gap-treatment"><i class="fa fa-check"></i><b>7.6.1</b> Treating Gaps as Missing Information</a></li>
<li class="chapter" data-level="7.6.2" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html#the-regularized-full-log-likelihood-and-its-gradient-with-gap-treatment"><i class="fa fa-check"></i><b>7.6.2</b> The Regularized Full Log Likelihood and its Gradient With Gap Treatment</a></li>
<li class="chapter" data-level="7.6.3" data-path="potts-full-likelihood.html"><a href="potts-full-likelihood.html#prior-v"><i class="fa fa-check"></i><b>7.6.3</b> The prior on <span class="math inline">\(\v\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html"><i class="fa fa-check"></i><b>7.7</b> Analysis of Coupling Matrices</a><ul>
<li class="chapter" data-level="7.7.1" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-correlation"><i class="fa fa-check"></i><b>7.7.1</b> Correlation of Couplings with Contact Class</a></li>
<li class="chapter" data-level="7.7.2" data-path="analysis-of-coupling-matrices.html"><a href="analysis-of-coupling-matrices.html#method-coupling-profile"><i class="fa fa-check"></i><b>7.7.2</b> Coupling Distribution Plots</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="methods-sgd.html"><a href="methods-sgd.html"><i class="fa fa-check"></i><b>7.8</b> Optimizing Contrastive Divergence with Stochastic Gradient Descent</a><ul>
<li class="chapter" data-level="7.8.1" data-path="methods-sgd.html"><a href="methods-sgd.html#methods-full-likelihood-adam"><i class="fa fa-check"></i><b>7.8.1</b> Tuning Hyperparameters of <em>ADAM</em> Optimizer</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="methods-cd-sampling.html"><a href="methods-cd-sampling.html"><i class="fa fa-check"></i><b>7.9</b> Computing the Gradient with Contrastive Divergence</a></li>
<li class="chapter" data-level="7.10" data-path="Hessian-offdiagonal.html"><a href="Hessian-offdiagonal.html"><i class="fa fa-check"></i><b>7.10</b> The Hessian off-diagonal Elements Carry a Negligible Signal</a></li>
<li class="chapter" data-level="7.11" data-path="neg-Hessian-computation.html"><a href="neg-Hessian-computation.html"><i class="fa fa-check"></i><b>7.11</b> Efficiently Computing the negative Hessian of the regularized log-likelihood</a></li>
<li class="chapter" data-level="7.12" data-path="inv-lambda-ij-k.html"><a href="inv-lambda-ij-k.html"><i class="fa fa-check"></i><b>7.12</b> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></a></li>
<li class="chapter" data-level="7.13" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html"><i class="fa fa-check"></i><b>7.13</b> Training the Hyperparameters in the Likelihood Function of Contact States</a><ul>
<li class="chapter" data-level="7.13.1" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html#dataset-training-hyperparmeters"><i class="fa fa-check"></i><b>7.13.1</b> Dataset Specifications</a></li>
<li class="chapter" data-level="7.13.2" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html#model-specifications-training-hyperparmeters"><i class="fa fa-check"></i><b>7.13.2</b> Model Specifications</a></li>
<li class="chapter" data-level="7.13.3" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html#gradient-muk"><i class="fa fa-check"></i><b>7.13.3</b> The gradient of the log likelihood with respect to <span class="math inline">\(\muk\)</span></a></li>
<li class="chapter" data-level="7.13.4" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html#gradient-lambdak"><i class="fa fa-check"></i><b>7.13.4</b> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></a></li>
<li class="chapter" data-level="7.13.5" data-path="training-hyperparameters.html"><a href="training-hyperparameters.html#the-gradient-of-the-log-likelihood-with-respect-to-gamma_k"><i class="fa fa-check"></i><b>7.13.5</b> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.14" data-path="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html"><a href="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html"><i class="fa fa-check"></i><b>7.14</b> Extending the Bayesian Statistical Model for the Prediction of Protein Residue-Residue Distances</a><ul>
<li class="chapter" data-level="7.14.1" data-path="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html"><a href="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html#the-derivative-of-the-log-likelihood-with-respect-to-rho_k"><i class="fa fa-check"></i><b>7.14.1</b> The derivative of the log likelihood with respect to <span class="math inline">\(\rho_k\)</span></a></li>
<li class="chapter" data-level="7.14.2" data-path="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html"><a href="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances.html#the-derivative-of-the-log-likelihood-with-respect-to-alpha_k"><i class="fa fa-check"></i><b>7.14.2</b> The derivative of the log likelihood with respect to <span class="math inline">\(\alpha_k\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.15" data-path="seq-features.html"><a href="seq-features.html"><i class="fa fa-check"></i><b>7.15</b> Features used to train Random Forest Model</a><ul>
<li class="chapter" data-level="7.15.1" data-path="seq-features.html"><a href="seq-features.html#seq-features-global"><i class="fa fa-check"></i><b>7.15.1</b> Global Features</a></li>
<li class="chapter" data-level="7.15.2" data-path="seq-features.html"><a href="seq-features.html#seq-features-single"><i class="fa fa-check"></i><b>7.15.2</b> Single Position Features</a></li>
<li class="chapter" data-level="7.15.3" data-path="seq-features.html"><a href="seq-features.html#seq-features-pairwise"><i class="fa fa-check"></i><b>7.15.3</b> Pairwise Features</a></li>
</ul></li>
<li class="chapter" data-level="7.16" data-path="rf-training.html"><a href="rf-training.html"><i class="fa fa-check"></i><b>7.16</b> Training Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="7.16.1" data-path="rf-training.html"><a href="rf-training.html#rf-feature-selection"><i class="fa fa-check"></i><b>7.16.1</b> Feature Selection</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="abbrev.html"><a href="abbrev.html"><i class="fa fa-check"></i><b>A</b> Abbreviations</a></li>
<li class="chapter" data-level="B" data-path="amino-acids.html"><a href="amino-acids.html"><i class="fa fa-check"></i><b>B</b> Amino Acid Alphabet</a></li>
<li class="chapter" data-level="C" data-path="dataset-properties.html"><a href="dataset-properties.html"><i class="fa fa-check"></i><b>C</b> Dataset Properties</a></li>
<li class="chapter" data-level="D" data-path="standard-deviation-of-couplings-for-noncontacts.html"><a href="standard-deviation-of-couplings-for-noncontacts.html"><i class="fa fa-check"></i><b>D</b> Standard Deviation of Couplings for Noncontacts</a></li>
<li class="chapter" data-level="E" data-path="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><a href="amino-acid-interaction-preferences-reflected-in-coupling-matrices.html"><i class="fa fa-check"></i><b>E</b> Amino Acid Interaction Preferences Reflected in Coupling Matrices</a><ul>
<li class="chapter" data-level="E.1" data-path="pi-cation.html"><a href="pi-cation.html"><i class="fa fa-check"></i><b>E.1</b> Pi-Cation interactions</a></li>
<li class="chapter" data-level="E.2" data-path="disulfide.html"><a href="disulfide.html"><i class="fa fa-check"></i><b>E.2</b> Disulfide Bonds</a></li>
<li class="chapter" data-level="E.3" data-path="aromatic-proline.html"><a href="aromatic-proline.html"><i class="fa fa-check"></i><b>E.3</b> Aromatic-Proline Interactions</a></li>
<li class="chapter" data-level="E.4" data-path="aromatic-network.html"><a href="aromatic-network.html"><i class="fa fa-check"></i><b>E.4</b> Network-like structure of aromatic residues</a></li>
<li class="chapter" data-level="E.5" data-path="aromatic-small-distances.html"><a href="aromatic-small-distances.html"><i class="fa fa-check"></i><b>E.5</b> Aromatic Sidechains at small <span class="math inline">\(Cb\)</span>-<span class="math inline">\(\Cb\)</span> distances</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="optimizing-full-likelihood-with-gradient-descent.html"><a href="optimizing-full-likelihood-with-gradient-descent.html"><i class="fa fa-check"></i><b>F</b> Optimizing Full Likelihood with Gradient Descent</a><ul>
<li class="chapter" data-level="F.1" data-path="visualisation-of-learning-rate-schedules.html"><a href="visualisation-of-learning-rate-schedules.html"><i class="fa fa-check"></i><b>F.1</b> Visualisation of learning rate schedules</a></li>
<li class="chapter" data-level="F.2" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html"><i class="fa fa-check"></i><b>F.2</b> Benchmarking learning rate schedules</a><ul>
<li class="chapter" data-level="F.2.1" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#linear-learning-rate-schedule"><i class="fa fa-check"></i><b>F.2.1</b> Linear learning rate schedule</a></li>
<li class="chapter" data-level="F.2.2" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#sigmoidal-learning-rate-schedule"><i class="fa fa-check"></i><b>F.2.2</b> Sigmoidal learning rate schedule</a></li>
<li class="chapter" data-level="F.2.3" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#square-root-learning-rate-schedule"><i class="fa fa-check"></i><b>F.2.3</b> Square root learning rate schedule</a></li>
<li class="chapter" data-level="F.2.4" data-path="benchmark-learning-rate-annealing-schedules.html"><a href="benchmark-learning-rate-annealing-schedules.html#exponential-learning-rate-schedule"><i class="fa fa-check"></i><b>F.2.4</b> Exponential learning rate schedule</a></li>
</ul></li>
<li class="chapter" data-level="F.3" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html"><i class="fa fa-check"></i><b>F.3</b> Number of iterations until convergence for different learning rate schedules</a><ul>
<li class="chapter" data-level="F.3.1" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#linear-learning-rate-schedule-1"><i class="fa fa-check"></i><b>F.3.1</b> Linear learning rate schedule</a></li>
<li class="chapter" data-level="F.3.2" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#sigmoidal-learning-rate-schedule-1"><i class="fa fa-check"></i><b>F.3.2</b> Sigmoidal learning rate schedule</a></li>
<li class="chapter" data-level="F.3.3" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#square-root-learning-rate-schedule-1"><i class="fa fa-check"></i><b>F.3.3</b> Square root learning rate schedule</a></li>
<li class="chapter" data-level="F.3.4" data-path="learning-rate-schedules-distribution-iterations.html"><a href="learning-rate-schedules-distribution-iterations.html#exponential-learning-rate-schedule-1"><i class="fa fa-check"></i><b>F.3.4</b> Exponential learning rate schedule</a></li>
</ul></li>
<li class="chapter" data-level="F.4" data-path="modifying-number-of-iterations-over-which-relative-change-of-coupling-norm-is-evaluated.html"><a href="modifying-number-of-iterations-over-which-relative-change-of-coupling-norm-is-evaluated.html"><i class="fa fa-check"></i><b>F.4</b> Modifying Number of Iterations over which Relative Change of Coupling Norm is Evaluated</a></li>
<li class="chapter" data-level="F.5" data-path="number-of-gibbs-steps-with-respect-to-neff.html"><a href="number-of-gibbs-steps-with-respect-to-neff.html"><i class="fa fa-check"></i><b>F.5</b> Number of Gibbs steps with respect to Neff</a></li>
<li class="chapter" data-level="F.6" data-path="fix-single-potentials-at-maximum-likelihood-estimate-v.html"><a href="fix-single-potentials-at-maximum-likelihood-estimate-v.html"><i class="fa fa-check"></i><b>F.6</b> Fix single potentials at maximum-likelihood estimate v*</a></li>
<li class="chapter" data-level="F.7" data-path="monitoring-optimization-for-different-sample-sizes.html"><a href="monitoring-optimization-for-different-sample-sizes.html"><i class="fa fa-check"></i><b>F.7</b> Monitoring Optimization for Different Sample Sizes</a></li>
<li class="chapter" data-level="F.8" data-path="monitoring-norm-of-gradients-for-different-number-of-gibbs-steps.html"><a href="monitoring-norm-of-gradients-for-different-number-of-gibbs-steps.html"><i class="fa fa-check"></i><b>F.8</b> Monitoring Norm of Gradients for Different Number of Gibbs Steps</a></li>
<li class="chapter" data-level="F.9" data-path="statistics-for-comparing-couplings-computed-with-pseudo-likelihood-and-contrastive-divergence.html"><a href="statistics-for-comparing-couplings-computed-with-pseudo-likelihood-and-contrastive-divergence.html"><i class="fa fa-check"></i><b>F.9</b> Statistics for Comparing Couplings computed with Pseudo-likelihood and Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="training-of-the-random-forest-contact-prior.html"><a href="training-of-the-random-forest-contact-prior.html"><i class="fa fa-check"></i><b>G</b> Training of the Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="G.1" data-path="training-random-forest-model-with-pseudo-likelihood-feature.html"><a href="training-random-forest-model-with-pseudo-likelihood-feature.html"><i class="fa fa-check"></i><b>G.1</b> Training Random Forest Model with pseudo-likelihood Feature</a></li>
<li class="chapter" data-level="G.2" data-path="rf-window-size.html"><a href="rf-window-size.html"><i class="fa fa-check"></i><b>G.2</b> Evaluating window size with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="G.3" data-path="rf-noncontact-threshold.html"><a href="rf-noncontact-threshold.html"><i class="fa fa-check"></i><b>G.3</b> Evaluating non-contact threshold with 5-fold Cross-validation</a></li>
<li class="chapter" data-level="G.4" data-path="rf-ratio-noncontacts.html"><a href="rf-ratio-noncontacts.html"><i class="fa fa-check"></i><b>G.4</b> Evaluating ratio of non-contacts and contacts in the training set with 5-fold Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PhD thesis: residue-residue contact prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="maxent" class="section level2">
<h2><span class="header-section-number">2.4</span> Modelling Protein Families with Potts Model</h2>
<p>Infering contacts from a joint probability distribution over all residues in a protein sequence instead of using simple pairwise statistics has been proven to enable the distinction of direct statistical dependencies between residues from indirect dependencies mediated through other residues. The global statistical model that is commonly used to describe this joint probability distribution is the <em>Potts model</em>. It is a well-established model in statistical mechanics and can be derived from a maximum entropy assumption which is explained in the following.</p>
<p>The principle of maximum entropy, proposed by Jaynes in 1957 <span class="citation">[<a href="#ref-Jaynes1957a">89</a>,<a href="#ref-Jaynes1957b">90</a>]</span>, states that the probability distribution which makes minimal assumptions and best represents observed data is the one that is in agreement with measured constraints (prior information) and has the largest entropy. In other words, from all distributions that are consistent with measured data, the distribution with maximal entropy should be chosen.</p>
<p>A protein family is represented by a <a href="abbrev.html#abbrev">MSA</a> <span class="math inline">\(\X = \{ \seq_1, \ldots, \seq_N \}\)</span> of <span class="math inline">\(N\)</span> protein sequences. Every protein sequence of the protein family represents a sample drawn from a target distribution <span class="math inline">\(p(\seq)\)</span>, so that each protein sequence is associated with a probability. Each sequence <span class="math inline">\(\seq_n = (\seq_{n1}, ..., \seq_{nL})\)</span> is of length <span class="math inline">\(L\)</span> and every position constitutes a categorical variable <span class="math inline">\(x_{i}\)</span> that can take values from an alphabet indexed by <span class="math inline">\(\{0, ..., 20\}\)</span>, where 0 stands for a gap and <span class="math inline">\(\{1, ... , 20\}\)</span> stand for the 20 types of amino acids. The measured constraints are given by the empirically observed single and pairwise amino acid frequencies that can be calculated as</p>
<!--
Applied to the problem of modelling protein families, one seeks a probability distribution $p(\seq)$ for protein sequences $\seq = (x_1, \ldots, x_L)$ of length $L$ from the protein family under study. 
The categorical variables $x_{i}$ can take one of $q=21$ values representing the 20 naturally occuring amino acids and a gap ('-').
Given $N$ sequences of the protein family in a [MSA](#abbrev) with $\X = \{ \seq_1, \ldots, \seq_N \}$, the empirically observed single and pairwise amino acid frequencies can be calculated as
-->
<span class="math display">\[\begin{equation}
    f_i(a) = f(x_i\eq a) = \frac{1}{N}\sum_{n=1}^N I(x_{ni} \eq a) \; ,
\end{equation}\]</span>
<span class="math display" id="eq:emp-freq">\[\begin{equation}
    f_{ij}(a,b) = f(x_i\eq a, x_j\eq b) = \frac{1}{N} \sum_{n=1}^N I(x_{ni} \eq a, x_{nj} \eq b) \; .
 \tag{2.1}
\end{equation}\]</span>
<p>According to the maximum entropy principle, the distribution <span class="math inline">\(p(\seq)\)</span> should have maximal entropy and reproduce the empirically observed amino acid frequencies, so that</p>
<span class="math display" id="eq:maxent-reproducing-emp-freq">\[\begin{align}
   f(x_i\eq a)            &amp;\equiv p(x_i\eq a)  \nonumber\\
                                    &amp;= \sum_{\seq\prime_1, \ldots, \seq\prime_L = 1}^{q} p(x\prime) I(x\prime_i \eq a) \\
  f(x_i\eq a, x_j\eq b)   &amp;\equiv p(x_i\eq a, x_j \eq b) \nonumber \\
                                    &amp;= \sum_{\seq\prime_1, \ldots, \seq\prime_L = 1}^{q}  p(x\prime) I(x\prime_i\eq a, x\prime_j \eq b)  \; .
 \tag{2.2}
\end{align}\]</span>
<p>Solving for the distribution <span class="math inline">\(p(\seq)\)</span> that maximizes the Shannon entropy <span class="math inline">\(S= -\sum_{\seq\prime} p(\seq\prime) \log p(\seq\prime)\)</span> while satisfying the constraints given by the empircial amino acid frequencies in eq. <a href="maxent.html#eq:maxent-reproducing-emp-freq">(2.2)</a> by introducing Lagrange multipliers <span class="math inline">\(\wij\)</span> and <span class="math inline">\(\vi\)</span>, results in the formulation of the <em>Potts model</em>,</p>
<!--
\begin{align}
F \left[ p(\seq) \right] =& -\sum_{\seq\prime} p(\seq\prime) \log p(\seq\prime) \\
        & + \sum_{i=1}^L \sum_{a=1}^{q} \vi(a) \left( p(x_i\eq a) - \mathcal{f}(x_i\eq a) \right) \\
        & + \sum_{1 \leq i < j \leq L}^L \; \sum_{a,b=1}^{q} \wij(a,b) \left( p(x_i\eq a, x_j \eq b) - \mathcal{f}(x_i\eq a, x_j\eq b) \right) \\
        & + \Omega \left( 1-\sum_{\seq\prime} p(\seq\prime)  \right)
(\#eq:derivation-max-ent-model)
\end{align}
-->
<span class="math display" id="eq:max-ent-model">\[\begin{equation}
    p(\seq | \v, \w ) = \frac{1}{Z(\v, \w)} \exp \left( \sum_{i=1}^L v_i(x_i) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_i, x_j) \right) \; .
\tag{2.3}
\end{equation}\]</span>
<p>The Lagrange multipliers <span class="math inline">\(\wij\)</span> and <span class="math inline">\(\vi\)</span> remain as model parameters to be fitted to data. <span class="math inline">\(Z\)</span> is a normalization constant also known as <em>partition function</em> that ensures the total probabilty adds up to one by summing over all possible assignments to <span class="math inline">\(\seq\)</span>,</p>
<span class="math display" id="eq:partition-fct-likelihood">\[\begin{equation}
  Z(\v, \w) = \sum_{\seq\prime_1, \ldots, \seq\prime_L = 1}^{q} \exp  \left( \sum_{i=1}^L v_i(x_i) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_i, x_j) \right) \; .
  \tag{2.4}
\end{equation}\]</span>
<div id="potts-model-properties" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Model Properties</h3>
<p>The Potts model is specified by singlet terms <span class="math inline">\(\via\)</span> which describe the tendency for each amino acid a to appear at position <span class="math inline">\(i\)</span>, and pair terms <span class="math inline">\(\wijab\)</span>, also called couplings, which describe the tendency of amino acid a at position <span class="math inline">\(i\)</span> to co-occur with amino acid b at position <span class="math inline">\(j\)</span>. In contrast to mere correlations, the couplings explain the causative dependence structure between positions by jointly modelling the distribution of all positions in a protein sequence and thus account for transitive effects. By doing so, a major source of noise in contact prediction methods is eliminated.</p>
<p>To get some intuition for the coupling coefficients, note that <span class="math inline">\(\wijab = 1\)</span> corresponds to a 2.7-fold higher probability for a and b to occur together than what is expected from the singlet frequencies if a and b were independent. Pairs of residues that are not in contact tend to have negligable couplings, <span class="math inline">\(\wij \approx 0\)</span>, whereas pairs in contact tend to have vectors significantly different from 0. For contacting residues <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in real world <a href="abbrev.html#abbrev">MSAs</a> typical coupling strengths are on the order of <span class="math inline">\(||\wij || \approx 0.1\)</span> (regularization dependent).</p>
<p>Maximum entropy models naturally give rise to exponential family distributions that express useful properties for statistical modelling, such as the convexity of the likelihood function which consequently has a unique, global minimum <span class="citation">[<a href="#ref-Wainwright2007">91</a>,<a href="#ref-Murphy2012">92</a>]</span>.</p>
<p>The Potts model is a discrete instance of what is referred to as a pairwise <a href="abbrev.html#abbrev">Markov random field</a> in the statistics community. <a href="abbrev.html#abbrev">MRFs</a> belong to the class of undirected graphical models, that represent the probability distribution in terms of a graph with nodes and edges characterizing the variables and the dependence structure between variables, respectively.</p>
<div id="gauge-invariance" class="section level4">
<h4><span class="header-section-number">2.4.1.1</span> Gauge Invariance</h4>
<p>As every variable <span class="math inline">\(x_{ni}\)</span> can take <span class="math inline">\(q=21\)</span> values, the model has <span class="math inline">\(L \! \times \! q + L(L-1)/2 \! \times \! q^2\)</span> parameters. But the parameters are not uniquely determined and multiple parametrizations yield identical probability distributions.</p>
<p>For example, adding a constant to all elements in <span class="math inline">\(v_i\)</span> for any fixed position <span class="math inline">\(i\)</span> or similarly adding a constant to <span class="math inline">\(\via\)</span> for any fixed position <span class="math inline">\(i\)</span> and amino acid <span class="math inline">\(a\)</span> and subtracting the same constant from the <span class="math inline">\(qL\)</span> coefficients <span class="math inline">\(\wijab\)</span> with <span class="math inline">\(b \in \{1, \ldots, q\}\)</span> and <span class="math inline">\(j \in \{1, \ldots, L \}\)</span> leaves the probabilities for all sequences under the model unchanged, since such a change will be compensated by a change of <span class="math inline">\(Z(\v, \w)\)</span> in eq. <a href="maxent.html#eq:partition-fct-likelihood">(2.4)</a>.</p>
<p>The overparametrization is referred to as <em>gauge invariance</em> in statistical physics literature and can be eliminated by removing parameters <span class="citation">[<a href="#ref-Weigt2009">61</a>,<a href="#ref-Morcos2011">93</a>]</span>. An appropriate choice of which parameters to remove, referred to as <em>gauge choice</em>, reduces the number of parameters to <span class="math inline">\(L \! \times \! (q-1) + L(L-1)/2 \! \times \! (q-1)^2\)</span>. Popular gauge choices are the <em>zero-sum gauge</em> or <em>Ising-gauge</em> used by Weigt et al. <span class="citation">[<a href="#ref-Weigt2009">61</a>]</span> imposed by the restraints,</p>
<span class="math display" id="eq:zero-sum-gauge">\[\begin{equation}
    \sum_{a=1}^{q} v_{ia} = \sum_{a=1}^{q} \wijab = \sum_{a=1}^{q} w_{ijba} = 0
\tag{2.5}
\end{equation}\]</span>
<p>for all <span class="math inline">\(i,j,b\)</span> or the <em>lattice-gas gauge</em> used by Morcos et al <span class="citation">[<a href="#ref-Morcos2011">93</a>]</span> and Marks et al <span class="citation">[<a href="#ref-Marks2011">36</a>]</span> imposed by restraints</p>
<span class="math display" id="eq:ising-gauge">\[\begin{equation}
    \wij(q,a) = \wij(a,q) = \vi(q) = 0
\tag{2.6}
\end{equation}\]</span>
<p>for all <span class="math inline">\(i,j,a\)</span> <span class="citation">[<a href="#ref-Cocco2017">94</a>]</span>.</p>
<p>Alternatively, the indeterminacy can be fixed by including a regularization prior (see next section). The regularizer selects for a unique solution among all parametrizations of the optimal distribution and therefore eliminates the need to choose a gauge <span class="citation">[<a href="#ref-Koller2009">95</a>–<a href="#ref-Stein2015a">97</a>]</span>.</p>
</div>
</div>
<div id="potts-mle" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Inferring Parameters for the Potts Model</h3>
<p>Typically, parameter estimates are obtained by maximizing the log-likelihood function of the parameters over observed data. For the Potts model, the log-likelihood function is computed over sequences in the alignment <span class="math inline">\(\mathbf{X}\)</span>:</p>
<span class="math display" id="eq:full-log-likelihood">\[\begin{align}
    \text{LL}(\v, \w | \mathbf{X}) =&amp; \sum_{n=1}^N \log p(\seq_n)  \nonumber\\
    =&amp; \sum_{n=1}^N \left[ \sum_{i=1}^L v_i(x_{ni}) + \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_{xn}, x_{nj}) - \log Z \right]
\tag{2.7}
\end{align}\]</span>
<p>The number of parameters in a Potts model is typically larger than the number of observations, i.e. the number of sequences in the <a href="abbrev.html#abbrev">MSA</a>. Considering a protein of length <span class="math inline">\(L=100\)</span>, there are approximately <span class="math inline">\(2 \times 10^6\)</span> parameters in the model whereas the largest protein families comprise only around <span class="math inline">\(10^5\)</span> sequences (see Figure <a href="challenges.html#fig:pfam">2.9</a>). An underdetermined problem like this renders the use of regularizers neccessary in order to prevent overfitting.</p>
<p>Typically, an L2-regularization is used that pushes the single and pairwise terms smoothly towards zero and is equivalent to the logarithm of a zero-centered Gaussian prior,</p>
<span class="math display" id="eq:l2-reg">\[\begin{align}
  R(\v, \w)  &amp;= \log \left[ \mathcal{N}(\v | \mathbf{0}, \lambda_v^{-1} I) \mathcal{N}(\w | \mathbf{0}, \lambda_w^{-1} I) \right] \nonumber \\
             &amp;= -\frac{\lambda_v}{2} ||\v||_2^2 - \frac{\lambda_w}{2} ||\w||_2^2 + \text{const.} \; ,
\tag{2.8}
\end{align}\]</span>
<p>where the strength of regularization is tuned via the regularization coefficients <span class="math inline">\(\lambda_v\)</span> and <span class="math inline">\(\lambda_w\)</span> <span class="citation">[<a href="#ref-Seemayer2014">98</a>–<a href="#ref-Kamisetty2013">100</a>]</span>.</p>
<p>However, optimizing the log-likelihood requires computing the partition function <span class="math inline">\(Z\)</span> given in eq. <a href="maxent.html#eq:partition-fct-likelihood">(2.4)</a> that sums <span class="math inline">\(q^L\)</span> terms. Computing this sum is intractable for realistic protein domains with more than 100 residues. Consequently, evaluating the likelihood function at each iteration of an optimization procedure is infeasible due to the exponential complexity of the partition function in protein length <span class="math inline">\(L\)</span>.</p>
<p>Many approximate inference techniques have been developed to sidestep the infeasible computation of the partition function for the specific problem of predicting contacts that are briefly explained in the next section.</p>
</div>
<div id="potts-model-solutions" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Solving the Inverse Potts Problem</h3>
<p>In 1999 Lapedes et al. were the first to propose maximum entropy models for the prediction of residue-residue contacts in order to disentangle transitive effects <span class="citation">[<a href="#ref-Lapedes1999">59</a>]</span>. In 2002 they applied their idea to 11 small proteins using an iterative Monte Carlo procedure to obtain estimates of the model parameters and achieved an increase in accuracy of 10-20% compared to the local statistical models <span class="citation">[<a href="#ref-Lapedes2012a">101</a>]</span>. As the calculations involved were very time-consuming and at that time required supercomputing resources, the wider implications were not noted yet.</p>
<p>Ten years later Weight et al proposed an iterative message-passing algorithm, here referred to as <em>mpDCA</em>, to approximate the partition function <span class="citation">[<a href="#ref-Weigt2009">61</a>]</span>. Eventhough their approach is computationally very expensive and in practice only applicable to small proteins, they obtained remarkable results for the two-component signaling system in bacteria.</p>
<p>Balakrishnan et al were the first to apply pseudo-likelihood approximations to the full likelihood in 2011 <span class="citation">[<a href="#ref-Balakrishnan2011">102</a>]</span>. The pseudo-likelihood optimizes a different objective and replaces the global partition function <span class="math inline">\(Z\)</span> with local estimates. Balakrishnan and colleagues applied their method <em>GREMLIN</em> to learn sparse graphical models for 71 protein families. In a follow-up study in 2013, the authors proposed an improved version of <em>GREMLIN</em> that uses additional prior information <span class="citation">[<a href="#ref-Kamisetty2013">100</a>]</span>.</p>
<p>Also in 2011, Morcos et al. introduced a naive mean-field inversion approximation to the partition function, named <em>mfDCA</em> <span class="citation">[<a href="#ref-Morcos2011">93</a>]</span>. This method allows for drastically shorter running times as the mean-field approach boils down to inverting the empirical covariance matrix calculated from observed amino acid frequencies for each residue pair <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> of the alignment. This study performed the first high-throughput analysis of intradomain contacts for 131 protein families and facilitated the prediction of protein structures from accurately predicted contacts in <span class="citation">[<a href="#ref-Marks2011">36</a>]</span>.</p>
<p>The initial work by Balakrishnan and collegueas went almost unnoted as it was not primarily targeted to the problem of contact prediction. Ekeberg and collegueas independently developed the pseudo-likelihood method <em>plmDCA</em> in 2013 and showed its superior precision over <em>mfDCA</em> <span class="citation">[<a href="#ref-Ekeberg2013">96</a>]</span>.</p>
<p>A related approach to mean-field approximation is sparse inverse covariance estimation, named <em>PSICOV</em>, developed by Jones et al. (2012) <span class="citation">[<a href="#ref-Jones2012">63</a>]</span>. PSICOV uses an L1-regularization, known as graphical Lasso, to invert the correlation matrix and learn a sparse graphical model <span class="citation">[<a href="#ref-Friedman2008">103</a>]</span>. Both procedures, <em>mfDCA</em> and <em>PSICOV</em>, assume the model distribution to be a multivariate Gaussian. It has been shown by Banerjee et al. (2008)that this dual optimization solution also applies to binary data, as is the case in this application, where each position is encoded as a 20-dimensional binary vector <span class="citation">[<a href="#ref-Banerjee2008">104</a>]</span>.</p>
<p>Another related approach to <em>mfDCA</em> and <em>PSICOV</em> is <em>gaussianDCA</em>, proposed in 2014 by Baldassi et al. <span class="citation">[<a href="#ref-Baldassi2014">105</a>]</span>. Similar to the other both approaches, they model the data as multivariate Gaussian but within a simple Bayesian formalism by using a suitable prior and estimating parameters over the posterior distribution.</p>
<p>So far, pseudo-likelihood has proven to be the most successful approximation of the likelihood with respect to contact prediction performance. Currently, there exist several implementations of pseudo-likelihood maximization that vary in slight details, perform similarly and thus are equally popular in the community, such as CCMpred <span class="citation">[<a href="#ref-Seemayer2014">98</a>]</span>, plmDCA<span class="citation">[<a href="#ref-Ekeberg2014">99</a>]</span> and GREMLIN <span class="citation">[<a href="#ref-Kamisetty2013">100</a>]</span>.</p>
<div id="pseudo-likelihood" class="section level4">
<h4><span class="header-section-number">2.4.3.1</span> Maximum Likelihood Inference for Pseudo-Likelihood</h4>
<p>The pseudo-likelihood is a rather old estimation principle that was suggested by Besag already in 1975 <span class="citation">[<a href="#ref-Besag1975">106</a>]</span>. It represents a different objective function than the full likelihood and approximates the joint probability with the product over conditionals for each variable, i.e. the conditional probability of observing one variable given all the others:</p>
<span class="math display">\[\begin{align}
  p(\seq | \v,\w) \approx&amp;   \prod_{i=1}^L p(x_i | \seq_{\backslash xi}, \v,\w) \nonumber \\
                        =&amp;  \prod_{i=1}^L \frac{1}{Z_i} \exp \left(  v_i(x_i) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_i, x_j) \right)
\end{align}\]</span>
<p>Here, the normalization term <span class="math inline">\(Z_i\)</span> sums only over all assignments to one position <span class="math inline">\(i\)</span> in sequence:</p>
<span class="math display" id="eq:partition-fct-pll">\[\begin{equation}
  Z_i = \sum_{a=1}^{q} \exp \left( v_i(a) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(a, x_j) \right)
\tag{2.9}
\end{equation}\]</span>
<p>Replacing the global partition function in the full likelihood with local estimates of lower complexity in the pseudo-likelihood objective resolves the computational intractability of the parameter optimization procedure. Hence, it is feasible to maximize the pseudo-log-likelihood function,</p>
<span class="math display">\[\begin{align}
    \text{pLL}(\v, \w | \mathbf{X}) =&amp; \sum_{n=1}^N \sum_{i=1}^L \log p(x_i | \seq_{\backslash xi}, \v,\w) \nonumber \\
    =&amp; \sum_{n=1}^N \sum_{i=1}^L  \left[ v_i(x_{ni}) + \sum_{j=i+1}^L  w_{ij}(x_{ni}, x_{nj}) - \log Z_{ni} \right] \;,
\end{align}\]</span>
<p>plus an additional regularization term in order to prevent overfitting and to fix the gauge to arrive at a <a href="abbrev.html#abbrev">MAP</a> estimate of the parameters,</p>
<span class="math display">\[\begin{equation}
    \hat{\v}, \hat{\w} = \underset{\v, \w}{\operatorname{argmax}} \; \text{pLL}(\v, \w | \mathbf{X}) + R(\v, \w) \; .
\end{equation}\]</span>
<p>Eventhough the pseudo-likelihood optimizes a different objective than the full-likelihood, it has been found to work well in practice for many problems, including contact prediction <span class="citation">[<a href="#ref-Murphy2012">92</a>,<a href="#ref-Koller2009">95</a>–<a href="#ref-Stein2015a">97</a>]</span>. The pseudo-likelihood function retains the concavity of the likelihood and it has been proven to be a consistent estimator in the limit of infinite data for models of the exponential family <span class="citation">[<a href="#ref-Koller2009">95</a>,<a href="#ref-Besag1975">106</a>,<a href="#ref-Gidas1988">107</a>]</span>. That is, as the number of sequences in the alignment increases, pseudo-likelihood estimates converge towards the true full likelihood parameters.</p>
</div>
</div>
<div id="post-processing-heuristics" class="section level3">
<h3><span class="header-section-number">2.4.4</span> Computing Contact Maps</h3>
<p>Model inference as described in the last section yields <a href="abbrev.html#abbrev">MAP</a> estimates of the couplings <span class="math inline">\(\hat{\w}_{ij}\)</span>. In order to obtain a scalar measure for the coupling strength between two residues <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, all available methods presented in section <a href="maxent.html#potts-model-solutions">2.4.3</a> heuristically map the <span class="math inline">\(21 \! \times \! 21\)</span> dimensional coupling matrix <span class="math inline">\(\wij\)</span> to a single scalar quantity.</p>
<p><em>mpDCA</em> <span class="citation">[<a href="#ref-Weigt2009">61</a>]</span> and <em>mfDCA</em> <span class="citation">[<a href="#ref-Marks2011">36</a>,<a href="#ref-Morcos2011">93</a>]</span> employ a score called <a href="abbrev.html#abbrev">DI</a>, that essentially computes the <a href="abbrev.html#abbrev">MI</a> for two positions <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> using the couplings <span class="math inline">\(\wij\)</span> instead of pairwise amino acid frequencies. Most pseudo-likelihood methods (<em>plmDCA</em> <span class="citation">[<a href="#ref-Ekeberg2013">96</a>,<a href="#ref-Ekeberg2014">99</a>]</span>, <em>CCMpred</em> <span class="citation">[<a href="#ref-Seemayer2014">98</a>]</span>, <em>GREMLIN</em> <span class="citation">[<a href="#ref-Kamisetty2013">100</a>]</span>) compute the <em>Frobenius norm</em> of the coupling matrix <span class="math inline">\(\wij\)</span> to obtain a scalar contact score <span class="math inline">\(C_{ij}\)</span>,</p>
<span class="math display" id="eq:frobenius-norm">\[\begin{equation}
    C_{ij}  = ||\wij||_2 = \sqrt{\sum_{a,b=1}^q \wijab^2} \; .
\tag{2.10}
\end{equation}\]</span>
<p>The Frobenius norm improves prediction performance over <a href="abbrev.html#abbrev">DI</a> and further improvements can be obtained by computing the Frobenius norm only on the <span class="math inline">\(20 \times 20\)</span> submatrix thus ignoring contributions from gaps <span class="citation">[<a href="#ref-Ekeberg2013">96</a>,<a href="#ref-Baldassi2014">105</a>,<a href="#ref-Feinauer2014">108</a>]</span>. <em>PSICOV</em> <span class="citation">[<a href="#ref-Jones2012">63</a>]</span> uses an L1-norm on the <span class="math inline">\(20 \times 20\)</span> submatrix instead of the Frobenius norm.</p>
<p>Furthermore it should be noted that the Frobenius norm is gauge dependent and is minimized by the <em>zero-sum gauge</em> <span class="citation">[<a href="#ref-Weigt2009">61</a>]</span>. Therefore, the coupling matrices should be transformed to <em>zero-sum gauge</em> before computing the Frobenius norm</p>
<span class="math display" id="eq:zero-sum-gauge-transform">\[\begin{equation}
    \w^{\prime}_{ij}  = \wij - \wij(\cdot, b) - \wij(a, \cdot) + \wij(\cdot, \cdot) \; ,
\tag{2.11}
\end{equation}\]</span>
<p>where <span class="math inline">\(\cdot\)</span> denotes average over the respective indices <span class="citation">[<a href="#ref-Ekeberg2013">96</a>,<a href="#ref-Seemayer2014">98</a>,<a href="#ref-Ekeberg2014">99</a>,<a href="#ref-Baldassi2014">105</a>]</span>.</p>
<p>Another commonly applied heuristic known as <a href="abbrev.html#abbrev">APC</a> has been introduced by Dunn et al. in order to reduce background noise arising from correlations between positions with high entropy or phylogenetic couplings <span class="citation">[<a href="#ref-Dunn2008">56</a>]</span>. <a href="abbrev.html#abbrev">APC</a> is a correction term that is computed from the raw contact map as the product over average row and column contact scores <span class="math inline">\(\overline{C_i}\)</span> divided by the average contact score over all pairs <span class="math inline">\(\overline{C_{ij}}\)</span>. The corrected contact score <span class="math inline">\(C_{ij}^{APC}\)</span> is obtained by subtracting the <a href="abbrev.html#abbrev">APC</a> term from the raw contact score <span class="math inline">\(C_{ij}\)</span>,</p>
<span class="math display" id="eq:apc">\[\begin{equation}
    C_{ij}^{APC}  = C_{ij} - \frac{\overline{C_i} \; \overline{C_j}}{\overline{C_{ij}}}\; .
\tag{2.12}
\end{equation}\]</span>
<p>Visually, <a href="abbrev.html#abbrev">APC</a> creates a <em>smoothing</em> effect on the contact maps that is illustrated in Figure <a href="maxent.html#fig:apc-correction">2.3</a> and it has been found to substantially boost contact prediction performance <span class="citation">[<a href="#ref-Dunn2008">56</a>,<a href="#ref-Kamisetty2013">100</a>]</span>. It was first adopted by <em>PSICOV</em> <span class="citation">[<a href="#ref-Jones2012">63</a>]</span> but is now used by most methods to adjust raw contact scores.</p>
<p>It was long under debate why <a href="abbrev.html#abbrev">APC</a> works so well and how it can be interpreted. Zhang et al. showed that <a href="abbrev.html#abbrev">APC</a> essentially approximates the first principal component of the contact matrix and therefore removes the highest variability in the matrix that is assumed to arise from background biases <span class="citation">[<a href="#ref-Zhang2016">109</a>]</span>. Furthermore, they studied an advanced decomposition technique, called LRS matrix decomposition, that decomposes the contact matrix into a low-rank and a sparse component, representing background noise and true correlations, respectively.<br />
Inferring contacts from the sparse component works astonishing well, improving precision further over <a href="abbrev.html#abbrev">APC</a> independent of the underlying statistical model.</p>
<p>Dr Stefan Seemayer could show that the main component of background noise can be attributed to entropic effects and that a substantial part of <a href="abbrev.html#abbrev">APC</a> amounts to correcting for these entropic biases (unpublished). In his doctoral thesis, he developed an entropy correction, computed as the geometric mean of per-column entropies, that correlates well with the <a href="abbrev.html#abbrev">APC</a> correction term and yields similar precision for predicted contacts. The entropy correction has the advantage that it is computed from input statistics and therefore is independent of the statistical model used to infer the couplings. In contrast, <a href="abbrev.html#abbrev">APC</a> and other denoising techniques such as LRS <span class="citation">[<a href="#ref-Zhang2016">109</a>]</span> discussed above, estimate a background model from the final contact matrix, thus depending on the statistical model used to infer the contact matrix.</p>

<div class="figure" style="text-align: center"><span id="fig:apc-correction"></span>
<img src="img/intro/apc_correction_with_entropy.png" alt="Contact maps computed from pseudo-likelihood couplings. Subplot on top of the contact maps illustrates the normalized Shannon entropy (pink line) and percentage of gaps for every position in the alignment (brown line). Left: Contact map computed with Frobenius norm as in eq. (2.10). Overall coupling values are dominated by entropic effects, i.e. the amount of variation for a MSA position, leading to striped brightness patterns. For example, positions with high column entropy (e.g. positions 7, 12 or 31) have higher overall coupling values than positions with low column entropy (e.g. positions 11, 24 or 33). b: previous contact map but corrected for background noise with the APC as in eq. (2.12)." width="90%" />
<p class="caption">
Figure 2.3: Contact maps computed from pseudo-likelihood couplings. Subplot on top of the contact maps illustrates the normalized Shannon entropy (<span style="color:#e7539d;">pink </span> line) and percentage of gaps for every position in the alignment (<span style="color:brown;">brown </span> line). <strong>Left</strong>: Contact map computed with Frobenius norm as in eq. <a href="maxent.html#eq:frobenius-norm">(2.10)</a>. Overall coupling values are dominated by entropic effects, i.e. the amount of variation for a <a href="abbrev.html#abbrev">MSA</a> position, leading to striped brightness patterns. For example, positions with high column entropy (e.g. positions 7, 12 or 31) have higher overall coupling values than positions with low column entropy (e.g. positions 11, 24 or 33). <strong>b</strong>: previous contact map but corrected for background noise with the <a href="abbrev.html#abbrev">APC</a> as in eq. <a href="maxent.html#eq:apc">(2.12)</a>.
</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Jaynes1957a">
<p>89. Jaynes, E.T. (1957). Information Theory and Statistical Mechanics I. Phys. Rev. <em>106</em>, 620–630. Available at: <a href="https://link.aps.org/doi/10.1103/PhysRev.106.620" class="uri">https://link.aps.org/doi/10.1103/PhysRev.106.620</a>.</p>
</div>
<div id="ref-Jaynes1957b">
<p>90. Jaynes, E.T. (1957). Information Theory and Statistical Mechanics. II. Phys. Rev. <em>108</em>, 171–190. Available at: <a href="https://link.aps.org/doi/10.1103/PhysRev.108.171" class="uri">https://link.aps.org/doi/10.1103/PhysRev.108.171</a>.</p>
</div>
<div id="ref-Wainwright2007">
<p>91. Wainwright, M.J., and Jordan, M.I. (2007). Graphical Models, Exponential Families, and Variational Inference. Found. Trends Mach. Learn. <em>1</em>, 1–305. Available at: <a href="http://www.nowpublishers.com/article/Details/MAL-001" class="uri">http://www.nowpublishers.com/article/Details/MAL-001</a>.</p>
</div>
<div id="ref-Murphy2012">
<p>92. Murphy, K.P. (2012). Machine Learning: A Probabilistic Perspective (MIT Press).</p>
</div>
<div id="ref-Weigt2009">
<p>61. Weigt, M., White, R.A., Szurmant, H., Hoch, J.A., and Hwa, T. (2009). Identification of direct residue contacts in protein-protein interaction by message passing. Proc. Natl. Acad. Sci. U. S. A. <em>106</em>, 67–72. Available at: <a href="http://www.pnas.org/content/106/1/67.abstract" class="uri">http://www.pnas.org/content/106/1/67.abstract</a>.</p>
</div>
<div id="ref-Morcos2011">
<p>93. Morcos, F., Pagnani, A., Lunt, B., Bertolino, A., Marks, D.S., Sander, C., Zecchina, R., Onuchic, J.N., Hwa, T., and Weigt, M. (2011). Direct-coupling analysis of residue coevolution captures native contacts across many protein families. Proc. Natl. Acad. Sci. U. S. A. <em>108</em>, E1293–301. Available at: <a href="http://www.pnas.org/content/108/49/E1293.full" class="uri">http://www.pnas.org/content/108/49/E1293.full</a>.</p>
</div>
<div id="ref-Marks2011">
<p>36. Marks, D.S., Colwell, L.J., Sheridan, R., Hopf, T.A., Pagnani, A., Zecchina, R., and Sander, C. (2011). Protein 3D structure computed from evolutionary sequence variation. PLoS One <em>6</em>, e28766. Available at: <a href="http://dx.plos.org/10.1371/journal.pone.0028766" class="uri">http://dx.plos.org/10.1371/journal.pone.0028766</a>.</p>
</div>
<div id="ref-Cocco2017">
<p>94. Cocco, S., Feinauer, C., Figliuzzi, M., Monasson, R., and Weigt, M. (2017). Inverse Statistical Physics of Protein Sequences: A Key Issues Review. arXiv. Available at: <a href="https://arxiv.org/pdf/1703.01222.pdf" class="uri">https://arxiv.org/pdf/1703.01222.pdf</a>.</p>
</div>
<div id="ref-Koller2009">
<p>95. Koller, D., and Friedman, N.I.R. (2009). Probabilistic graphical models: Principles and Techniques (MIT Press).</p>
</div>
<div id="ref-Stein2015a">
<p>97. Stein, R.R., Marks, D.S., and Sander, C. (2015). Inferring Pairwise Interactions from Biological Data Using Maximum-Entropy Probability Models. PLOS Comput. Biol. <em>11</em>, e1004182. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4520494{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract" class="uri">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4520494{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract</a>.</p>
</div>
<div id="ref-Seemayer2014">
<p>98. Seemayer, S., Gruber, M., and Söding, J. (2014). CCMpred-fast and precise prediction of protein residue-residue contacts from correlated mutations. Bioinformatics, btu500. Available at: <a href="http://bioinformatics.oxfordjournals.org/content/early/2014/08/12/bioinformatics.btu500" class="uri">http://bioinformatics.oxfordjournals.org/content/early/2014/08/12/bioinformatics.btu500</a>.</p>
</div>
<div id="ref-Kamisetty2013">
<p>100. Kamisetty, H., Ovchinnikov, S., and Baker, D. (2013). Assessing the utility of coevolution-based residue-residue contact predictions in a sequence- and structure-rich era. Proc. Natl. Acad. Sci. U. S. A. <em>110</em>, 15674–9. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3785744{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract" class="uri">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3785744{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract</a>.</p>
</div>
<div id="ref-Lapedes1999">
<p>59. Lapedes, A., Giraud, B., Liu, L., and Stormo, G. (1999). Correlated mutations in models of protein sequences: phylogenetic and structural effects. <em>33</em>, 236–256. Available at: <a href="http://www.citeulike.org/user/qluo/article/5092214" class="uri">http://www.citeulike.org/user/qluo/article/5092214</a>.</p>
</div>
<div id="ref-Lapedes2012a">
<p>101. Lapedes, A., Giraud, B., and Jarzynski, C. (2012). Using Sequence Alignments to Predict Protein Structure and Stability With High Accuracy. Available at: <a href="http://arxiv.org/abs/1207.2484" class="uri">http://arxiv.org/abs/1207.2484</a>.</p>
</div>
<div id="ref-Balakrishnan2011">
<p>102. Balakrishnan, S., Kamisetty, H., Carbonell, J.G., Lee, S.-I., and Langmead, C.J. (2011). Learning generative models for protein fold families. Proteins <em>79</em>, 1061–78. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/21268112" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/21268112</a>.</p>
</div>
<div id="ref-Ekeberg2013">
<p>96. Ekeberg, M., Lövkvist, C., Lan, Y., Weigt, M., and Aurell, E. (2013). Improved contact prediction in proteins: Using pseudolikelihoods to infer Potts models. Phys. Rev. E <em>87</em>, 012707. Available at: <a href="http://link.aps.org/doi/10.1103/PhysRevE.87.012707" class="uri">http://link.aps.org/doi/10.1103/PhysRevE.87.012707</a>.</p>
</div>
<div id="ref-Jones2012">
<p>63. Jones, D.T., Buchan, D.W.A., Cozzetto, D., and Pontil, M. (2012). PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments. Bioinformatics <em>28</em>, 184–90. Available at: <a href="http://bioinformatics.oxfordjournals.org/content/28/2/184.full" class="uri">http://bioinformatics.oxfordjournals.org/content/28/2/184.full</a>.</p>
</div>
<div id="ref-Friedman2008">
<p>103. Friedman, J., Hastie, T., and Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics <em>9</em>, 432–41. Available at: <a href="http://biostatistics.oxfordjournals.org/content/9/3/432.abstract" class="uri">http://biostatistics.oxfordjournals.org/content/9/3/432.abstract</a>.</p>
</div>
<div id="ref-Banerjee2008">
<p>104. Banerjee, O., El Ghaoui, L., and D’Aspremont, A. (2008). Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data. J. Mach. Learn. Res. <em>9</em>, 485–516. Available at: <a href="http://dl.acm.org/citation.cfm?id=1390681.1390696" class="uri">http://dl.acm.org/citation.cfm?id=1390681.1390696</a>.</p>
</div>
<div id="ref-Baldassi2014">
<p>105. Baldassi, C., Zamparo, M., Feinauer, C., Procaccini, A., Zecchina, R., Weigt, M., and Pagnani, A. (2014). Fast and accurate multivariate gaussian modeling of protein families: predicting residue contacts and protein-interaction partners. PLoS One <em>9</em>, e92721. Available at: <a href="http://dx.plos.org/10.1371/journal.pone.0092721" class="uri">http://dx.plos.org/10.1371/journal.pone.0092721</a>.</p>
</div>
<div id="ref-Ekeberg2014">
<p>99. Ekeberg, M., Hartonen, T., and Aurell, E. (2014). Fast pseudolikelihood maximization for direct-coupling analysis of protein structure from many homologous amino-acid sequences. J. Comput. Phys. <em>276</em>, 341–356. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S0021999114005178" class="uri">http://www.sciencedirect.com/science/article/pii/S0021999114005178</a>.</p>
</div>
<div id="ref-Besag1975">
<p>106. Besag, J. (1975). Statistical Analysis of Non-Lattice Data. Source Stat. <em>24</em>, 179–195. Available at: <a href="http://www.jstor.org http://www.jstor.org/stable/2987782" class="uri">http://www.jstor.org http://www.jstor.org/stable/2987782</a>.</p>
</div>
<div id="ref-Gidas1988">
<p>107. Gidas, B. (1988). Consistency of maximum likelihood and pseudo-likelihood estimators for Gibbs Distributions. Stoch. Differ. Syst. Stoch. Control Theory Appl. Available at: <a href="http://www.researchgate.net/publication/244456377{\_}Consistency{\_}of{\_}maximum{\_}likelihood{\_}and{\_}pseudo-likelihood{\_}estimators{\_}for{\_}Gibbs{\_}Distributions" class="uri">http://www.researchgate.net/publication/244456377{\_}Consistency{\_}of{\_}maximum{\_}likelihood{\_}and{\_}pseudo-likelihood{\_}estimators{\_}for{\_}Gibbs{\_}Distributions</a>.</p>
</div>
<div id="ref-Feinauer2014">
<p>108. Feinauer, C., Skwark, M.J., Pagnani, A., and Aurell, E. (2014). Improving contact prediction along three dimensions. 19. Available at: <a href="http://arxiv.org/abs/1403.0379" class="uri">http://arxiv.org/abs/1403.0379</a>.</p>
</div>
<div id="ref-Dunn2008">
<p>56. Dunn, S.D., Wahl, L.M., and Gloor, G.B. (2008). Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction. Bioinformatics <em>24</em>, 333–40. Available at: <a href="http://bioinformatics.oxfordjournals.org/content/24/3/333" class="uri">http://bioinformatics.oxfordjournals.org/content/24/3/333</a>.</p>
</div>
<div id="ref-Zhang2016">
<p>109. Zhang, H., Huang, Q., Bei, Z., Wei, Y., and Floudas, C.A. (2016). COMSAT: Residue contact prediction of transmembrane proteins based on support vector machines and mixed integer linear programming. Proteins Struct. Funct. Bioinforma., n/a–n/a. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/26756402" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/26756402</a>.</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://contactpredictionthesis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="meta-predictors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="application-contact-prediction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/susannvorberg/phd_thesis/edit/master/04-intro-contact-prediction.Rmd",
"text": "Edit"
},
"download": ["PhD_thesis_Susann_Vorberg.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
