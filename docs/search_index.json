[
["index.html", "PhD thesis: residue-residue contact prediction Summary", " PhD thesis: residue-residue contact prediction Susann Vorberg 2017-07-21 Summary Awesome contact prediction project abstract "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements I thank the world. "],
["introduction.html", "1 Introduction", " 1 Introduction In his Nobel Prize speech in 1973 [1] Anfinsen postulated one of the basic principles in molecular biology, which is known as Anfinsen’s dogma: a protein’s native structure is uniquely determined by its amino acid sequence. With certain exceptions (e.g. IDP [2]), this dogma has proven to hold true for the majority of proteins. Ever since, it is regarded as the biggest challenge in structural bioinformatics [3], to realiably predict a protein’s structure given only its amino acid sequence. De-novo protein structure prediction methods use physical or knowledge based energy potentials to find a protein conformation that minimizes the protein’s energy landscape. However, these methods are limited by the complexity of the conformational space and the accuracy of the energy potentials. Considering a protein with 150 amino acids, that has approximately 450 degrees of freedom, Regarding the rotational and translational degrees of freedom of the protein chain, the complexity scales with XXX [1]. Far more successfull are template-based modelling approaches. Given the observation that structure is more conserved than sequence in a protein family [4], the structure of a target protein can be inferred from a homologue protein [5]. The degree of structural conservation is linked to the level of pairwise sequence identity [6]. Therefore, the accuracy of a model crucially depends on the sequence identity between target and template and determines the applicability of the model [7]. By definition, homology derived models are unable to capture new folds [8] and their main limitation lies in the availability of suitable templates. Figure 1.1: Yearly growth of number of solved structures in the PDB[9] and protein sequences in the Uniprot[10]. Unfortunately, the number of solved protein structures increases only slowly, as experimental methods are both time consuming and expensive [8]. The PDB[9] is the main repository for marcomolecular structures and currently (Jul 2017) holds about 120 000 atomic models of proteins. The primary technique for determining protein structures is X-ray crystallography, accounting for roughly 90% of entries in the PDB. About 9% of protein structures have been solved using NMR and less than 1% using EM (see FIG 1). All three experimental techniques have advantages and limitations with respect to certain modelling aspects. X-ray crystallography requires the protein to form crystals, which is an arduous and sometimes impossible task. Furthermore, crystal packing forces the protein into a unnatural and rigid environment preventing the observation of conformational flexibility. \\(\\ac{NMR}\\) studies the protein in an physiological environment in solution and enables the study of protein dynamics as ensembles of protein structures can be observed. However, \\(\\ac{NMR}\\) is limited to look at small proteins. Recently, \\(\\ac{EM}\\) has undergone a “resolution revolution” [11] and macromolecular structures have been solved with resolutions up to 2A[citation]. The limit of \\(\\ac{cryo-EM}\\) lies in the size of proteins. Compared to the tedious task of revealing atomic resolution of a protein tertiary structure, it has become very easy to decipher the primary sequence of proteins. With the latest sequencing technologies [examples], it takes only hours to sequence millions of basepaires at low costs [example numbers] and the number of sequenced genomes has risen tremendously. The UniProtKB [10], the leading resource for protein sequences, contains more than 80 million sequence entries (24 July 2017). Consequently, the gap between the number of protein structures and sequences is still growing and even new developments as single protein structure determination [???] are not expected to close this gap near in time. [Figure sequence structure gap] Protein structure determines protein function. Therefore, structural insights are of uttermost importance. They are essential for a detailed understanding of chemical reactions, regulatory processes and transport mechanisms. They are fundamental for the design of drugs and antibiotics. Moreover structural abnormalities can lead to misfolding and aggregation potentially causing diseases so studying them is pathologically relevant. The aformentioned trends illustrate the need of computational methods and motivate research to solve Ansinsens Dogma to reliably predict protein structures from sequence alone. References "],
["structure-prediction.html", "1.1 Structure Prediction", " 1.1 Structure Prediction Despite the knowledge of Anfinsen’s postulate, we are not able to reliably predict the structure of a protein from its sequence alone. Generally it is assumed that a protein folds into a unique, well-defined native structure that is near the global free energy minimum (). Levinthal’s paradox [12] describes the complexity of the folding process towards this minimum. It stresses the problem that it is not possible for a protein to exhaustively search the conformational space to get to its native fold. Due to the “combinatorial explosion” of possible conformations, an exhaustive search would take unreasonably long. Hence, it is not a feasible approach for structure prediction to scan all possible conformations. Different approaches have been developed over time to overcome or elude this problem. 1.1.1 Template-based methods Homology modeling is by far the most successful approach to structure prediction. The basic concept of this strategy relates to the fact that structure is more conserved than sequence [4]. After detecting a homologous protein of known structure, that has sufficient sequence similarity, it can be used as a template to model the structure of the target protein. The degree of structural conservation is linked to the level of pariwise sequence identity [6]. Homology Modelling is assumed to yield reliably accurate models when query and target protein share more than 30% sequence similarity, depending on the sequence length (safe homology zone) [5]. Below a threshold of ~20-35% pairwise sequence identity (twighlight-zone) the number of false positives regarding structural similarity explodes and structural inference becomes less reliable and more than 95% of structures are dissmilar [13]. Advances in remote homology detection and alignment generation have improved the quality of models, even beyond the once postulated limit of the twighlight-zone [14]. Integration of multiple templates has also proved to increase model quality [15] After the identification of a suitable template, there are different strategies that can be followed to obtain a model for the target protein. The the backbone of the model is generated by simply copying the coordinates of the target backbone atoms onto the model. Non-aligned residues due to gaps in the alignment have to be modelled , meaning from scratch. This can be done by a knowledge-based search for suitable fragments in the PDB or by true energy-based modelling. When the backbone is generated, the side chains are modelled, usually by searching rotamer libraries for energetically favoured residue conformations. Finally, the model is energetically optimized in an iterative procedure. Force fields are applied to correct the backbone and side chain conformations [[16]}. Several automated pipelines for homology modelling are well-established (Modeller [[17]}, 3D-Jigsaw [[18]}, SwissModel [[19]}) which allow more or less manual intervention in the modelling process. Fold Recognition describes the inverse folding problem : instead of finding the compatible structure for a given sequence, one tries to find sequences that fit onto a given structure. Whether the query sequence fits a structure from the database is not determined by sequence similarities but rather energetic or environment specific measures. Thus, fold recognition methods are able to recognize structural similarity even in the absence of sequence similarity. The rationale basis for this strategy is the assumption that the fold space is limited. It has been found that seemingly unrelated proteins often adopt similar folds. This might be due to divergent evolution (proteins are related, but homology cannot be detected at the corresponding sequence level) or convergent evolution (functional requirements lead to similar folds for unrelated proteins) . Early approaches include profile based methods. Here, the structural information of the protein is encoded into profiles, which subsequently are aligned to the sequences . Advanced techniques are known as “threading” techniques, describing the process of threading a sequence through a structure and determining the optimal fit via energy functions. 1.1.2 Template-free structure prediction Ab initio or de-novo modeling techniques implement Anfinsen’s Dogma most closely in mimicking the folding process based only on physico-chemical principles. Energy functions (physical or knowledge-based) are used to describe the folding landscape and are minimized to arrive at the global energy minimum corresponding to the native conformation. Since the native conformation can be found near the global energy minimum of the folding landscape, energy functions (physical or knowledge-based) have been developed to describe this landscape. With respect to the idea of a folding funnel, the energy function is minimized to mimic the folding process that automatically leads to the global minimum. Again, there exist numerous webservers that combine energy minimization, threading techniques and fragment-based approaches, e.g. Rosetta , Tasser , Touchstone II . Drawbacks of these methods are the time requirements due to the computational complexity of energy functions as well as their inaccuracy. Minimize a physical or knowledge-based energy function for the protein. This has huge complexity due to large conformational space that needs to be sampled. 1.1.3 contact assisted denovo predictions Structure Reconstruction from true contacts maps works well. Even a small number of contacts is sufficient to reconstruct the fold of the protein. Distance maps work even better. What is the optimal distance cutoff to define a contact? Duarte et al 2010: between 8 and 12A Dyrka et al 2016 Konopka et al 2014 Sathyapriya et al 2009 Many studies that successfuly predict structures denovo with the help of predicted contact. References "],
["contact-prediction.html", "1.2 Contact Prediction", " 1.2 Contact Prediction 1.2.1 Correlated mutations contact prediction methods aim to identify correlated mutations from an alignment of homologue protein sequences. main assumption is that two interacting amino acid residues are coevolving: mutation of one of the two residues can be compensated by mutation of the other residue 1.2.2 Benchmarking methods threshold for defining a contact: usually distance between \\(C_\\beta\\) atoms (\\(C_\\alpha\\) for Glycin) &lt; 8 angstrom. PPV: TP/(FP+TP) fraction of correct predictions among all predictions 1.2.3 Pitfalls Coevolution of residues can be mediated by molecules (e.g zinc ions) and will not always imply spatial proximity in structure. Transitivity can lead to correlation signals. Phylogenetic bias can also lead to correlations. Sampling bias needs to be taken into account. See sequence reweighting strategies in methods section "],
["state-of-the-art-cp.html", "1.3 State of the Art CP", " 1.3 State of the Art CP 1.3.1 Correlation vs Causation A recurring difficulty when dealing with interacting systems is distinguishing the direct interactions, resulting from adjacent interplay between two units, from interactions mediated via multi-step paths across other elements [Ekeberg master thesis] 1.3.2 Local methods MI and correlation measures suffer from transitivity of correlations 1.3.3 Maximum entropy models for modelling protein families The principle of maximum entropy was postulated by Jaynes in 1957 [20] [21], stating that the probability distribution which best represents observed data is the one that is in agreement with measured constraints and has the largest entropy. It is a distribution that makes minimal assumptions and is the least biased estimate of a distribution given the measured constraints. Applied to the problem of modelling protein families, one would seek a probability distribution over protein sequences \\(\\seq = (x_i, ..., x_L)\\) of length \\(L\\) reproducing the empirical single \\(\\mathcal{f}(x_i \\eq a)\\) and pairwise \\(\\mathcal{f}(x_i \\eq a, x_j \\eq b)\\) amino acid frequencies of an alignment \\(\\mathbf{X}\\) with \\(N\\) sequences: \\[\\begin{equation} p(x_i\\eq a) = \\mathcal{f}(x_i\\eq a) = \\frac{1}{N}\\sum_{n=1}^N I(x_i^{(n)} \\eq a) \\\\ p(x_i\\eq a, x_j \\eq b) = \\mathcal{f}(x_i\\eq a, x_j\\eq b) = \\frac{1}{N} \\sum_{n=1}^N I(x_i^{(n)} \\eq a, x_j^{(n)} \\eq b) \\tag{1.1} \\end{equation}\\] Maximizing the entropy \\(S= -\\sum_{\\seq} p(\\seq) \\log p(\\seq)\\) of the distribution under the given constraints (1.1) using Lagrange multipliers results in the formulation of an exponential model known as Potts model or its generalized form a Markov Random Field: \\[\\begin{equation} p(\\seq) = \\frac{1}{Z} \\exp \\left( \\sum_{i=1}^L v_i(x_i) \\sum_{\\substack{i,j=1 \\\\ i \\neq j}}^L w_{ij}(x_i, x_j) \\right) \\tag{1.2} \\end{equation}\\] \\(Z\\) is a normalization constant also known as partition function ensuring the total probabilty adds up to one by summing over all possible assignments to \\(\\seq\\). \\[\\begin{equation} Z = \\sum_{\\seq&#39; \\in [1,...,20]^L} p(\\seq&#39; | \\v, \\w) \\tag{1.3} \\end{equation}\\] 1.3.3.1 Interpretation of model parameters Markov Random Field belongs to class of undirected graphical models. Therefore they can be represented as a graph with nodes corresponding to positions in the alignment and edges describing the dependency structure between nodes. A protein sequence can be encoded in multiple ways, altering the resultign model. Using a binary encoding for amino acids \\(x_i \\in [0, 1]^{21}\\), a sequence \\(x\\) can be interpreted as continous random variable. For continous, real-valued variables x, the maximum entropy model can be represented as a graphical Gaussian model. This interpretation of the model is used as the basis for PSICOV. Otherwise sequences are represented as multinomial variables with 21 states (20 amino acids and one gap). The single-emission potentials \\(v_i(a)\\) correspond to the single amino acid frequencies whereas the pairwise-emission potentials \\(w_{ij}(a,b)\\) also called couplings reflect the tendency of an amino acid a at position i to co-occur with an amino acid b at position j in the alignment. Can disentangle direct and indirect correlations. Infer parameters of a maximum entropy model, more specifically a Potts model (statistical physics) aka markov random fiels (computer science). Likelihood function is convex, but Maximum Likelihood inference of model is infeasible: Likelihood function needs to be reevaluated at each iteration during optimization but partition function term sums over 20^L sequences. Many approximations: - mean field (Marks), Psicov - pseudo-likelihood - belief-propagation (accurate but slow) 1.3.3.2 Infering model parameters Typically, one would obtain parameter estimates by maximizing the log-likelihood of observed data. However, given the 20 amino acids and typical proteins of length \\(L=100\\), the partition function \\(Z\\) has to sum over \\(20^100\\) sequences. Because of this exponential complexityit is computationally intractable to evaluate the likelihood-function at every iteration of an optimization procedure. Several approaches have been developed to evade the infeasible computation of the partition function and obtain approximate solutions for the model parameters. Some of these approaches have successfully been adapted to the problem of modelling protein families. 1.3.3.3 Pseudo-Likelihood Instead of optimizing the likelihood, Besag suggested in 1975 to rather optimize a function that he termed pseudo-likelihood that replaces the joint probability with the product over conditionals [22]: \\[\\begin{equation} p(\\seq | \\v,\\w) = \\prod_{i=1}^L p(x_i | \\seq_{/xi}\\v,\\w) = \\prod_{i=1}^L \\left( \\frac{1}{Z_i} \\exp[v_i(x_i)] \\prod_{\\substack{j=1 \\\\ j \\neq i}}^L \\exp[w_{ij}(x_i, x_j)] \\right) \\end{equation}\\] Here, the normalization term \\(Z_i\\) sums only over all assignments to one position \\(i\\) in sequence: \\[\\begin{equation} Z_i = \\sum_{a=1}^{20} \\exp[v_i(a)] \\prod_{\\substack{j=1 \\\\ j \\neq i}}^L \\exp[w_{ij}(a, x_j)] \\end{equation}\\] Besag showed that pseudo-likelihood is an consistent estimator for the likelihood in the limit of large data. the proba- bility of them-th observation, xm, is approximated by the product of the conditional probabilities[23] Recall from above that our pseudo-likelihood uses the full representation and fixes the gauge by the regularization terms Rl2. Our procedure is therefore first to infer the interaction parameters using using the pseudo-likelihood and the regularization, and then change to the zero-sum gauge [24] On one hand, this improvement might not be surpris-ing: it is known that, for very large data sets, pseudo- likelihood maximization becomes asymptotically equiva- lent to full maximum-likelihood inference, whereas mean- field inference remains intrinsically approximate, and this may result in an improved PLM performance. On the other hand, the above advantage holds if and only if the following two conditions are fulfilled: data a drawn independently from a probability distribution, and this probability distribution is the the Boltzmann distribution of a Potts model. None of these two con- ditions actually hold for real protein sequences [24] Disregarding the improvements, we find that overall the predicted contact pairs for plmDCA and mfDCA are highly overlapping, illustrating the robustness of DCA results with respect to the algorithmic implementation[24] The pseudo-likelihood inherits the concavity[23] 1.3.4 Computing contact map from coupling matrix direct information frobenius norm average product correction (also for MI) (benchmark plot for localmethods + ccmpred) 1.3.5 Meta-predictors combining different approaches jones et al: overlap between methods but also many unique predictions machine learning methods incorporate sequence-derived features: secondary structure predictions solvent accessibilty contact potentials msa properties pssms physico-chemcial properties of amino acids However, Meta-predictors will improve if basic methods improve. Ultra-deep learning paper identifies coevolution features as crucial feature. --> References "],
["results.html", "2 Results ", " 2 Results "],
["optimizing-full-likelihood.html", "2.1 Optimizing Full-Likelihood", " 2.1 Optimizing Full-Likelihood The full likelihood of the maximum entropy model cannot be optimized with ML methods due to the exponential complexity of the partition function (see section ??). As elaborated in the introduction, many approximations to maximum likelihood inference have been developed that resolve the computational intractability of the partition function. Pseudo-likelihood methods are now the state-of-the-art model for contact prediction that outperformed other approximations like mean-field methods or methods based on the Bethe-approximation or sparse inverse covariance. Even though pseudo-likelihood has been shown to be a consistent estimator of the true likelihood estimates in the limit of large datasets [22], it is not clear how well pseudo-likelihood approximation is for small data. 2.1.1 Likelihood Gradient 2.1.2 Contrastive Divergence CD is about the difference between the original data set and a perturbed data set perturbed data set : The contrasting data set needs to represent A data sample characteristic of the current PARAMETERS –&gt; Gibbs Sampling starting from data Note: as contrasting dataset towards true_parameters, the elements of the gradient converge to the gradient of the max log likelihood – At the limit of the Markov chain, the CD converges to the actual MLE --> References "],
["methods.html", "3 Methods", " 3 Methods all you need to know "],
["dataset.html", "3.1 Dataset", " 3.1 Dataset A protein dataset has been constructed from the CATH (v4.1) [26] database for classification of protein domains. All CATH domains from classes 1(mainly \\(\\alpha\\)), 2(mainly \\(\\beta\\)), 3(\\(\\alpha+\\beta\\)) have been selected and filtered for internal redundancy at the sequence level using the pdbfilter script from the HH-suite[27] with an E-value cutoff=0.1. The dataset has been split into ten subsets aiming at the best possible balance between CATH classes 1,2,3 in the subsets. All domains from a given CATH topology (=fold) go into the same subsets, so that any two subsets are non-redundant at the fold level. Some overrepresented folds (e.g. Rossman Fold) have been subsampled to ensure that every subset contains at max 30% domains of the same fold. In total there are 6741 domains in the dataset. Multiple sequence alignments were built from the CATH domain sequences (COMBS) using HHblits [27] with parameters to maximize the detection of homologous sequences: hhblits -maxfilt 100000 -realign_max 100000 -B 100000 -Z 100000 -n 5 -e 0.1 -all hhfilter -id 90 -neff 15 -qsc -30 The COMBS sequences are derived from the SEQRES records of the PDB file and sometimes contain extra residues that are not resolved in the structure. Therefore, residues in PDB files have been renumbered to match the COMBS sequences. The process of renumbering residues in PDB files yielded ambigious solutions for 293 proteins, that were removed from the dataset. Another filtering step was applied to remove 80 proteins that do not hold the following properties: more than 10 sequences in the multiple sequence alignment (\\(N&gt;10\\)) protein length between 30 and 600 residues (\\(30 \\leq L \\leq 600\\)) less than 80% gaps in the multiple sequence alignment (percent gaps &lt; 0.8) at least one residue-pair in contact at \\(C_\\beta &lt; 8\\AA\\) and minimum sequence separation of 6 positions The final dataset is comprised of 6368 proteins with almost evenly distributed CATH classes over the ten subsets (Figure 3.1). Figure 3.1: Distribution of CATH topologies in the ten datasets. References "],
["contact-prediction-1.html", "3.2 Contact Prediction", " 3.2 Contact Prediction Dr Stefan Seemayer wrote a new implementation of the open-source software CCMpred [28] in Python. His code forms the basis of all extensions that have been developed in this thesis. 3.2.1 Sequence Reweighting Multiple sequence alignments do not represent iid samples of the sequence space of a protein family. In fact, there is selection bias from sequencing species of special interest (e.g human pathogens) or sequencing closely related species, e.g multiple strains. This uneven sampling of sequence space thus leaves certain regions unexplored whereas others are statistically overrepresented. To reduce the effects of overrepresented sequences, typically a simple weighting strategy [23] is applied that assigns a weight to each sequence that is the inverse of the number of similar sequences according to an identity threshold. It has been found that reweighting improves contact prediction performance but results are robust against the choice of the identity threshold in a range between 0.7 and 0.9 [29]. We chose an identity threshold of 0.8. Every sequence \\(x_n\\) of length \\(L\\) in an alignment with \\(N\\) sequences has an associated weight \\(w_n = 1/m_n\\), where \\(m_n\\) represents the number of similar sequences: \\[\\begin{equation} w_n = \\frac{1}{m_n}, m_n = \\sum_{m=1}^N I \\left( ID(x_n, x_m) \\geq 0.8 \\right) \\\\ ID(x_n, x_m)=\\frac{1}{L} \\sum_{i=1}^L I(x_n^i = x_m^i) \\tag{3.1} \\end{equation}\\] The number of effective sequences \\(\\mathbf{\\neff}\\) of an alignment is then the number of sequence clusters computed as: \\[\\begin{equation} \\neff = \\sum_{n=1}^N w_n \\tag{3.2} \\end{equation}\\] 3.2.2 Computing Amino Acid Frequencies Single and pairwise amino acid frequencies are computed from the alignment by taking into account the weights of sequences (section 3.2.1) and adding pseudocounts for numerical stability. Let \\(a,b \\in \\{1,\\ldots,20\\}\\) be amino acids, \\(q(x_i=a), q(x_i=a, x_j=b)\\) and \\(q_0(x_i=a), q_0(x_i=a,x_j=b)\\) be the empirical single and pair frequencies with and without pseudocounts, respectively. We define \\[\\begin{align} q(x_i \\eq a) :=&amp; (1-\\tau) \\; q_0(x_i \\eq a) + \\tau \\tilde{q}(x_i\\eq a) \\\\ q(x_i \\eq a, x_j \\eq b) :=&amp; (1-\\tau)^2 \\; [ q_0(x_i \\eq a, x_j \\eq b) - q_0(x_i \\eq a) q_0(x_j \\eq b) ] + \\\\ &amp; q(x_i \\eq a) \\; q(x_j \\eq b) \\tag{3.3} \\end{align}\\] with \\(\\tilde{q}(x_i \\eq a) := f(a)\\) being background amino acid frequencies and \\(\\tau \\in [0,1]\\) is a pseudocount admixture coefficient, which is a function of the diversity of the multiple sequence alignment: \\[\\begin{equation} \\tau = \\frac{N_\\mathrm{pc}}{(N_\\mathrm{eff} + N_\\mathrm{pc})} \\tag{3.4} \\end{equation}\\] where \\(N_{pc} &gt; 0\\). The formula for \\(q(x_i \\eq a, x_j \\eq b)\\) in the second line in eq (3.3) was chosen such that for \\(\\tau \\eq0\\) we obtain \\(q(x_i \\eq a, x_j \\eq b) = q_0(x_i \\eq a, x_j \\eq b)\\), and furthermore \\(q(x_i \\eq a, x_j \\eq b) = q(x_i \\eq a) q(x_j \\eq b)\\) exactly if \\(q_0(x_i \\eq a, x_j \\eq b) = q_0(x_i \\eq a) q_0(x_j \\eq b)\\). 3.2.3 Pseudolikelihood 3.2.4 Full-likelihood Dr Stefan Seemayer provided a Python implementation of CCMpred that was extended to optimize the full-likelihood of the MRF. References "],
["abbrev.html", "A Abbreviations", " A Abbreviations APC Avarage Product Correction CASP Critical Assessment of protein Structure Prediction DCA Direct Coupling Analysis DI Direct Information EM electron microscopy IDP intrinsically disordered proteins ML Maximum-Likelihood MRF Markov-Random Field PDB protein data bank "],
["dataset-properties.html", "B Dataset Properties", " B Dataset Properties Figure B.1: Diversity of alignments in the ten datasets. Figure B.2: Percentage of gaps in alignments in the ten datasets. Figure B.3: Distribution of alignment size in the ten datasets. Figure B.4: Distribution of protein length in the ten datasets. "],
["references.html", "References", " References "]
]
