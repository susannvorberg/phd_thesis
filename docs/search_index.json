[
["index.html", "PhD thesis: residue-residue contact prediction Summary", " PhD thesis: residue-residue contact prediction Susann Vorberg 2017-08-20 Summary Awesome contact prediction project abstract "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements I thank the world. "],
["introduction.html", "1 Introduction", " 1 Introduction In his Nobel Prize speech in 1973 [1] Anfinsen postulated one of the basic principles in molecular biology, which is known as Anfinsen’s dogma: a protein’s native structure is uniquely determined by its amino acid sequence. With certain exceptions (e.g. IDP [2]), this dogma has proven to hold true for the majority of proteins. Ever since, it is regarded as the biggest challenge in structural bioinformatics [3], to realiably predict a protein’s structure given only its amino acid sequence. De-novo protein structure prediction methods use physical or knowledge based energy potentials to find a protein conformation that minimizes the protein’s energy landscape. However, these methods are limited by the complexity of the conformational space and the accuracy of the energy potentials. Considering a protein with 150 amino acids, that has approximately 450 degrees of freedom, Regarding the rotational and translational degrees of freedom of the protein chain, the complexity scales with XXX [1]. Far more successfull are template-based modelling approaches. Given the observation that structure is more conserved than sequence in a protein family [4], the structure of a target protein can be inferred from a homologue protein [5]. The degree of structural conservation is linked to the level of pairwise sequence identity [6]. Therefore, the accuracy of a model crucially depends on the sequence identity between target and template and determines the applicability of the model [7]. By definition, homology derived models are unable to capture new folds [8] and their main limitation lies in the availability of suitable templates. Figure 1.1: Yearly growth of number of solved structures in the PDB[9] and protein sequences in the Uniprot[10]. Unfortunately, the number of solved protein structures increases only slowly, as experimental methods are both time consuming and expensive [8]. The PDB[9] is the main repository for marcomolecular structures and currently (Jul 2017) holds about 120 000 atomic models of proteins. The primary technique for determining protein structures is X-ray crystallography, accounting for roughly 90% of entries in the PDB. About 9% of protein structures have been solved using NMR and less than 1% using EM (see FIG 1). All three experimental techniques have advantages and limitations with respect to certain modelling aspects. X-ray crystallography requires the protein to form crystals, which is an arduous and sometimes impossible task. Furthermore, crystal packing forces the protein into a unnatural and rigid environment preventing the observation of conformational flexibility. \\(\\ac{NMR}\\) studies the protein in an physiological environment in solution and enables the study of protein dynamics as ensembles of protein structures can be observed. However, \\(\\ac{NMR}\\) is limited to look at small proteins. Recently, \\(\\ac{EM}\\) has undergone a “resolution revolution” [11] and macromolecular structures have been solved with resolutions up to 2A[citation]. The limit of \\(\\ac{cryo-EM}\\) lies in the size of proteins. Compared to the tedious task of revealing atomic resolution of a protein tertiary structure, it has become very easy to decipher the primary sequence of proteins. With the latest sequencing technologies [examples], it takes only hours to sequence millions of basepaires at low costs [example numbers] and the number of sequenced genomes has risen tremendously. The UniProtKB [10], the leading resource for protein sequences, contains more than 80 million sequence entries (24 July 2017). Consequently, the gap between the number of protein structures and sequences is still growing and even new developments as single protein structure determination [???] are not expected to close this gap near in time. [Figure sequence structure gap] Protein structure determines protein function. Therefore, structural insights are of uttermost importance. They are essential for a detailed understanding of chemical reactions, regulatory processes and transport mechanisms. They are fundamental for the design of drugs and antibiotics. Moreover structural abnormalities can lead to misfolding and aggregation potentially causing diseases so studying them is pathologically relevant. The aformentioned trends illustrate the need of computational methods and motivate research to solve Ansinsens Dogma to reliably predict protein structures from sequence alone. References "],
["protein-structure.html", "1.1 Protein Structure", " 1.1 Protein Structure Primary: Amino Acid Sewuence Secondary: Helices, sheets, coils, repeats,.. tertiary: interaction of secondary structure elementws quartary: interaction of domains 1.1.1 Amino Acid Interactions The Venn diagram in figure 1.2 displays a typical classification of amino acids with respect to their physico-chemical properties. The aromatic amino acids tryptophan (W), tyrosine (Y), phenylalanine (F), and histidine (H) contain an aromatic ring system. Generally, aromatic ring systems are planar, and electons are shared over the whole ring structure. Interactions between aromatic residues have very constrained geometries regarding the angle between the centroid of their rings. The \\(\\pi\\)-electron systems favour T-shaped or offset stacked conformations [12]. Preferred distances between aromatic residues have been observed between 4.5and 7of their ring centroids [13]. Cysteine (C) residues can form disulphide bonds, which are the only covalent bonds between two amino acid side chains. They comprise the strongest side chain interactions in protein structures and their length varies between 3.5to 4. Disulphide bonds also have a well defined geometry: there are five dihedral angles in a disulphide bond resulting in 20 different possible configurations. Only one configuration is favoured so that the dihedral angle between the carbon and sulfur atoms is close to 90 degrees [14]. They play a very important role in stabilizing protein structures. The number of disulfide bonds is negatively correlated with protein length: smaller proteins have more disulfide bonds helping to stabilize the structure in absence of strong hydrophobic packing in the core. It has also been found that disulfide bonds are more frequently observed in proteins of hyperthermophilic bacteria, being positively selected for increased stability [15]. Salt bridges are based on electrostatic interactions between positively charged residues (arginine (R) and lysine (K)) and negatively charged residues (aspartic acid (D) and glutamic acid (E)). The strength of electrostatic interactions, as described by Coulomb’s law, decreases with distance between the point charges at the functional groups. It has been found to be maximal at 4with respect to the functional groups of the both residues [16]. Hydrogen bonds can be formed between a donor residue which possesses an hydrogen atom attached to a strongly electronegative atom and an acceptor residue which possesses an electronegative atom with a lone electron pair. They are electrostatic interactions as well and thus their strength depends on distance as well. Hydrogen bonds are formed at distances of 2.4to 3.5between the non-hydrogen atoms (Berg JM, Tymoczko JL, 2002). Salt bridges as well as hydrogen bonds have strong geometric preferences (Kumar and Nussinov, 1999). The geometry of a hydrogen bond depends on the angle between the HB donor, the hydrogen atom and the HB acceptor (Torshin et al., 2002). Cation–\\(\\pi\\) interactions are formed between positively charged or partially charged amino acids with amino groups (K,R,Q,E) and aromatic residues (W,Y,F,H). The preferential distance of the amino group to the \\(\\pi\\)-electron system has been determined between 3.4and 6[17] [18] Their role in stabilizing protein structures is still under debate [19]. Proline residues are conformationally restricted, with the alpha-amino group of the backbone directly attached to the side chain. The sterical rigidity of the proline side chain restricts the backbone angle and thus affects secondary structure formation. Proline is known as a helix-breaker. Whereas other aromatic side chains are defined by their negatively charged \\(\\pi\\) faces, the face of proline side chains is partially positively charged. Thus, aromatic and proline residues can interact favorably with each other. Once due to the hydrophobic nature of the residues and also due to the interaction between the negatively charged aroamtic \\(\\pi\\) face and the polarized C-H bonds in proline, called a CH/\\(\\pi\\) interaction. Petersen et al. (2012) found clear secondary structure elements preferences for each amino acid pair. For example, residue pairs containing Alanine and Leucine are predominantly found in buried \\(\\alpha\\)-helices, whereas pairs containing Isoleucine and Valine preferentially are located in \\(\\beta\\)-sheet environments. Of course, solvent accessibility represents an important criterion for residue interactions. Hydrophobic residues are rather buried in the structure, whereas polar and charged residues are found more frequently on the protein surface and interact with water molecules. Figure 1.2: Physico-chemical properties of amino acids. The 20 naturally occuring amino acids are grouped with respect to ten physico-chemical properties. Adapted from Figure 1a in [20]. References "],
["structure-prediction.html", "1.2 Structure Prediction", " 1.2 Structure Prediction Despite the knowledge of Anfinsen’s postulate, we are not able to reliably predict the structure of a protein from its sequence alone. Generally it is assumed that a protein folds into a unique, well-defined native structure that is near the global free energy minimum (). Levinthal’s paradox [21] describes the complexity of the folding process towards this minimum. It stresses the problem that it is not possible for a protein to exhaustively search the conformational space to get to its native fold. Due to the “combinatorial explosion” of possible conformations, an exhaustive search would take unreasonably long. Hence, it is not a feasible approach for structure prediction to scan all possible conformations. Different approaches have been developed over time to overcome or elude this problem. 1.2.1 Template-based methods Homology modeling is by far the most successful approach to structure prediction. The basic concept of this strategy relates to the fact that structure is more conserved than sequence [4]. After detecting a homologous protein of known structure, that has sufficient sequence similarity, it can be used as a template to model the structure of the target protein. The degree of structural conservation is linked to the level of pariwise sequence identity [6]. Homology Modelling is assumed to yield reliably accurate models when query and target protein share more than 30% sequence similarity, depending on the sequence length (safe homology zone) [5]. Below a threshold of ~20-35% pairwise sequence identity (twighlight-zone) the number of false positives regarding structural similarity explodes and structural inference becomes less reliable and more than 95% of structures are dissmilar [22]. Advances in remote homology detection and alignment generation have improved the quality of models, even beyond the once postulated limit of the twighlight-zone [23]. Integration of multiple templates has also proved to increase model quality [24] After the identification of a suitable template, there are different strategies that can be followed to obtain a model for the target protein. The the backbone of the model is generated by simply copying the coordinates of the target backbone atoms onto the model. Non-aligned residues due to gaps in the alignment have to be modelled , meaning from scratch. This can be done by a knowledge-based search for suitable fragments in the PDB or by true energy-based modelling. When the backbone is generated, the side chains are modelled, usually by searching rotamer libraries for energetically favoured residue conformations. Finally, the model is energetically optimized in an iterative procedure. Force fields are applied to correct the backbone and side chain conformations [[25]}. Several automated pipelines for homology modelling are well-established (Modeller [[26]}, 3D-Jigsaw [[27]}, SwissModel [[28]}) which allow more or less manual intervention in the modelling process. Fold Recognition describes the inverse folding problem : instead of finding the compatible structure for a given sequence, one tries to find sequences that fit onto a given structure. Whether the query sequence fits a structure from the database is not determined by sequence similarities but rather energetic or environment specific measures. Thus, fold recognition methods are able to recognize structural similarity even in the absence of sequence similarity. The rationale basis for this strategy is the assumption that the fold space is limited. It has been found that seemingly unrelated proteins often adopt similar folds. This might be due to divergent evolution (proteins are related, but homology cannot be detected at the corresponding sequence level) or convergent evolution (functional requirements lead to similar folds for unrelated proteins) . Early approaches include profile based methods. Here, the structural information of the protein is encoded into profiles, which subsequently are aligned to the sequences . Advanced techniques are known as “threading” techniques, describing the process of threading a sequence through a structure and determining the optimal fit via energy functions. 1.2.2 Template-free structure prediction Ab initio or de-novo modeling techniques implement Anfinsen’s Dogma most closely in mimicking the folding process based only on physico-chemical principles. Energy functions (physical or knowledge-based) are used to describe the folding landscape and are minimized to arrive at the global energy minimum corresponding to the native conformation. Since the native conformation can be found near the global energy minimum of the folding landscape, energy functions (physical or knowledge-based) have been developed to describe this landscape. With respect to the idea of a folding funnel, the energy function is minimized to mimic the folding process that automatically leads to the global minimum. Again, there exist numerous webservers that combine energy minimization, threading techniques and fragment-based approaches, e.g. Rosetta , Tasser , Touchstone II . Drawbacks of these methods are the time requirements due to the computational complexity of energy functions as well as their inaccuracy. Minimize a physical or knowledge-based energy function for the protein. This has huge complexity due to large conformational space that needs to be sampled. 1.2.3 contact assisted denovo predictions Structure Reconstruction from true contacts maps works well. Even a small number of contacts is sufficient to reconstruct the fold of the protein. Distance maps work even better. What is the optimal distance cutoff to define a contact? Duarte et al 2010: between 8 and 12A Dyrka et al 2016 Konopka et al 2014 Sathyapriya et al 2009 Many studies that successfuly predict structures denovo with the help of predicted contact. Vice versa, because contacts at large primary distances are rare, they are most informative for protein structure prediction: Izarzugaza J, Gran ˜a O, Tress M, Valencia A, Clarke N (2007) Assessment of intramolecular contact predictions for CASP7 References "],
["contact-prediction.html", "1.3 Contact Prediction", " 1.3 Contact Prediction 1.3.1 Correlated mutations contact prediction methods aim to identify correlated mutations from an alignment of homologue protein sequences. main assumption is that two interacting amino acid residues are coevolving: mutation of one of the two residues can be compensated by mutation of the other residue 1.3.2 Local methods MI and correlation measures suffer from transitivity of correlations 1.3.3 Global methods In 2010, Burger and Nijmwegen developed a Bayesian network model [29] that is able to disentangle direct from indirect correlations and introducing informative priors to improve precision. independently of weight in 2009 1.3.4 Meta-predictors combining different approaches jones et al: overlap between methods but also many unique predictions machine learning methods incorporate sequence-derived features: secondary structure predictions solvent accessibilty contact potentials msa properties pssms physico-chemcial properties of amino acids However, Meta-predictors will improve if basic methods improve. Ultra-deep learning paper identifies coevolution features as crucial feature. 1.3.5 Benchmarking methods threshold for defining a contact: usually distance between \\(C_\\beta\\) atoms (\\(C_\\alpha\\) for Glycin) &lt; 8 angstrom. PPV: TP/(FP+TP) fraction of correct predictions among all predictions 1.3.5.1 Sequence Separation Residue pairs too close in sequence (e.g \\(|i-j| &lt; 6\\)) are filtered out for benchmarking contact prediction methods. Commonly, sequence separation bins are applied to distuinguish short (\\(6 \\le |i-j| &lt; 12\\)), medium (\\(12 \\le |i-j| &lt; 24\\)) and long range (\\(|i-j| \\ge 24\\)) contacts. Especially long range contacts are of importance for structure prediction as they are informative and able to constrain the overal fold of a protein [???]. In contrast, residue pairs close in sequence (\\(|i-j| &lt; 6\\)) are trivial to predict and often reflect local geometrical constraints from secondary structure elements. Figure 1.3 shows the distribution of \\(\\Cb\\) distances for various minimal sequence separation thresholds. Without filtering local residue pairs (sequence separation 1), there are several additional peaks in the distribution around \\(5.5\\AA\\), \\(7.4\\AA\\) and \\(10.6\\AA\\) that can be attributed to local interactions in e.g. helices (see Figure 1.4). Figure 1.3: Distribution of residue pair \\(\\Cb\\) distances over all proteins in the dataset (see Methods 5.1) at different minimal sequence separation thresholds: blue = \\(|i-j| &gt; 1\\) (all residue pairs), orange = \\(|i-j| &gt; 6\\), green = \\(|i-j| &gt; 12\\), red = \\(|i-j| &gt; 24\\). Figure 1.4: \\(\\Cb\\) distances between neighboring residues in \\(\\alpha\\)-helices. Left: Direct neighbors in \\(\\alpha\\)-helices have \\(\\Cb\\) distances around \\(5.4\\AA\\) due to the geometrical constraints from \\(\\alpha\\)-helical architecture. Right: Residues separated by two positions (\\(|i-j| = 2\\)) are less geometrically restricted to \\(\\Cb\\) distances between \\(7\\AA\\) and \\(7.5\\AA\\). 1.3.5.2 Contact Definition Methods are validated with respect to precision of predictions (see section 1.3.5), where a correctly predicted contact is commonly defined as a residue pair whose \\(\\Cb\\) atoms are within 8 \\(\\AA\\) [???]. However, whether two residues truly interact in a protein structure depends only marginally on the distance between their atoms. More importantly, interactions between side-chains depend on their physico-chemical properties, on their orientation and on their environment [30] (see section 1.1.1. Other distance thresholds or definitions for contacts (e.g minimal atomic distances or distance between functional groups) have been studied as well. In fact, Duarte and colleagues found that using a \\(\\Cb\\) distance threshold between 9\\(\\AA\\) and 11\\(\\AA\\) yields optimal results when predicting the 3D structure from the respective contacts [31]. Anishchenko and colleagues [32] analysed false positive predictions with respect to a minimal atom distance threshold \\(&lt; 5 \\AA\\), as they found that this cutoff optimally defines direct physical interactions of residue pairs. However, keeping in mind that predicted contacts shall be used for structure prediction protocols, it is more convenient to apply a simple \\(\\CB\\) cutoff as this threshold can be easily implemented into protocols (e.g Modeller). Amino acid interactions strongly depend physico-chemical properties, distance, geometry and environmental preferences. Therefore, one has to keep in mind that a contact definition based merely on the distance between the atoms of two residues is a poor gold standard. It cannot capture the true interaction preferences of amino acids which are based on their physico-chemical properties and which vary within the vast number of alternative environments within proteins. 1.3.6 Pitfalls Figure 1.5: Distribution of PFAM family sizes. Less than half of the families in PFAM (7990 compared to 8489 families) do not have an annotated structure. The median family size in number of sequences for families with and without annotated structures is 185 and 827 respectively. Data taken from PFAM 31.0 (March 2017, 16712 entries). Coevolution of residues can be mediated by intermediate molecules (e.g metal ions) and will not always imply spatial proximity in structure [33]. Transitivity can lead to correlation signals. Phylogenetic bias can also lead to correlations. Some of these distant dependencies have been suggested to be caused by homooligomeric interactions [14,22][29][33]. Alternative conformations may lead to false positive predictions [33]. Sampling bias needs to be taken into account. See sequence reweighting strategies in methods section That is, very low entropy columns have on average almost twice as many contacts as high entropy columns[29]….reiterates the well-known dependence between surface accessibility and conservation…Obviously, since this position shows no variation whatsoever it cannot display any signs of statistical dependency with any other column, even though it may contact many other residues. This is a basic limitation of using statistical dependency for contact prediction that cannot be avoided theoretical upper boundary on precision: simply not all contacts within 8A interact with each other and therefore will never be detected using co-variance methods –&gt; influences sensitivity. strongly conserved residues can also not show co-variance signals –&gt; influences sensitivity 1.3.6.1 Correlation vs Causation One important shortcoming of covariance approaches arises from the fact that chains of amino acid interactions are very common in protein structures and lead to direct as well as indirect correlation signals [34] [29]. Many erronous couplings occur due to the correlation versus causation phenomenon. Considering three protein residues. Residue A interacts with residue B and B interacts with residue C. Residue A and C do not physically interact, but there will still be a correlation between A and C. These transitive effects of covariation need to be disentangled to obtain the true coupling signal. Traditional covariance methods are unable to distinguish direct and indirect correlations. References "],
["maxent.html", "1.4 Maximum entropy models for modelling protein families", " 1.4 Maximum entropy models for modelling protein families The principle of maximum entropy was postulated by Jaynes in 1957 [35] [36], stating that the probability distribution which best represents observed data is the one that is in agreement with measured constraints and has the largest entropy. It is a distribution that makes minimal assumptions and therefore is the least biased estimate of a distribution given the measured constraints. Applied to the problem of modelling protein families, one would seek a probability distribution over an alignment \\(\\mathbf{X}\\) comprising \\(N\\) protein sequences \\(\\seq^{(n)} = (x_i^{(n)},..., x_L^{(n)})\\) of length \\(L\\). Every position \\(x_i^{(n)}\\) in the MSA can take one of 21 states representing the 20 naturally occuring amino acids and a gap (‘-’) state. The distribution should reproduce the empirical single \\(\\mathcal{f}(x_i \\eq a)\\) and pairwise \\(\\mathcal{f}(x_i \\eq a, x_j \\eq b)\\) amino acid frequencies of the alignment: \\[\\begin{equation} p(x_i\\eq a) = \\mathcal{f}(x_i\\eq a) = \\frac{1}{N}\\sum_{n=1}^N I(x_i^{(n)} \\eq a) \\\\ p(x_i\\eq a, x_j \\eq b) = \\mathcal{f}(x_i\\eq a, x_j\\eq b) = \\frac{1}{N} \\sum_{n=1}^N I(x_i^{(n)} \\eq a, x_j^{(n)} \\eq b) \\tag{1.1} \\end{equation}\\] Introducing Lagrange multipliers to maximize the entropy \\(S= -\\sum_{\\seq} p(\\seq) \\log p(\\seq)\\) of the distribution subject to the constraints given in eq. (1.1) results in the formulation of an exponential model known as Potts model or its generalized form a Markov Random Field: \\[\\begin{equation} p(\\seq | \\v,\\w) = \\frac{1}{Z} \\exp \\left( \\sum_{i=1}^L v_i(x_i) \\sum_{1 \\leq i &lt; j \\leq L}^L w_{ij}(x_i, x_j) \\right) \\tag{1.2} \\end{equation}\\] \\(Z\\) is a normalization constant also known as partition function that ensures the total probabilty adds up to one by summing over all possible assignments to \\(\\seq\\). \\[\\begin{equation} Z = \\sum_{\\seq&#39; \\in [1,...,20]^L} p(\\seq&#39; | \\v, \\w) \\tag{1.3} \\end{equation}\\] 1.4.1 Properties of model parameters The Langrange multipliers \\(\\v\\) and \\(\\w\\), introduced to satisfy the constraints in eq. (1.1), specify the maximum entropy model and need to be tuned. The single potentials \\(v_i(a)\\) correspond to the single amino acid frequencies whereas the pairwise potentials \\(w_{ij}(a,b)\\), also called couplings, reflect the tendency of an amino acid a at position i to co-occur with an amino acid b at position j in the alignment. Maximum entropy models belong to the family of exponential models and thus have a unique global minimum. Markov Random Field belongs to the class of undirected graphical models. Therefore they can be represented as a graph with nodes corresponding to positions in the alignment and edges describing the dependency structure between nodes. Can disentangle direct and indirect correlations. Infer parameters of a maximum entropy model, more specifically a Potts model (statistical physics) aka markov random fiels (computer science). Likelihood function is convex, but Maximum Likelihood inference of model is infeasible: Likelihood function needs to be reevaluated at each iteration during optimization but partition function term sums over 20^L sequences. 1.4.2 Infering model parameters Typically, one would obtain parameter estimates by maximizing the log-likelihood function of the parameters over observed sequences in the alignment \\(\\mathbf{X}\\): \\[\\begin{equation} \\mathcal{L}(\\v, \\w | \\mathbf{X}) = \\sum_{n=1}^N \\log p(\\seq^{(n)}) = \\sum_{n=1}^N \\left[ \\sum_{i=1}^L v_i(x_i^{(n)}) \\sum_{1 \\leq i &lt; j \\leq L}^L w_{ij}(x_i^{(n)}, x_j^{(n)}) - \\log Z \\right] \\end{equation}\\] However, optimizing the log-likelihood requires computing the partition function that sums \\(20^L\\) terms. Naturally occurig protein domains comprise hundreds of residues. Because of this exponential complexity in protein length L it is computationally intractable to evaluate the log-likelihood function at every iteration of an optimization procedure. Several approximate solutions have been developed to evade the infeasible computation of the partition function. In 1999 Lapedes et al. [34] were the first to apply maximum entropy models to the problem of predicting residue pairs in spatial proximity and thus entangling transitive effects. They used an iterative Monte Carlo procedure to obtain estimates of the partition function. In 2009 Weight et al proposed a message-passing algorithm to approximate the partition function [37]. Eventhough their approach is computationally very expensive and only applicable to small proteins, they obtained remarkable results for the two-component signaling system in bacteria. Balakrishnan and collegues [38] were the first to apply L1-regularized pseudo-likelihood optimization in 2011. Pseudo-likelihood optimizes a different objective and does not aim at approximating the partition function Z. This approach is described in more detail in ?? as it represents the most successfull approach, eventhough it was not taken notice of at that time. Also in 2011, another approximate solution to the partition fucntion was proposed that uses mean-field expansion [33]. This method is termed mfDCA. This study resulted in the first 3D models of yet unsolved protein structures being computed with the help of predicted contacts [???] This method allowed high-throughput predictions as the mean-field approach boils down to inverting the covariance matrix which allows for shorter running times. A related approach is sparse inverse covariance estimation (PSICOV) method by Jones and colleggues.[39] They use L1-regularization to invert the correlation matrix and enforce sparsity which is known as grapgical Lasso [40]. Both procedures, mfDCA and PSICOV, assume that model distribution is a multivariate Gaussian. it has been shown by Banerjee et al. (2008) that this dual optimization solution also applies to binary data (as is the case in this application). In order to represent the MSA as continuous distributed, each position is encoded as a 20-dimensional binary vector. So far using pseudo-likelihood maximization to infer the model directly from the input alignment has proven to be the most accurate approach. There exist several implementations of pseudo-likelihood maximization that vary only in implemenation details, perform similarly and thus are equally popular in the community: CCmpred [41], plmDCA[42], GREMLIN [43]. 1.4.3 Pseudo-Likelihood Instead of optimizing the likelihood, Besag suggested in 1975 to rather optimize a function that he termed pseudo-likelihood that replaces the joint probability with the product over conditionals [44]: \\[\\begin{equation} p(\\seq | \\v,\\w) = \\prod_{i=1}^L p(x_i | \\seq_{/xi}\\v,\\w) = \\prod_{i=1}^L \\frac{1}{Z_i} \\exp \\left( v_i(x_i) \\sum_{1 \\leq i &lt; j \\leq L}^L w_{ij}(x_i, x_j) \\right) \\end{equation}\\] Here, the normalization term \\(Z_i\\) sums only over all assignments to one position \\(i\\) in sequence: \\[\\begin{equation} Z_i = \\sum_{a=1}^{20} \\exp \\left( v_i(a) \\sum_{1 \\leq i &lt; j \\leq L}^L w_{ij}(a, x_j) \\right) \\end{equation}\\] The learning objective must include a regularization, as it is overparametrized. The indeterminacy can be fixed by including a regularization prior that also prevents overfitting. Commonly [41] [42], an L2-regularization is used which corresponds to a zero centered Gaussian prior. \\[\\begin{align} R(\\v, \\w) &amp;= \\mathcal{N}(\\v | \\vec{0}, \\lambda_v I^{-1}) + \\mathcal{N}(\\w | \\vec{0}, \\lambda_w I^{-1}) \\\\ &amp;= \\lambda_v ||\\v||_2^2 + \\lambda_w ||\\w||_2^2 \\end{align}\\] The pseudo-likelihood function retains the concavity of the likelihood.[45] Moreover, it has been shown that pseudo-likelihood is a consistent estimator in the limit of infinite data for pairwise MRF [44] [46]. That is, as the number of sequences in the alignment increases, pseudo-likelihood estimates converge towards the true full likelihood parameters. the proba- bility of them-th observation, xm, is approximated by the product of the conditional probabilities[45] Recall from above that our pseudo-likelihood uses the full representation and fixes the gauge by the regularization terms Rl2. Our procedure is therefore first to infer the interaction parameters using using the pseudo-likelihood and the regularization, and then change to the zero-sum gauge [47] On one hand, this improvement might not be surpris-ing: it is known that, for very large data sets, pseudo- likelihood maximization becomes asymptotically equiva- lent to full maximum-likelihood inference, whereas mean- field inference remains intrinsically approximate, and this may result in an improved PLM performance. On the other hand, the above advantage holds if and only if the following two conditions are fulfilled: data a drawn independently from a probability distribution, and this probability distribution is the the Boltzmann distribution of a Potts model. None of these two con- ditions actually hold for real protein sequences [47] Disregarding the improvements, we find that overall the predicted contact pairs for plmDCA and mfDCA are highly overlapping, illustrating the robustness of DCA results with respect to the algorithmic implementation[47] 1.4.4 Computing contact map from coupling matrix direct information frobenius norm average product correction (also for MI) (benchmark plot for localmethods + ccmpred) --> References "],
["interpretation-of-coupling-matrices.html", "2 Interpretation of Coupling Matrices", " 2 Interpretation of Coupling Matrices State-of-the-art contact prediction methods map the 20 x 20 coupling matrices \\(w_{ij}\\) onto scalar values to obtain contact scores for each residue pair (see section 1.4.4). By doing so, the full information contained in coupling matrices is lost: the contribution of individual couplings \\(\\wijab\\) the direction of couplings (positive or negative) the correlation between couplings \\(\\wijab\\) and \\(\\wijcd\\) intrinsic biological meaning The following analyses give some intuition for the information contained in coupling matrices. "],
["single-coupling-values-carry-evidence-of-contacts.html", "2.1 Single Coupling Values Carry Evidence of Contacts", " 2.1 Single Coupling Values Carry Evidence of Contacts Given the success of DCA methods, it is clear that contact scores are good indicators of spatial proximity for residue pairs. As described in section 1.4.4, a contact score for a residue pair is commonly computed as the square root over the sum of squared coupling values. Figure 2.1 shows the correlation between squared coupling values and contact class. All couplings have a positive class correlation, meaning the stronger their squared value, the more likely a contact can be inferred. Generally, couplings that involve any aliphatic amino acid (I, L, V) or alanine express the strongest class correlation. In contrast, C-C or aromatic pairings (involving Y, F, W) correlate only weakly with contact class. Therefore, these couplings often might contribute to false positive predictions. Figure 2.1: Correlation of squared coupling values \\((\\wijab)^2\\) with contact class (contact=1, non-contact=0) for approximately 100 000 residue pairs per class (details see section 5.3.1). Contacts defined as residue pairs with \\(\\Cb &lt; 8 \\AA\\) and non-contacts as residue pairs with \\(\\Cb &gt; 25 \\AA\\). Apparantly, distinct couplings are of varying importance for contact inference. Without squaring the coupling values, these charateristics become even more pronounced. Figure 2.2 shows the correlation of raw coupling values with contact class. Interestingly, in contrast to the finding with squared coupling values, only couplings for charged pairs have strong correlation (positive and negative) with class value, whereas couplings for hydrophobic pairs correlate to a much lesser extent (mostly negative). This implies that absolute (squared) coupling strength is much more indicative of a contact for hydrophobic pairings than the direction of coupling. On the contrary, for charged residue pairs the direction of a coupling value is a stronger indicator than the strength of the squared coupling value. As with squared couplings, raw couplings for aromatic pairs or C-C pairs correlate only weakly with contact class. For these pairings, neither coupling strength, nor direction of coupling seems to be a good indicator for a contact. Figure 2.2: (ref:caption-coupling-correlation) Of course, looking only at correlations can be misleading if there are non-linear patterns in the data, for example higher order dependencies between couplings. For this reason it is advisable to take a more detailed view at coupling matrices and the distributions of their values. "],
["physico-chemical-fingerprints-in-coupling-matrices.html", "2.2 Physico-Chemical Fingerprints in Coupling Matrices", " 2.2 Physico-Chemical Fingerprints in Coupling Matrices The correlation analysis of raw coupling matrices in the last section revealed that certain coupling values tend to indicate a contact more strongly than others. Single coupling matrices of residue contacts often display striking patterns that agree with the previuos findings. Most often, these patterns suggest biological relevant details of the interdependency between both residues. Figure 2.3 visualizes the inferred coupling matrix for a residue pair (residues 6 and 82) in protein 1awq, chain A. Clearly visible is a cluster of strong coupling values for charged and polar residues (E,D,K,R,Q). Positive coupling values can be observed between positively charged residues (R,K) and negatively charged residues (E,D), whereas coupling values between equally charged residues are negative. The coupling matrix perfectly reflects the interaction preference for residues forming salt bridges. Indeed, in the protein structure residue 6 (glutamic acid) forms a salt bridge with residue 82 (lysine) as can be seen in figure 2.5. Figure 2.3: Coupling matrix for residues 6 and 82 in protein 1awq chain A. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis and y-axis represent the corresponding single potentials for both residues. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values. Figure 2.4 visualizes the coupling matrix for a pair of hydrophobic residues (residues 29 and 39) in protein 1ae9 chain A. Hydrophobic pairings have strong coupling values but the couplings also reflect a sterical constraint: alanine as a small hydrophobic residue is favoured at either position 29 or position 39, but disfavoured to appear at both positions. Figure 2.5 illustrates the location of the two residues in the protein core. Here, hydrophobic residues are densely packed and the limited space allows for only small hydrophobic residues. Figure 2.4: Coupling matrix for residues 29 and 39 in protein 1ae9 chain A. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis and y-axis represent the corresponding single potentials for both residues. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values. --> Figure 2.5: Interactions between protein side chains. Left: residue 6 (glutamic acid) forming a salt bridge with residue 82 (lysine) in protein 1awq, chain A. Right: residue 29 (alanine) and residue 39 (leucine) within the hydrophobic core of protein 1ae9 chain A. Many more biological interpretable signals can be identified from coupling matrices, including pi-cation interactions (see Appendix C.1), aromatic-proline interactions (see Appendix C.3), sulfur-aromatic interactions or disulphide bonds (see Appendix C.2). Coucke and collegues [48] performed a thorough quantitative analysis of coupling matrices selected from confidently predicted residue pairs. They showed that eigenmodes obtained from a spectral analysis of averaged coupling matrices are closely related to physico-chemical properties of amino acid interactions, like electrostaticity, hydrophobicity, steric interactions or disulphide bonds. By looking at specific populations of residue pairs, like buried and exposed residues or residues pairs from specific protein classes (small, mainly \\(\\alpha\\), etc), the eigenmodes capture very characteristic interactions for each class, e.g. rare disulfide contacts withins small proteins and hydrophilic contacts between exposed residues. Their study confirms our qualitative observation that amino acid interactions can leave characteristic physico-chemical fingerprints in coupling matrices. References "],
["coupling-profiles-vary-with-distance.html", "2.3 Coupling Profiles Vary with Distance", " 2.3 Coupling Profiles Vary with Distance Analyses in the previous sections showed that certain coupling values correlate more or less strong with contact class and that coupling matrices for contacts express biological meaningpull patterns. More insights can be obtained by looking at the distribution of distinct coupling values for contacts, non-contacts and arbitrary populations of residue pairs. To avoid uninformative couplings, we consider only residue pairs with a sequence separation &gt; 10 and with enough evidence for a certain amino acid pairing (see Methods section 5.3.2 for details). Figure 2.6 shows the distribution of selected couplings for residue pairs within a \\(\\Cb\\) distance \\(&lt; 5\\AA\\). The distribution of R-E and E-E coupling values is shifted and skewed towards positive and negative values respectively. This is in accordance with attracting electrostatic interactions between the positively charged side chain of arginine and the negatively charged side chain of gluatamic acid and also with repulsive interactions between the two negatively charged gluatamic acid side chains. Couling values for C-C pairs have a broad distribution that is skewed towards positive values, reflecting the strong signals obtained from covalent disulphide bonds. Hydrophobic pairs like V-I have an almost symmetric coupling distribution, confirming the finding that the direction of coupling is not indicative of a true contact whereas the strength of the coupling is. Hydrophobic interactions arising from the hydrophobic effect are not specific or directed and can easily be substituted by other hydrophobic residues, which explains the not very pronounced positive coupling signal compared to more specific interactions, e.g ionic interactions. The distribution of aromatic coupling values like F-W is slightly skewed towards negative values, accounting for steric hindrance of their large side chains at small distances. Figure 2.6: Distribution of selected couplings for approximately 10000 filtered residue pairs with \\(\\Cb\\) distance \\(&lt; 5\\AA\\) (see Methods section 5.3.2 for details). In an intermediate \\(\\Cb\\) distance range between \\(8\\AA\\) and \\(12\\AA\\) the distributions for all coupling values are centered close to zero and are less broad. The distributions are still shifted and skewed as for \\(\\Cb\\) distance \\(&lt; 5\\AA\\) but much less pronounced. For aromatic pairs like F-W, the distribution of coupling values has very long tails, suggesting strong couplings for aroamtic side chains at this distance. Figure 2.7: Distribution of selected couplings for approximately 10000 filtered residue pairs with \\(\\Cb\\) distance \\(&lt; 5\\AA\\) (see Methods section 5.3.2 for details). Figure 2.8 shows the distribution of selected couplings for residue pairs far apart in the protein structure (\\(\\Cb\\) distance \\(&gt; 20\\AA\\)). The distribution for all couplings is centered at zero and has small variance. For C-C coupling values, the distribution has a long tail for positve values, presumably arising from the fact that the maximum entropy model cannot distuinguish highly conserved signals of multiple disulphide bonds within a protein. This observation also agrees with the previous finding that C-C couplings correlate only weakly with contact class. The same arguments apply to couplings of aromatic pairs that have a comparably broad distribution and do not correlate strongly with contact class. The strong coevolution signals for aromatic pairs even at high distance ranges might be to insufficient disentanglig of transitive effects, as aromatic residues are known to form network-like structures in the protein core that stabilize protein structure (see Figure C.7 in Appendix)[13]. Figure 2.8: Distribution of selected couplings for approximately 10000 filtered residue pairs with \\(\\Cb\\) distance \\(&gt; 25\\AA\\) (see Methods section 5.3.2 for details). References "],
["higher-order-dependencies-between-couplings.html", "2.4 Higher Order Dependencies Between Couplings", " 2.4 Higher Order Dependencies Between Couplings The analyses in the previous sections focused on single coupling values of the \\(20 \\times 20\\)-dimensional coupling matrices. As mentioned before, looking at single variables might be misleading if they are dependent on another. Unfortunately, it is not possible to reasonably visualize the high dimensional coupling matrices. But there are several ways to identify interesting dimensions. First of all, 2-dimensional scatter plots of couplings for biological relevant pairings confirm the previous trend that couplings reflect amino acid interactions. Figure 2.9 and 2.10 illustrate the distribution of attractive and repulsive ionic interactions at \\(\\Cb\\) distances less than \\(8\\AA\\). Whereas coupling values for R-E and E-R are positively correlated, coupling values for R-E and E-E are negatively correlated. Hydrophobic coupling values for residue pairs at \\(\\Cb\\) distances less than \\(8\\AA\\) are symmetrically distributed around zero, in agreement with all previous analyses (Figure 2.11). Figure 2.9: Two-dimensional distribution of coupling values R-E and E-R for approximately 10000 residue pairs with \\(\\Delta\\Cb &lt; 8\\AA\\). The distribution is almost symmetrical and the coupling values are positively correlated. Residue pairs have been filtered for sequence separation, percentage of gaps and evidence in alignment (see Methods 5.3.2). Figure 2.10: Two-dimensional distribution of coupling values R-E and E-E for approximately 10000 residue pairs with \\(\\Delta\\Cb &lt; 8\\AA\\). The coupling values are negatively correlated. Residue paris have been filtered for sequence separation, percentage of gaps and evidence in alignment (see Methods 5.3.2). Figure 2.11: Two-dimensional distribution of coupling values V-I and I-L for approximately 10000 residue pairs with \\(\\Delta\\Cb &lt; 8\\AA\\). The coupling values are symmetrically distributed around zero. Residue paris have been filtered for sequence separation, percentage of gaps and evidence in alignment (see Methods 5.3.2). -->"],
["bayesian-model-for-contact-prediction.html", "3 Bayesian Model for Contact Prediction", " 3 Bayesian Model for Contact Prediction The most popular and successfull methods for contact prediction so far optimize the pseudo-log-likelihood of the MSA and use several heuristics to calculate a contact score (see section 1.4.4). The next sections introduce a principled Bayesian statistical approach that eradicates these heuristics. Instead of transforming the model parameters \\(\\w\\) into heuristic contact scores, one can compute the posterior probability distributions of the distances \\(r_{ij}\\) between \\(\\Cb\\) atoms of all residues pairs \\(i\\) and \\(j\\), given the MSA \\(\\X\\). The coupling parameters \\(\\w\\) are treated as hidden variables that will be integrated out analytically. This approach also allows for extraction of information contained in the particular types of amino acids, since each pair of amino acids will have a different preference to be coupled at certain distances. "],
["likelihood-of-the-sequences-as-a-potts-model.html", "3.1 Likelihood of the sequences as a Potts model", " 3.1 Likelihood of the sequences as a Potts model We denote the \\(N\\) sequences in the MSA \\(\\X\\) with \\({\\seq_1, ..., \\seq_N}\\). Each sequence \\(\\seq_n = (\\seq_{n1}, ..., \\seq_{nL})\\) is a string of \\(L\\) letters from an alphabet indexed by \\(\\{0, ..., 20\\}\\), where 0 stands for a gap and \\(\\{1, ... , 20\\}\\) stand for the 20 types of amino acids. The goal is to predict from \\(\\X\\) the distances \\(r_{ij}\\) between the \\(\\Cb\\) atoms of all pairs of residues \\((i, j) \\in \\{1, ..., L\\}\\). The link between the MSA \\(\\X\\) and the vector \\(\\mathbf{r}\\) of all inter-\\(\\Cb\\) distances is described via the evolutionary couplings of residue pairs that are the \\(20^2\\)-dimensional vectors \\(w_{ij}\\). As already described in detail in section 1.4, we model the likelihood of the sequences in an MSA with a Potts Model, also known as MRF: \\[\\begin{equation} p(\\X | \\v, \\w) = \\prod_{n=1}^N p(\\seq_n | \\v, \\w) = \\prod_{n=1}^N \\frac{1}{Z(\\v, \\w)} \\exp \\left( \\sum_{i=1}^L v_i(x_{ni}) \\sum_{1 \\leq i &lt; j \\leq L} w_{ij}(x_{ni}, x_{nj}) \\right) \\end{equation}\\] The coefficients \\(\\via\\) are the single potentials and \\(\\wijab\\) denote the coupling strengths for pairs of residues. \\(Z(\\v, \\w)\\) is the so-called partition sum that normalizes the probability distribution \\(p(\\seq_n |\\v, \\w)\\): \\[\\begin{equation} Z(\\v, \\w) = \\sum_{y_1, ..., y_L = 1}^{20} \\exp \\left( \\sum_{i=1}^L v_i(y_i) \\sum_{1 \\leq i &lt; j \\leq L} w_{ij}(y_i, y_j) \\right) \\end{equation}\\] TODO: this is irrelevant for CD, isn’t it? For an efficient computational implementation, we might sum over all \\(1 \\le i, j \\le L\\) without demanding \\(i &lt; j\\) and enforce trivial constraints \\(\\wijab = w_{jiba}\\) during the optimization. "],
["gap-treatment.html", "3.2 Treating Gaps as Missing Information", " 3.2 Treating Gaps as Missing Information Treating gaps explicitly as 0’th letter of the alphabet would lead to couplings between columns that are not in physical contact. To see why, imagine a hypothetical alignment consisting of two sets of sequences as it is illustrated in Figure 3.1. The first set has sequences covering only the left half of columns in the MSA, while the second set has sequences covering only the right half of columns. The two blocks could correspond to protein domains that were aligned to a single query sequence. Now consider couplings between a pair of columns \\(i, j\\) with \\(i\\) from the left half and \\(j\\) from the right half. Since no sequence (except the single query sequence) overlaps both domains, the empirical amino acid pair frequencies \\(q(x_i = a, x_j = b)\\) will vanish for all \\(a, b \\in \\{1,... , L\\}\\). Figure 3.1: Hypothetical MSA consisting of two sets of sequences: the first set has sequences covering only the left half of columns, while the second set has sequences covering only the right half of columns. The two blocks could correspond to protein domains that were aligned to a single query sequence. Empirical amino acid pair frequencies \\(q(x_i \\eq a, x_j \\eq b)\\) will vanish for positions \\(i\\) from the left half and \\(j\\) from the right half of the alignment. The gradient of the log likelihood for couplings is \\[\\begin{align} \\frac{\\partial LL}{\\partial \\wijab} &amp;= \\sum_{n=1}^N I(x_{ni}=a, x_{nj}=b) - N \\frac{\\partial}{\\partial \\wijab} \\log Z(\\v,\\w) \\\\ &amp;= \\sum_{n=1}^N I(x_{ni} \\eq a, x_{nj} \\eq b) \\\\ &amp; - N \\sum_{y_1,\\ldots,y_L=1}^{20} \\!\\! \\frac{ \\exp \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L} w_{ij}(y_i,y_j) \\right)}{Z(\\v,\\w)} I(y_i \\eq a, y_j \\eq b) \\\\ &amp;= N q(x_{i} \\eq a, x_{j} \\eq b) - N \\sum_{y_1,\\ldots,y_L=1}^{20} p(y_1, \\ldots, y_L | \\v,\\w) \\, I(y_i \\eq a, y_j \\eq b) \\\\ &amp;= N q(x_{i} \\eq a, x_{j} \\eq b) - N p(x_i \\eq a, x_j \\eq b | \\v,\\w) \\tag{3.1} \\end{align}\\] Note that the empirical frequencies are equal to the model probabilities at the maximum of the likelihood when the gradient vanishes. Therefore, \\(p(x_i \\eq a, x_j \\eq b | \\v, \\w)\\) would have to be zero in the optimum when the empirical amino acid frequencies \\(q(x_i \\eq a, x_j \\eq b)\\) vanish for pairs of columns as described above. However, \\(p(x_i \\eq a, x_j \\eq b | \\v, \\w)\\) can only become zero, when the exponential term in \\(p(x_i \\eq a, x_j \\eq b | \\v, \\w)\\) ammounts to zero, which would only be possible if \\(\\wijab\\) goes to \\(−\\infty\\). This is clearly undesirable, as we want to deduce physical contacts from the size of the couplings. The solution is to treat gaps as missing information. This means that the normalisation of \\(p(\\seq_n | \\v, \\w)\\) should not run over all positions \\(i \\in \\{1,... , L\\}\\) but only over those \\(i\\) that are not gaps in \\(\\seq_n\\). Therefore we define the set of sequences \\(\\Sn\\) used for normalization of \\(p(\\seq_n | \\v, \\w)\\) as: \\[\\begin{equation} \\Sn := \\{(y_1,... , y_L): 0 \\leq y_i \\leq 20 \\land (y_i \\eq 0 \\textrm{ iff } x_{ni} \\eq 0) \\} \\end{equation}\\] and the partition function becomes: \\[\\begin{equation} Z_n(\\v, \\w) = \\sum_{\\mathbf{y} \\in \\Sn} \\exp \\left( \\sum_{i=1}^L v_i(y_i) \\sum_{1 \\leq i &lt; j \\leq L} w_{ij}(y_i, y_j) \\right) \\end{equation}\\] To ensure that the gaps in \\(x_n\\) do not contribute anything to the sums, we fix all parameters associated with a gap to 0: \\(v_i(0) = 0\\) and \\(w_{ij}(0, b) = w_{ij}(a, 0) = 0\\) for all \\(i, j \\in \\{1, ..., L\\}\\) and \\(a, b \\in \\{0, ..., 20\\}\\). Furthermore, we redefine the empirical amino acid frequencies \\(q_{ia}\\) and \\(q_{ijab}\\) such that they are normalised over \\(\\{1, ..., 20\\}\\): \\[\\begin{align} N_i :=&amp; \\sum_{n=1}^N I(x_{ni} \\!\\ne\\! 0) &amp; q_{ia} = q(x_i \\eq a) :=&amp; \\frac{1}{N_i} \\sum_{n=1}^N I(x_{ni} \\eq a) \\\\ N_{ij} :=&amp; \\sum_{n=1}^N I(x_{ni} \\!\\ne\\! 0, x_{nj} \\!\\ne\\! 0) &amp; q_{ijab} = q(x_i \\eq a, x_j \\eq b) :=&amp; \\frac{1}{N_{ij}} \\sum_{n=1}^N I(x_{ni} \\eq a, x_{nj} \\eq b) \\end{align}\\] With this definition, empirical amino acid frequencies are normalized without gaps, so that \\[\\begin{equation} \\sum_{a=1}^{20} q_{ia} = 1 \\; , \\; \\sum_{a,b=1}^{20} q_{ijab} = 1. \\tag{3.2} \\end{equation}\\] "],
["gauge-transformation.html", "3.3 Gauge transformation", " 3.3 Gauge transformation The model contains \\(L \\times 20 + \\frac{L(L − 1)}{2} \\times 20^2\\) parameters, but the parameters are not uniquely determined. For example, for any fixed position \\(i\\) and amino acid a we can add a constant to \\(\\via\\) and subtract the same constant from the \\(20L\\) coefficients \\(\\wijab\\) with \\(b \\in \\{1, ..., 20\\}\\) and \\(j \\in \\{1, ..., L\\}\\). This overparametrization, the so-called gauge transformation, would leave the probabilities for all sequences under the model unchanged. We could eliminate parameters by enforcing the restraints \\(\\sum_{a=1}^{20} v_{ia} = 0\\) and \\(\\sum_{a=1}^{20} \\wijab = 0 = \\sum_{a=1}^{20} w_{ijba}\\). However, it is easier to rather formulate carefully the link between the distribution of \\(\\w_{ij}\\) vectors and the distance \\(r_ij\\) while taking the non-uniqueness of parameters into acount, as we will see below. "],
["the-regularized-log-likelihood-function-llregvw.html", "3.4 The regularized log likelihood function LLreg(v,w)", " 3.4 The regularized log likelihood function LLreg(v,w) In pseudo-likelihood based methods, a regularisation is commonly used that can be interpreted to arise from a prior probability. We will do the same here, constraining \\(\\v\\) and \\(\\w\\) by Gaussian priors \\(\\mathcal{N}( \\v | \\v^*, \\lambda_v^{-1} \\I)\\) and \\(\\mathcal{N}( \\w |\\boldsymbol 0, \\lambda_w^{-1} \\I)\\). The choice of \\(v^*\\) will be discussed in the section 3.6. By including the logarithm of this prior into the log likelihood using the gap treatment described in section @ref{gap-treatment}, we obtain the regularised likelihood, \\[\\begin{equation} \\LLreg(\\v,\\w) = \\log \\left[ p(\\X | \\v,\\w) \\; \\Gauss (\\v | \\v^*, \\lambda_v^{-1} \\I) \\; \\Gauss( \\w | \\boldsymbol 0, \\lambda_w^{-1} \\I) \\right] \\end{equation}\\] or explicitely, \\[\\begin{align} \\LLreg(\\v,\\w) =&amp; \\sum_{n=1}^N \\left[ \\sum_{i=1}^L v_i(x_{ni}) + \\sum_{1\\le i&lt;j\\le L} w_{ij}(x_{ni},x_{nj}) - \\log Z_n(\\v,\\w) \\right] \\\\ &amp; - \\frac{\\lambda_v}{2} \\!\\! \\sum_{i=1}^L \\sum_{a=1}^{20} (\\via - \\via^*)^2 - \\frac{\\lambda_w}{2} \\sum_{1 \\le i &lt; j \\le L} \\sum_{a,b=1}^{20} \\wijab^2 . \\end{align}\\] "],
["the-gradient-of-the-regularized-log-likelihood.html", "3.5 The gradient of the regularized log likelihood", " 3.5 The gradient of the regularized log likelihood The gradient of the regularized log likelihood has single components \\[\\begin{align} \\frac{\\partial \\LLreg}{\\partial \\via} =&amp; \\sum_{n=1}^N I(x_{ni}=a) - \\sum_{n=1}^N \\frac{\\partial}{\\partial \\via} \\, \\log Z_n(\\v,\\w) - \\lambda_v (\\via - \\via^*)\\\\ =&amp; \\; N_i q(x_i \\eq a) \\\\ &amp; - \\sum_{n=1}^N \\sum_{\\mathbf{y} \\in \\Sn} \\frac{ \\exp \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i&lt;j \\le L}^L w_{ij}(y_i,y_j) \\right) }{Z_n(\\v,\\w)} I(y_i=a) \\\\ &amp; - \\lambda_v (\\via - \\via^*) \\tag{3.3} \\end{align}\\] and pair components \\[\\begin{align} \\frac{\\partial \\LLreg}{\\partial \\wijab} =&amp; \\sum_{n=1}^N I(x_{ni} \\eq a, x_{nj} \\eq b) - \\sum_{n=1}^N \\frac{\\partial}{\\partial \\wijab} \\log Z_n(\\v,\\w) - \\lambda_w \\wijab \\\\ =&amp; \\; N_{ij} q(x_i \\eq a, x_j=b) \\\\ &amp; - \\sum_{n=1}^N \\sum_{\\mathbf{y} \\in \\Sn} \\frac{ \\exp \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i&lt;j \\le L}^L w_{ij}(y_i,y_j) \\right) }{Z_n(\\v,\\w)} I(y_i \\eq a, y_j \\eq b) \\\\ &amp; - \\lambda_w \\wijab \\tag{3.4} \\end{align}\\] Note that (without regulariation \\(\\lambda_v = \\lambda_w = 0\\)) the empirical frequencies \\(q(x_i \\eq a)\\) and \\(q(x_i \\eq a, x_j=b)\\) are equal to the model probabilities at the maximum of the likelihood. If the proportion of gap positions in \\(\\X\\) is small (e.g. \\(&lt;5\\%\\), also compare percentage of gaps in dataset in Appendix Figure B.2), we can approximate the sums over \\(\\mathbf{y} \\in \\Sn\\) in eqs. (3.3) and (3.4) by \\(p(x_i=a | \\v,\\w) I(x_{ni} \\ne 0)\\) and \\(p(x_i=a, x_j=b | \\v,\\w) I(x_{ni} \\ne 0, x_{nj} \\ne 0)\\), respectively, and the partial derivatives become \\[\\begin{align} \\frac{\\partial \\LLreg}{\\partial \\via} =&amp; \\; N_i q(x_i \\eq a) - N_i \\; p(x_i \\eq a | \\v,\\w) - \\lambda_v (\\via - \\via^*) \\\\ \\frac{\\partial \\LLreg}{\\partial \\wijab} =&amp; \\; N_{ij} q(x_i \\eq a, x_j=b) - N_{ij} \\; p(x_i \\eq a, x_j \\eq b | \\v,\\w) - \\lambda_w \\wijab \\tag{3.5} \\end{align}\\] Note that the couplings between columns \\(i\\) and \\(j\\) in our hypothetical MSA (see section 3.2) will now vanish since \\(N_{ij} \\eq 0\\) and the gradient with respect to \\(\\wijab\\) is equal to \\(-\\lambda_w \\wijab\\). "],
["prior-v.html", "3.6 The prior on \\(\\v\\)", " 3.6 The prior on \\(\\v\\) Most previous approaches chose a prior around the origin, \\(p(\\v) = \\Gauss ( \\v| \\mathbf{0}, \\lambda_v \\I)\\). This choice has an obvious draw-back. To see why, we take the sum over \\(b=1,\\ldots, 20\\) of the gradient of couplings in eq. (3.5) at the optimum, where the gradient vanishes. This yields \\[\\begin{equation} 0 = N_{ij}\\, q(x_i \\eq a, x_j \\ne 0) - N_{ij}\\, p(x_i \\eq a | \\v, \\w) - \\lambda_w \\sum_{b=1}^{20} \\wijab. \\end{equation}\\] Incidentally, we note that by taking the sum over \\(a\\) we find \\[\\begin{equation} \\sum_{a,b=1}^{20} \\wijab = 0. \\tag{3.6} \\end{equation}\\] At the optimum the gradient with respect to \\(v_{ia}\\) vanishes and we can substitute \\(p(x_i=a|\\v,\\w) = q(x_i=a) - \\lambda_v (\\via - \\via^*) / N_i\\), yielding \\[\\begin{equation} 0 = N_{ij} \\, q(x_i \\eq a, x_j \\ne 0) - N_{ij} \\, q(x_i=a) + \\frac{N_{ij}}{N_i}\\lambda_v (\\via - \\via^*) - \\lambda_w \\sum_{b=1}^{20} \\wijab . \\tag{3.7} \\end{equation}\\] for all \\(i,j \\in \\{1,\\ldots,L\\}\\) and all \\(a \\in \\{1,\\ldots,20\\}\\). To show that the choice \\(\\v^*= \\mathbf{0}\\) leads to undesirable results, we take an MSA without gaps. The first two terms \\(N_{ij} \\, q(x_i \\eq a, x_j \\ne 0) - N_{ij} \\, q(x_i=a)\\) vanish as they add up to zero, which leaves \\[\\begin{equation} 0 = \\lambda_v (\\via - \\via^*) - \\lambda_w \\sum_{b=1}^{20} \\wijab . \\tag{3.8} \\end{equation}\\] Consider a column \\(i\\) that is not coupled to any other and assume that amino acid \\(a\\) was frequent in column \\(i\\) and therefore \\(\\via\\) would be large and positive. Then according to eq. (3.8), for any other column \\(j\\) the 20 coefficients \\(\\wijab\\) for \\(b \\in \\{1,\\ldots,20\\}\\) would have to take up the bill and deviate from zero! To correct this unwanted behaviour, we instead chose a Gaussian prior centered around \\(\\v^*\\) obeying \\[\\begin{equation} \\frac{\\exp(\\via^*)}{\\sum_{a&#39;=1}^{20} \\exp(v_{ia&#39;}^*)} = q(x_i=a) . \\end{equation}\\] This choice ensures that if no columns are coupled, i.e. \\(p(\\seq | \\v,\\w) = \\prod_{i=1}^L p(x_i)\\), \\(\\v=\\v^*\\) and \\(\\w= \\mathbf{0}\\) gives the correct probability model for the sequences in the MSA. If we impose the restraint \\(\\sum_{a=1}^{20} \\via = 0\\) to fix the gauge of the \\(\\via\\) (i.e. to remove the indeterminacy), we get \\[\\begin{align} \\via^* = \\log q(x_i=a) - \\frac{1}{20} \\sum_{a&#39;=1}^{20} \\log q(x_i=a&#39;) . \\tag{3.9} \\end{align}\\] For this choice, \\(\\via - \\via^*\\) will be approximately zero and will certainly be much smaller than \\(\\via\\), hence the sum over coupling coefficients in eq. (3.8) will be close to zero, as it should be. Another way to understand the choice of \\(\\v^*\\) in eq. (3.9) as opposed to \\(\\v^*=\\mathbf{0}\\) is by noting that in that case \\(q(x_i \\eq a) \\approx p(x_i \\eq a|\\v^*,\\w^*)\\). Therefore, if \\(q(x_i \\eq a,x_j \\eq b) = q(x_i \\eq a) \\, q(x_j \\eq b)\\) it follows that \\(p(x_i \\eq a, x_j \\eq b | \\v,\\w) \\approx q(x_i \\eq a, x_j \\eq b) = p(x_i \\eq a | \\v^*,\\w^*)\\, p(x_j \\eq b | \\v^*,\\w^*)\\), i.e. we would correctly conclude that \\(\\wijab=0\\) and \\((i,a)\\) and \\((j,b)\\) are not coupled. "],
["the-regularized-likelihood-as-multivariate-gaussian.html", "3.7 The regularized likelihood as multivariate Gaussian", " 3.7 The regularized likelihood as multivariate Gaussian From sampling experiments done by Markus Gruber we know that the regularized pseudo-log-likelihood for realistic examples of protein MSAs obeys the equipartition theorem. The equipartition theorem states that in a harmonic potential (where third and higher order derivatives around the energy minimum vanish) the mean potential energy per degree of freedom (i.e. per eigendirection of the Hessian of the potential) is equal to \\(k_B T/2\\), which is of course equal to the mean kinetic energy per degree of freedom. Hence we have a strong indication that in realistic examples the pseudo log likelihood is well approximated by a harmonic potential. We assume here that this will also be true for the regularized log likelihood. We will come back to justify this assumption of normality in the subsection following the next. The single potentials \\(\\v\\) will be set to the target vector \\(\\v^*\\) given in eq. (3.9) and the mode \\(\\w^*\\) of the regularized log-likelihood \\(\\LLreg(\\v^*,\\w)\\) will be determined with the CD approach introduced in the previous section and described in detail in section @ref{optimizing-full-likelihood}. By carrying out a second order Taylor expansion around the mode \\(\\w^*\\) , the regularized log-likelihood \\[\\begin{equation} \\LLreg(\\v^{*},\\w) = \\log \\; \\left[ p(\\X | \\v^{*},\\w) \\, \\Gauss(\\w | \\mathbf{0}, \\lambda_w^{-1} \\I) \\right] \\end{equation}\\] can be approximated as \\[\\begin{align} \\LLreg(\\v^{*},\\w) &amp;\\overset{!}{\\approx} \\LLreg(\\v^*, \\w^*) \\\\ &amp; + \\nabla_\\w \\LLreg(\\v^*,\\w^*)(\\w-\\w^*) \\\\ &amp; + \\frac{1}{2}\\nabla^2_\\w \\LLreg(\\v^*,\\w^*)(\\w-\\w^*)^2 \\\\ \\end{align}\\] Since the gradient for the couplings vanishes at the maximum of the regularized log-likelihood, \\(\\nabla_\\w \\LLreg(\\v^*,\\w^*) = \\mathbf{0}\\), the second order approximation can be written as \\[\\begin{equation} \\LLreg(\\v^{*},\\w) \\overset{!}{\\approx} \\LLreg(\\v^*, \\w^*) - \\frac{1}{2} (\\w-\\w^*)^{\\mathrm{T}} \\, \\H \\, (\\w-\\w^*) \\;, \\end{equation}\\] where \\(\\H\\) signifies the negative Hessian matrix with respect to the components of \\(\\w\\), \\[\\begin{equation} (\\H)_{klcd, ijab} = - \\left. \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\w_{klcd} \\, \\partial \\wijab } \\right|_{(\\w^*)} . \\end{equation}\\] By applying \\(\\exp(\\cdot)\\) to the second order approximation, it turns out that the regularized likelihood can be approximated by a multivariate Gaussian, \\[\\begin{align} p(\\X | \\v^*,\\w) \\, \\Gauss(\\w| \\mathbf{0}, \\lambda_w^{-1} \\I) &amp;= p(\\X | \\v^*, \\w^*) \\, \\exp \\left( -\\frac{1}{2} (\\w-\\w^*)^{\\mathrm{T}} \\H (\\w -\\w^*) \\right) \\nonumber \\\\ &amp;= p(\\X | \\v^*,\\w^*) \\frac{(2 \\pi)^\\frac{D}{2}} { |\\H|^\\frac{D}{2}} \\times \\Gauss (\\w | \\w^*, \\H^{-1} ) \\\\ &amp;\\propto \\Gauss (\\w | \\w^*, \\H^{-1}) \\,, \\tag{3.10} \\end{align}\\] with proportionality constant that depends only on the data and with a precision matrix equal to the negative Hessian matrix. The surprisingly easy computation of the Hessian can be found in Methods section 5.5. "],
["posterior-probabilty-of-distances.html", "3.8 Posterior Probabilty of distances", " 3.8 Posterior Probabilty of distances Our ultimate goal is to compute the posterior probability of the distances, \\(p(\\r | \\X) \\propto p(\\X | \\r) \\, p(\\r )\\) and the marginals \\(p(\\rij | \\X) = \\int p(\\r | \\X) d \\r_{\\backslash ij}\\), where \\(\\r_{\\backslash ij}\\) is the vector containing all coordinates of \\(\\r\\) except \\(\\rij\\). In Bayesian statistics the posterior can be computed by analytically integrating out the hidden variables \\(\\v\\) and \\(\\w\\) in the dependence of \\(\\r\\) on \\(\\X\\): \\[\\begin{equation} p(\\X | \\r) = \\int \\int p(\\X | \\v,\\w) \\, p(\\v, \\w | \\r) \\,d\\v\\,d\\w . \\tag{3.11} \\end{equation}\\] This can be achieved with the following strategy: factorize the integrand into factors over \\((i,j)\\) and perform each integration over the coupling coefficients \\(\\wij\\) for \\((i,j)\\) separately. As discussed earlier (see section 3.6), the single potentials \\(\\v\\) are fixed to its best estimate \\(\\v^*\\). Theoretically, this can be done by using a very tight prior \\(p(\\v) = \\Gauss(\\v|\\v^*,\\lambda_v^{-1} \\I) \\rightarrow \\delta(\\v-\\v*)\\) for \\(\\lambda_v \\rightarrow \\infty\\). In practice, \\(\\v\\) will not be optimized but simply be set to the value of \\(\\v^*\\). This allows the replacement of the intergral over \\(\\v\\) with the value of the integrand at its mode \\(\\v^*\\). Furthermore, the prior over \\(\\w\\) will be defined as a product over independent contributions over \\(\\wij\\) with \\(\\wij\\) depending only on the distance \\(\\rij\\): \\[\\begin{equation} p(\\v,\\w|\\r) = \\Gauss(\\v|\\v^*,\\lambda_v^{-1} \\I) \\, \\prod_{1\\le i&lt;j\\le L} p(\\wij|\\rij) . \\tag{3.12} \\end{equation}\\] 3.8.1 Modelling the dependence of \\(\\wij\\) on \\(\\rij\\) The prior over couplings \\(p(\\wij|\\rij)\\) will be modelled as a mixture of \\(K\\!+\\!1\\) 400-dimensional Gaussians, with means \\(\\muk \\in \\R^{400}\\), precision matrices \\(\\Lk \\in \\R^{400\\times 400}\\), and distance-dependent, normalised weights \\(g_k(\\rij)\\), \\[\\begin{align} p(\\wij | \\rij) = \\sum_{k=0}^K g_k(\\rij) \\, \\Gauss(\\wij | \\muk, \\Lk^{-1}) \\,. \\tag{3.13} \\end{align}\\] The mixture weights \\(g_k(\\rij)\\) in eq. (3.13) are modelled as softmax: \\[\\begin{equation} g_k(\\rij) = \\frac{\\exp \\gamma_k(\\rij)}{\\sum_{k&#39;=0}^K \\exp \\gamma_{k&#39;}(\\rij)} \\label{eq:def-g_k_binary} \\end{equation}\\] The functions \\(g_k(\\rij)\\) remain invariant when adding an offset to all \\(\\gamma_k(\\rij)\\). This degeneracy can be removed by setting \\(\\gamma_0(\\rij)=1\\). 3.8.2 Integrating out \\(\\v\\) and \\(\\w\\) Substituting eq. (3.12) into eq. (3.11) and performing the integral over \\(\\v\\) (where the tight Gaussian acts as a delta function) yields \\[\\begin{eqnarray} p(\\X | \\r) &amp;=&amp; \\int \\left( \\int p(\\X | \\v,\\w) \\, \\Gauss(\\v|\\v^*,\\lambda_v^{-1} \\I) \\,d\\v \\right) \\, \\prod_{1\\le i&lt;j\\le L} p(\\wij|\\rij) \\, d\\w \\\\ p(\\X | \\r) &amp;=&amp; \\int p(\\X | \\v^*,\\w) \\, \\prod_{1\\le i&lt;j\\le L} p(\\wij|\\rij) \\, d\\w \\label{eq:in_over_w_1} \\end{eqnarray}\\] Next, the likelihood will be multiplied with the regularisation prior and the distance-dependent prior will be divided by the regularisation prior again: \\[\\begin{eqnarray} p(\\X | \\r) &amp;=&amp; \\int p(\\X | \\v^*,\\w) \\, \\Gauss(\\w|\\mathbf{0}, \\lambda_w^{-1} \\I) \\, \\prod_{1\\le i&lt;j\\le L} \\frac{p(\\wij|\\rij)}{\\Gauss(\\wij|\\mathbf{0}, \\lambda_w^{-1} \\I)} \\,d\\w \\, . \\end{eqnarray}\\] Now the crucial advantage of our likelihood regularisation is borne out: We can chose the strength of the regularisation prior, \\(\\lambda_w\\), such that the mode \\(\\w^*\\) of the regularised likelihood is near to the mode of the integrand in the last integral. The regularisation prior \\(\\Gauss(\\wij|\\mathbf{0}, \\lambda_w^{-1} \\I)\\) is then a simpler, approximate version of the real, distance-dependent prior \\(\\prod_{1\\le i&lt;j\\le L} p(\\wij|\\rij)\\). This allows us to approximate the regularised likelihood with a Gaussian distribution (eq. (3.10)), because this approximation will be fairly accurate in the region around its mode, which is near the region around the mode of the integrand and this again is in the region that contributes most to the integral: \\[\\begin{eqnarray} p(\\X | \\r) &amp;\\propto&amp; \\int \\Gauss (\\w | \\w^*, \\H^{-1} ) \\, \\prod_{1 \\le i&lt;j \\le L} \\frac{p(\\wij | \\rij)}{\\Gauss(\\wij|\\mathbf{0}, \\lambda_w^{-1} \\I)} d\\w \\,. \\tag{3.14} \\end{eqnarray}\\] The matrix \\(\\H\\) has dimensions \\((L^2 \\times 20^2) \\times (L^2 \\times 20^2)\\). Computing it is obviously infeasible, even if there was a way to compute \\(p(x_i \\eq a, x_j \\eq b| \\v^*,\\w^*)\\) efficiently. In Methods section 5.4 is shown that in practice, the off-diagonal block matrices with \\((i,j) \\ne (k,l)\\) are negligible in comparison to the diagonal block matrices. For the purpose of computing the integral in eq. (3.14), it is therefore a good approximation to simply set the off-diagonal block matrices (case 3 in (5.5)) to zero! The first term in the integrand of eq. (3.14) now factorizes over \\((i,j)\\), \\[\\begin{equation} \\Gauss (\\w | \\w^{*}, \\H^{-1}) \\approx \\prod_{1 \\le i &lt; j \\le L} \\Gauss (\\wij | \\wij^{*}, \\H_{ij}^{-1}) , \\end{equation}\\] with the diagonal block matrices are \\((\\H_{ij})_{ab,cd} := (\\H)_{ijab,ijcd}\\). Now the product over all residue indices can be moved in front of the integral and each integral can be performed over \\(\\wij\\) separately, \\[\\begin{eqnarray} p(\\X | \\r) &amp;\\propto&amp; \\int \\prod_{1 \\le i &lt; j \\le L} \\Gauss (\\wij | \\wij^{*}, \\H_{ij}^{-1}) \\prod_{1 \\le i&lt;j \\le L} \\frac{p(\\wij | \\rij)}{\\Gauss(\\wij|\\mathbf{0}, \\lambda_w^{-1} \\I)} d\\w \\\\ p(\\X | \\r) &amp;\\propto&amp; \\int \\prod_{1\\le i&lt;j\\le L} \\left( \\Gauss (\\wij | \\wij^*, \\H_{ij}^{-1}) \\, \\frac{p(\\wij | \\rij)}{\\Gauss(\\wij | \\mathbf{0}, \\lambda_w^{-1} \\I)} \\right) d\\w \\\\ p(\\X | \\r) &amp;\\propto&amp; \\prod_{1\\le i&lt;j\\le L} \\int \\Gauss (\\wij | \\wij^*, \\H_{ij}^{-1}) \\frac{p(\\wij | \\rij)}{\\Gauss (\\wij | \\mathbf{0}, \\lambda_w^{-1} \\I)} d \\wij \\tag{3.15} \\end{eqnarray}\\] Inserting eq. (3.13) yields \\[\\begin{eqnarray} p(\\X | \\r) &amp;\\propto&amp; \\prod_{1\\le i&lt;j\\le L} \\int \\Gauss (\\wij | \\wij^*, \\H_{ij}^{-1}) \\frac{\\sum_{k=0}^K g_{k}(\\rij) \\Gauss(\\wij | \\muk, \\Lk^{-1})}{\\Gauss (\\wij | \\mathbf{0}, \\lambda_w^{-1} \\I)} d \\wij \\\\ p(\\X | \\r) &amp;\\propto&amp; \\prod_{1\\le i&lt;j\\le L} \\sum_{k=0}^K g_{k}(\\rij) \\int \\frac{\\Gauss (\\wij | \\wij^*, \\H_{ij}^{-1})}{\\Gauss (\\wij | \\mathbf{0}, \\lambda_w^{-1} \\I)} \\Gauss(\\wij | \\muk, \\Lk^{-1}) d\\wij \\; . \\tag{3.16} \\end{eqnarray}\\] The integral can be carried out using the following formula: \\[\\begin{equation} \\int d\\seq \\, \\frac{ \\Gauss( \\seq | \\mathbf{\\mu}_1, \\mathbf{\\Lambda}_1^{-1}) }{\\Gauss(\\seq|\\mathbf{0},\\mathbf{\\Lambda}3^{-1})} \\, \\Gauss(\\seq|\\mathbf{\\mu}_2,\\mathbf{\\Lambda}_2^{-1}) = \\\\ \\frac{\\Gauss(\\mathbf{0}| \\mathbf{\\mu}_1, \\mathbf{\\Lambda}_{1}^{-1}) \\Gauss(\\mathbf{0}| \\mathbf{\\mu}_2, \\mathbf{\\Lambda}_{2}^{-1})}{\\Gauss(\\mathbf{0}|\\mathbf{0}, \\mathbf{\\Lambda}_{3}^{-1}) \\Gauss(\\mathbf{0}| \\mathbf{\\mu}_{12}, \\mathbf{\\Lambda}_{123}^{-1})} \\end{equation}\\] with \\[\\begin{eqnarray} \\mathbf{\\Lambda}_{123} &amp;:=&amp; \\mathbf{\\Lambda}_1 - \\mathbf{\\Lambda}_3 + \\mathbf{\\Lambda}_2 \\\\ \\mathbf{\\mu}_{12} &amp;:=&amp; \\mathbf{\\Lambda}_{123}^{-1}(\\mathbf{\\Lambda}_1 \\mathbf{\\mu}_1 + \\mathbf{\\Lambda}_2 \\mathbf{\\mu}_2). \\end{eqnarray}\\] We define \\[\\begin{align} \\Lijk &amp;:= \\H_{ij} - \\lambda_w \\I + \\Lk \\\\ \\muijk &amp;:= \\Lijk^{-1}(\\H_{ij} \\wij^* + \\Lk \\muk) \\,. \\tag{3.17} \\end{align}\\] and obtain \\[\\begin{align} p(\\X | \\r) \\propto \\prod_{1 \\le i &lt; j \\le L} \\sum_{k=0}^K g_{k}(\\rij) \\frac{\\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss(\\mathbf{0} | \\muijk, \\Lijk^{-1})} \\,. \\tag{3.18} \\end{align}\\] \\(\\Gauss( \\mathbf{0} | \\mathbf{0}, \\lambda_w^{-1} \\I)\\) and \\(\\N( \\mathbf{0} | \\wij^*, \\H_{ij}^{-1})\\) are constants that depend only on \\(\\X\\) and \\(\\lambda_w\\) and can be omitted. -->"],
["optimizing-full-likelihood.html", "4 Optimizing the Full-Likelihood", " 4 Optimizing the Full-Likelihood Computing the gradient of the likelihood analytically according to the previous equations is infeasible, because computing \\(p(x_i \\eq a, x_j \\eq b | \\v, \\w) = \\sum_{y_1, \\dots, y_L =1}^{20} p(y_1, \\dots, y_L | \\v, \\w) I(y_i \\eq a, y_j \\eq b)\\) would require summing over \\(20^L\\) sequences \\((y_1,\\ldots,y_L)\\). Several approaches have been used to get around this problem as described in section 1.4.2. The most popular one for protein contact prediction is to optimize the pseudo likelihood instead (see section 1.4.3). Its gradient involves a sum over just the 20 amino acids instead of over all possible sequences of length \\(L\\). It is possible though to optimize the true likelihood by employing an approach called “persistent contrastive divergence” PCD that extends the “contrastive divergence” CD approach by G.E.~Hinton introduced in “Training products of experts by minimizing contrastive divergence”, (2002). In CD, we initialise \\(N\\) Markov chains, one with each of the \\(N\\) sequences from our MSA, and we generate \\(N\\) new samples by a single step of Gibbs sampling from each of the \\(N\\) sequences. From the \\(N\\) new sequences we can estimate the frequencies of pairs \\((x_{i}\\!=\\! a, x_{j}=b)\\) to approximate the second term in , just as the first term is computed from the original \\(N\\) sequences. Even though the approximation for the second term is very bad, it can be seen that this approximate gradient will become zero approximately where the true gradient of the likelihood also becomes zero. To see this, imagine \\((\\v^*, \\w^*)\\) is the maximum of the likelihood. Then, starting from the sequences in the MSA, the Gibbs sampling step should not lead away from the empirical distribution, because the parameters \\((\\v^*, \\w^*)\\) already describe the empirical distribution correctly. This equality of the two maxima is accurate to the extent that the empirical distribution with its finite number of sequences \\(N\\) can represent the true distribution given by parameters \\((\\v^*, \\w^*)\\). Therefore, the larger \\(N\\), the better CD will optimise into the maximum of the true likelihood. It can be shown that CD using a single-step Gibbs sampling is exactly equivalent to optimising the pseudo likelihood. For PCD, the Markov chains are not restarted from the \\(N\\) sequences in the MSA every time a new gradient is computed. Instead the Markov chains are evolved between successive gradient computations without resetting them. This ensures that, as we approach the maximum \\((\\v^*, \\w^*)\\), we acquire more and more samples from the distribution corresponding to parameters \\((\\v,\\w)\\) near the optimum. Hence our approximation to the gradient of the likelihood gets better the longer we sample, independent of the number of sequences \\(N\\) in the MSA. The optimization of the true likelihood with CD and PCD is discussed in section @ref{optimizing-full-likelihood}. 4.0.1 Full-likelihood Dr Stefan Seemayer provided a Python implementation of CCMpred that was extended to optimize the full-likelihood of the MRF. The full likelihood of the maximum entropy model cannot be optimized with ML methods due to the exponential complexity of the partition function (see section 1.4). As elaborated in the introduction, many approximations to maximum likelihood inference have been developed that resolve the computational intractability of the partition function. Pseudo-likelihood methods are now the state-of-the-art model for contact prediction that outperformed other approximations like mean-field methods or methods based on the Bethe-approximation or sparse inverse covariance. Even though pseudo-likelihood maximation has been shown to be a consistent estimator in the limit of infinite data [44], it is not clear how well pseudo-likelihood approximation is for real-world datasets. 4.0.2 Likelihood Gradient 4.0.3 Contrastive Divergence CD is about the difference between the original data set and a perturbed data set perturbed data set : The contrasting data set needs to represent A data sample characteristic of the current PARAMETERS –&gt; Gibbs Sampling starting from data Note: as contrasting dataset towards true_parameters, the elements of the gradient converge to the gradient of the max log likelihood – At the limit of the Markov chain, the CD converges to the actual MLE References "],
["methods.html", "5 Methods", " 5 Methods all you need to know "],
["dataset.html", "5.1 Dataset", " 5.1 Dataset A protein dataset has been constructed from the CATH (v4.1) [50] database for classification of protein domains. All CATH domains from classes 1(mainly \\(\\alpha\\)), 2(mainly \\(\\beta\\)), 3(\\(\\alpha+\\beta\\)) have been selected and filtered for internal redundancy at the sequence level using the pdbfilter script from the HH-suite[51] with an E-value cutoff=0.1. The dataset has been split into ten subsets aiming at the best possible balance between CATH classes 1,2,3 in the subsets. All domains from a given CATH topology (=fold) go into the same subsets, so that any two subsets are non-redundant at the fold level. Some overrepresented folds (e.g. Rossman Fold) have been subsampled ensuring that in every subset each class contains at max 50% domains of the same fold. Consequently, a fold is not allowed to dominate a subset or even a class in a subset. In total there are 6741 domains in the dataset. Multiple sequence alignments were built from the CATH domain sequences (COMBS) using HHblits [51] with parameters to maximize the detection of homologous sequences: hhblits -maxfilt 100000 -realign_max 100000 -B 100000 -Z 100000 -n 5 -e 0.1 -all hhfilter -id 90 -neff 15 -qsc -30 The COMBS sequences are derived from the SEQRES records of the PDB file and sometimes contain extra residues that are not resolved in the structure. Therefore, residues in PDB files have been renumbered to match the COMBS sequences. The process of renumbering residues in PDB files yielded ambigious solutions for 293 proteins, that were removed from the dataset. Another filtering step was applied to remove 80 proteins that do not hold the following properties: more than 10 sequences in the multiple sequence alignment (\\(N&gt;10\\)) protein length between 30 and 600 residues (\\(30 \\leq L \\leq 600\\)) less than 80% gaps in the multiple sequence alignment (percent gaps &lt; 0.8) at least one residue-pair in contact at \\(C_\\beta &lt; 8\\AA\\) and minimum sequence separation of 6 positions The final dataset is comprised of 6368 proteins with almost evenly distributed CATH classes over the ten subsets (Figure 5.1). Figure 5.1: Distribution of CATH classes (1=mainly \\(\\alpha\\), 2=mainly \\(\\beta\\), 3=\\(\\alpha-\\beta\\)) in the dataset and the ten subsets. References "],
["optimizing-pseudo-likelihood.html", "5.2 Optimizing Pseudo-Likelihood", " 5.2 Optimizing Pseudo-Likelihood Dr Stefan Seemayer has reimplementated the open-source software CCMpred [41] in Python. Based on a fork of his private github repository I continued development and extended the software, which is now called CCMpredPy. It will soon be available at https://github.com/soedinglab/CCMpredPy. All computations in this thesis are performed with CCMpredPy unless stated otherwise. 5.2.1 Pseudo-Likelihood Objective Function and its Gradients CCMpred optimizes the regularized negative pseudo-log-likelihood using conjugate gradients optimizer. The negative pseudo-log-likelihood, abbreviated \\(\\mathcal{npll}\\), is defined as: \\[\\begin{equation} \\mathcal{npll}(\\mathbf{X} | \\v,\\w) = - \\sum_{n=1}^N \\sum_{i=1}^L \\left( v_i(x_i^{(n)}) + \\sum_{\\substack{j=1 \\\\ j \\neq i}}^L w_{ij}(x_i^{(n)}, x_j^{(n)}) - \\log Z_i^{(n)} \\right) \\end{equation}\\] The normalization term \\(Z_i\\) sums over all assignments to one position \\(i\\) in sequence: \\[\\begin{equation} Z_i^{(n)} = \\sum_{a=1}^{20} \\exp \\left( v_i(a) + \\sum_{\\substack{j=1 \\\\ j \\neq i}}^L w_{ij}(a, x_j^{(n)}) \\right) \\end{equation}\\] 5.2.2 Differences between CCMpred and CCMpredpy CCMpredPy differs from CCMpred [41] which is available at https://github.com/soedinglab/CCMpred in several details: Initialization of potentials \\(\\v\\) and \\(\\w\\) CCMpred initializes single potentials \\(\\v_i(a) = \\log f_i(a) - \\log f_i(a= &quot;-&quot;)\\) with \\(f_i(a)\\) being the frequency of amino acid a at position i and \\(a=&quot;-&quot;\\) representing a gap. A single pseudo-count has been added before computing the frequencies. Pair potentials \\(\\w\\) are intialized at 0. CCMpredPy initializes single potentials \\(\\v\\) with the ML estimate of single potentials (see section 5.2.5) using amino acid frequencies computed as described in section 5.2.4. Pair potentials \\(\\w\\) are initialized at 0. Regularization CCMpred uses a Gaussian regularization prior centered at zero for both single and pair potentials. The regularization coefficient for single potentials \\(\\lambda_v = 0.01\\) and for pair potentials \\(\\lambda_w = 0.2 * (L-1)\\) with \\(L\\) being protein length. CCMpredPy uses a Gaussian regularization prior centered at zero for the pair potentials. For the single potentials the Gaussian regularization prior is centered at the ML estimate of single potentials (see section 5.2.5) using amino acid frequencies computed as described in section 5.2.4. The regularization coefficient for single potentials \\(\\lambda_v = 10\\) and for pair potentials \\(\\lambda_w = 0.2 * (L-1)\\) with \\(L\\) being protein length. Default settings for CCMpredPy have been chosen to best reproduce CCMpred results. A benchmark over a subset of approximately 3000 proteins confirms that performance measured as PPV for both methods is almost identical (see Figure 5.2). Figure 5.2: Benchmark for CCMpred and CCMpredPy on a dataset of 3124 proteins. ccmpred-vanilla+apc: CCMpred [41] with APC. ccmpred-pll-centerv+apc: CCMpredPy with APC. Specific flags that have been used to run both methods are described in detail in the text (see section 5.2.2). The benchmark in Figure 5.2 as well as all contacts predicted with CCMpred and CCMPredPy (using pseudo-likelihood) in my thesis have been computed using the following flags: Flags used with CCMpredPy (using pseudo-likelihood objective function): --maxit 250 # Compute a maximum of MAXIT operations --center-v # Use a Gaussian prior for single potentials centered at ML estimate v* --reg-l2-lambda-single 10 # regularization coefficient for single potentials --reg-l2-lambda-pair-factor 0.2 # regularization coefficient for pairwise potentials computed as reg-l2-lambda-pair-factor * (L-1) --pc-uniform # use uniform pseudocounts (1/21 for 20 amino acids + 1 gap state) --pc-count 1 # defining pseudo count admixture coefficient rho = pc-count/( pc-count+ Neff) --epsilon 1e-5 # convergence criterion for minimum decrease in the last K iterations --ofn-pll # using pseudo-likelihood as objective function --alg-cg # using conjugate gradient to optimize objective function Flags used with CCMpred: -n 250 # NUMITER: Compute a maximum of NUMITER operations -l 0.2 # LFACTOR: Set pairwise regularization coefficients to LFACTOR * (L-1) -w 0.8 # IDTHRES: Set sequence reweighting identity threshold to IDTHRES -e 1e-5 # EPSILON: Set convergence criterion for minimum decrease in the last K iterations to EPSILON 5.2.3 Sequence Reweighting Sequences in a MSA do not represent independent draws from a probabilistic model. Due to their evolutionary relationships they in fact have a complicated dependence structure. Multiple sequence alignments do not represent iid samples of the sequence space of a protein family. In fact, there is selection bias from sequencing species of special interest (e.g human pathogens) or sequencing closely related species, e.g multiple strains. This uneven sampling of sequence space thus leaves certain regions unexplored whereas others are statistically overrepresented. To reduce the effects of overrepresented sequences, typically a simple weighting strategy [45] is applied that assigns a weight to each sequence that is the inverse of the number of similar sequences according to an identity threshold. It has been found that reweighting improves contact prediction performance [39] significantly but results are robust against the choice of the identity threshold in a range between 0.7 and 0.9 [33]. We chose an identity threshold of 0.8. Every sequence \\(x_n\\) of length \\(L\\) in an alignment with \\(N\\) sequences has an associated weight \\(w_n = 1/m_n\\), where \\(m_n\\) represents the number of similar sequences: \\[\\begin{equation} w_n = \\frac{1}{m_n}, m_n = \\sum_{m=1}^N I \\left( ID(x_n, x_m) \\geq 0.8 \\right) \\\\ ID(x_n, x_m)=\\frac{1}{L} \\sum_{i=1}^L I(x_n^i = x_m^i) \\tag{5.1} \\end{equation}\\] The number of effective sequences \\(\\mathbf{\\neff}\\) of an alignment is then the number of sequence clusters computed as: \\[\\begin{equation} \\neff = \\sum_{n=1}^N w_n \\tag{5.2} \\end{equation}\\] 5.2.4 Computing Amino Acid Frequencies Single and pairwise amino acid frequencies are computed from the alignment by weighting amino acid counts (see section 5.2.3) and adding pseudocounts for numerical stability. Let \\(a,b \\in \\{1,\\ldots,20\\}\\) be amino acids, \\(q(x_i=a), q(x_i=a, x_j=b)\\) and \\(q_0(x_i=a), q_0(x_i=a,x_j=b)\\) be the empirical single and pair frequencies with and without pseudocounts, respectively. We define \\[\\begin{align} q(x_i \\eq a) :=&amp; (1-\\tau) \\; q_0(x_i \\eq a) + \\tau \\tilde{q}(x_i\\eq a) \\\\ q(x_i \\eq a, x_j \\eq b) :=&amp; (1-\\tau)^2 \\; [ q_0(x_i \\eq a, x_j \\eq b) - q_0(x_i \\eq a) q_0(x_j \\eq b) ] + \\\\ &amp; q(x_i \\eq a) \\; q(x_j \\eq b) \\tag{5.3} \\end{align}\\] with \\(\\tilde{q}(x_i \\eq a) := f(a)\\) being background amino acid frequencies and \\(\\tau \\in [0,1]\\) is a pseudocount admixture coefficient, which is a function of the diversity of the multiple sequence alignment: \\[\\begin{equation} \\tau = \\frac{N_\\mathrm{pc}}{(N_\\mathrm{eff} + N_\\mathrm{pc})} \\tag{5.4} \\end{equation}\\] where \\(N_{pc} &gt; 0\\). The formula for \\(q(x_i \\eq a, x_j \\eq b)\\) in the second line in eq (5.3) was chosen such that for \\(\\tau \\eq0\\) we obtain \\(q(x_i \\eq a, x_j \\eq b) = q_0(x_i \\eq a, x_j \\eq b)\\), and furthermore \\(q(x_i \\eq a, x_j \\eq b) = q(x_i \\eq a) q(x_j \\eq b)\\) exactly if \\(q_0(x_i \\eq a, x_j \\eq b) = q_0(x_i \\eq a) q_0(x_j \\eq b)\\). 5.2.5 Regularization As the model is overparameterized, regularization is an alternative solution compared to choosing a gauge. Furthermore it helps preventing overfitting. L2-regularization which corresponds to using a Gaussian prior, has proven to work better than L1 regularization [???]. \\[\\begin{equation} R(\\v, \\w) = \\mathcal{N}(\\v | \\vec{0}, \\lambda_v \\I^{-1}) + \\mathcal{N}(\\w | \\vec{0}, \\lambda_w \\I^{-1}) \\end{equation}\\] \\[\\begin{align} \\mathcal{N}(\\v | \\vec{0}, \\lambda_v \\I^{-1}) &amp;= \\lambda_v ||\\v||_2^2 \\\\ &amp;= \\frac{\\lambda_v}{2} \\sum_{i=1}^L \\sum_{a=1}^{20} \\via^2 \\end{align}\\] \\[\\begin{align} \\mathcal{N}(\\w | \\vec{0}, \\lambda_w \\I^{-1}) &amp;= \\lambda_w ||\\w||_2^2 \\\\ &amp;= \\frac{\\lambda_w}{2} \\sum_{i=1}^L \\sum_{\\substack{j=1 \\\\ i \\neq j}}^L \\sum_{a,b=1}^{20} \\wijab^2 \\end{align}\\] However, it makes sense to use a Gaussian prior for single emission potentials that is centered at the ML estimate of the single potentials. Consider, ….. \\[\\begin{align} \\mathcal{N}(\\v | \\v^{*}, \\lambda_v \\I^{-1}) &amp;= \\lambda_v ||\\v - \\v^{*}||_2^2 \\\\ &amp;= \\frac{\\lambda_v}{2} \\sum_{i=1}^L \\sum_{a=1}^{20} (\\via - \\via^{*})^2 \\end{align}\\] \\[\\begin{equation} \\via^* = \\log q(x_i=a) - \\frac{1}{20} \\sum_{a&#39;=1}^{20} \\log q(x_i=a&#39;) \\end{equation}\\] References "],
["analysis-of-coupling-matrices.html", "5.3 Analysis of Coupling Matrices", " 5.3 Analysis of Coupling Matrices 5.3.1 Correlation of Couplings with Contact Class Approximately 100000 residue pairs have been filtered for contacts and non-contacts respectively according to the following criteria: consider only residue pairs separated by at least 10 positions in sequence minimal diversity (\\(=\\frac{\\sqrt{N}}{L}\\)) of alignment = 0.3 minimal number of non-gapped sequences = 1000 \\(\\Cb\\) distance threshold for contact: \\(&lt;8\\AA\\) \\(\\Cb\\) distance threshold for noncontact: \\(&gt;25\\AA\\) 5.3.2 Coupling Distribution Plots For one-dimensional coupling distribution plots the residue pairs and respective pseudo-log-likelihood coupling values \\(\\wijab\\) have been selected as follows: consider only residue pairs separated by at least 10 positions in sequence discard residues that have more than 30% gaps in the alignment discard residue pairs that have insufficient evidence in the alignment: \\(N_{ij} \\cdot q_i(a) \\cdot q_j(b) &lt; 100\\) with: \\(N_{ij}\\) is the number of sequences with neither a gap at position i nor at position j \\(q_i(a)\\) and \\(q_j(b)\\) are the frequencies of amino acids a and b at positions i and j (computed as described in section 5.2.4) The same criteria have been applied for selecting couplings for the two-dimensional distribution plots with the difference that evidence for a single coupling term has to be \\(N_{ij} \\cdot q_i(a) \\cdot q_j(b) &lt; 80.\\) 5.3.3 Bayesian Model for Residue-Resdiue Contact Prediction "],
["Hessian-offdiagonal.html", "5.4 Off-diagonal elements in \\(\\H\\)", " 5.4 Off-diagonal elements in \\(\\H\\) "],
["neg-Hessian-computation.html", "5.5 Efficiently Computing the negative Hessian of the regularized log-likelihood", " 5.5 Efficiently Computing the negative Hessian of the regularized log-likelihood Surprisingly, the elements of the Hessian at the mode \\(\\w^*\\) are easy to compute. Let \\(i,j,k,l \\in \\{1,\\ldots,L\\}\\) be columns in the MSA and let \\(a, b, c, d \\in \\{1,\\ldots,20\\}\\) represent amino acids. The partial derivative \\(\\partial / \\partial \\w_{klcd}\\) of the second term in the gradient of the couplings in eq. (3.4) is \\[\\begin{eqnarray} \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\wklcd \\, \\partial \\wijab } &amp;=&amp; - \\sum_{n=1}^{N} \\, \\sum_{\\mathbf{y} \\in \\Sn} \\frac{\\partial \\left( \\frac{\\exp \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L}^L w_{ij}(y_i,y_j) \\right) }{Z_n(\\v,\\w)} \\right)}{\\partial \\wklcd} I(y_i \\eq a, y_j \\eq b) \\\\ &amp;&amp;- \\lambda_w \\delta_{ijab,klcd} \\,, \\end{eqnarray}\\] where \\(\\delta_{ijab,klcd} = I(ijab=klcd)\\) is the Kronecker delta. Applying the product rule, we find \\[\\begin{eqnarray} \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\wklcd \\, \\partial \\wijab } &amp;=&amp; - \\sum_{n=1}^{N} \\, \\sum_{\\mathbf{y} \\in \\Sn} \\frac{\\exp \\left(\\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L}^L w_{ij}(y_i,y_j) \\right)}{Z_n(\\v,\\w)} I(y_i \\eq a, y_j \\eq b) \\\\ &amp; \\times &amp; \\left[ \\frac{\\partial}{\\partial \\wklcd} \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L} w_{ij}(y_i,y_j) \\right) - \\frac{1}{Z_n(\\v,\\w)} \\frac{\\partial Z_n(\\v,\\w) }{\\partial\\wklcd} \\right] \\\\ &amp;-&amp; \\lambda_w \\delta_{ijab,klcd} \\\\ \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\wklcd \\, \\partial \\wijab } &amp;=&amp; - \\sum_{n=1}^{N} \\, \\sum_{\\mathbf{y} \\in \\Sn} \\frac{\\exp \\left(\\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L}^L w_{ij}(y_i,y_j) \\right)}{Z_n(\\v,\\w)} I(y_i \\eq a, y_j \\eq b) \\\\ &amp; \\times &amp; \\left[ I(y_k \\eq c, y_l \\eq d) - \\frac{\\partial}{\\partial \\wklcd} \\log Z_n(\\v,\\w) \\right] \\\\ &amp;-&amp; \\lambda_w \\delta_{ijab,klcd} \\,. \\end{eqnarray}\\] We simplify this expression using \\[\\begin{equation} p(\\mathbf{y} | \\v,\\w) = \\frac{\\exp \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L} w_{ij}(y_i,y_j) \\right)}{Z_n(\\v,\\w)} , \\end{equation}\\] yielding \\[\\begin{eqnarray} \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\wklcd \\, \\partial \\wijab} &amp;=&amp; - \\sum_{n=1}^{N} \\, \\sum_{\\mathbf{y} \\in \\Sn} p(\\mathbf{y} | \\v,\\w) \\, I(y_i \\eq a, y_j \\eq b, y_k \\eq c, y_l \\eq d) \\\\ &amp;+&amp; \\sum_{n=1}^{N} \\, \\sum_{\\mathbf{y} \\in \\mathcal{S}_n} p(\\mathbf{y} | \\v,\\w) \\, I(y_i \\eq a, y_j \\eq b ) \\sum_{\\mathbf{y} \\in \\Sn} p(\\mathbf{y} | \\v,\\w) I(y_k \\eq c, y_l \\eq d ) \\\\ &amp;-&amp; \\lambda_w \\delta_{ijab,klcd} \\,. \\end{eqnarray}\\] If \\(\\X\\) does not contain too many gaps, this expression can be approximated by \\[\\begin{eqnarray} \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\wklcd \\, \\partial \\wijab } &amp;=&amp; - N_{ijkl} \\: p(x_i \\eq a, x_j \\eq b, x_k \\eq c, x_l \\eq d | \\v,\\w) \\nonumber \\\\ &amp;&amp; + N_{ijkl} \\: p(x_i \\eq a, x_j \\eq b | \\v,\\w) \\, p(x_k \\eq c, x_l \\eq d | \\v,\\w) - \\lambda_w \\delta_{ijab,klcd} \\,, \\end{eqnarray}\\] where \\(N_{ijkl}\\) is the number of sequences that have a residue in \\(i\\), \\(j\\), \\(k\\) and \\(l\\). Looking at three cases separately: case 1: \\((k,l) = (i,j)\\) and \\((c,d) = (a,b)\\) case 2: \\((k,l) = (i,j)\\) and \\((c,d) \\ne (a,b)\\) case 3: \\((k,l) \\ne (i,j)\\) and \\((c,d) \\ne (a,b)\\), the elements of \\(\\H\\), which are the negative second partial derivatives of \\(\\LLreg(\\v^*,\\w)\\) with respect to the components of \\(\\w\\), are \\[\\begin{eqnarray} \\mathrm{case~1:} (\\H)_{ijab, ijab} &amp;=&amp; N_{ij} \\, p(x_i \\eq a, x_j \\eq b| \\v^*,\\w^*) \\, ( 1 - p(x_i \\eq a, x_j \\eq b| \\v^*,\\w^*) \\,) \\\\ &amp;&amp; + \\lambda_w \\\\ \\mathrm{case~2:} (\\H)_{ijcd, ijab} &amp;=&amp; - N_{ij} \\, p(x_i \\eq a, x_j \\eq b |\\v^*,\\w^*) \\, p(x_i \\eq c, x_j \\eq d |\\v^*,\\w^*) \\\\ \\mathrm{case~3:} (\\H)_{klcd, ijab} &amp;=&amp; N_{ijkl} \\, p(x_i \\eq a, x_j \\eq b, x_k \\eq c, x_l \\eq d | \\v^*,\\w^*) \\nonumber \\\\ &amp;&amp; - N_{ijkl} \\, p(x_i \\eq a, x_j \\eq b | \\v^*,\\w^*)\\, p(x_k \\eq c, x_l \\eq d | \\v^*,\\w^*) \\,. \\tag{5.5} \\end{eqnarray}\\] We know from eq. (3.5) that at the mode \\(\\w^*\\) the model probabilities match the empirical frequencies up to a small regularization term, \\[\\begin{equation} p(x_i \\eq a, x_j \\eq b | \\v^*,\\w^*) = q(x_i \\eq a, x_j \\eq b) - \\frac{\\lambda_w}{N_{ij}} \\wijab^* \\,, \\end{equation}\\] and therefore the negative Hessian elements in cases 1 and 2 can be expressed as \\[\\begin{align} (\\H)_{ijab, ijab} =&amp; N_{ij} \\left( q(x_i \\eq a, x_j \\eq b) - \\frac{\\lambda_w}{N_{ij}} \\wijab^* \\right) \\left( 1 - q(x_i \\eq a, x_j \\eq b) +\\frac{\\lambda_w}{N_{ij}} \\wijab^* \\right) \\\\ &amp; + \\lambda_w \\\\ (\\H)_{ijcd, ijab} =&amp; -N_{ij} \\left(\\,q(x_i \\eq a, x_j \\eq b) - \\frac{\\lambda_w}{N_{ij}} \\wijab^* \\right) \\left( q(x_i \\eq c, x_j \\eq d) -\\frac{\\lambda_w}{N_{ij}} \\wijcd^* \\right) . \\tag{5.6} \\end{align}\\] In order to write the previous eq. (5.6) in matrix form, the regularised empirical frequencies \\(\\qij\\) will be defined as \\[\\begin{equation} (\\qij)_{ab} = q&#39;_{ijab} := q(x_i \\eq a, x_j \\eq b) - \\lambda_w \\wijab^* / N_{ij} \\,, \\end{equation}\\] and the \\(400 \\times 400\\) diagonal matrix \\(\\Qij\\) will be defined as \\[\\begin{equation} \\Qij := \\text{diag}(\\qij) \\; . \\end{equation}\\] Now eq. (5.6) can be written in matrix form \\[\\begin{equation} \\H_{ij} = N_{ij} \\left( \\Qij - \\qij \\qij^{\\mathrm{T}} \\right) + \\lambda_w \\I \\; . \\tag{5.7} \\end{equation}\\] "],
["inv-lambda-ij-k.html", "5.6 Efficiently Computing the Inverse of Matrix \\(\\Lijk\\)", " 5.6 Efficiently Computing the Inverse of Matrix \\(\\Lijk\\) It is possible to efficiently invert the matrix \\(\\Lijk = \\H_{ij} - \\lambda_w \\I + \\Lambda_k\\), that is introduced in 3.8.1 where \\(\\H_{ij}\\) is the \\(400 \\times 400\\) diagonal block submatrix \\((\\H_{ij})_{ab,cd} := (\\H)_{ijab,ijcd}\\) and \\(\\Lambda_k\\) is an invertible diagonal precision matrix that is introduced in section ??. Equation (5.7) can be used to write \\(\\Lijk\\) in matrix form as \\[\\begin{equation} \\Lijk = \\H_{ij} - \\lambda_w \\I + \\Lk = N_{ij} \\Qij- N_{ij} \\qij \\qij^{\\mathrm{T}} + \\Lk \\,. \\tag{5.8} \\end{equation}\\] Owing to eqs. (3.2) and (3.6), \\(\\sum_{a,b=1}^{20} q&#39;_{ijab} = 1\\). The previous equation (5.8) facilitates the calculation of the inverse of this matrix using the Woodbury identity for matrices \\[\\begin{equation} (\\mathbf{A} + \\mathbf{B} \\mathbf{D}^{-1} \\mathbf{C})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{B} (\\mathbf{D} + \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B}) ^{-1} \\mathbf{C} \\mathbf{A}^{-1} \\;. \\end{equation}\\] by setting \\[\\begin{align} \\mathbf{A} &amp;= N_{ij} \\Qij + \\Lk \\\\ \\mathbf{B} &amp;= \\qij \\\\ \\mathbf{C} &amp;= \\qij^\\mathrm{T} \\\\ \\mathbf{D} &amp;=- N_{ij}^{-1} \\\\ \\end{align}\\] \\[\\begin{align} \\left( \\H_{ij} - \\lambda_w \\I + \\Lk \\right)^{-1} &amp; = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\qij \\left( -N_{ij}^{-1} + \\qij^\\mathrm{T} \\mathbf{A}^{-1} \\qij \\right)^{-1} \\qij^\\mathrm{T} \\mathbf{A}^{-1} \\\\ &amp; = \\mathbf{A}^{-1} + \\frac{ (\\mathbf{A}^{-1} \\qij) (\\mathbf{A}^{-1} \\qij)^{\\mathrm{T}} }{ N_{ij}^{-1} - \\qij^\\mathrm{T} \\mathbf{A}^{-1} \\qij} \\,. \\tag{5.9} \\end{align}\\] Note that \\(\\mathbf{A}\\) is diagonal as \\(\\Qij\\) and \\(\\Lk\\) are diagonal matrices: \\(\\mathbf{A} = \\text{diag}(N_{ij} q&#39;_{ijab} + (\\Lk)_{ab,ab})\\). Moreover, \\(\\mathbf{A}\\) has only positive diagonal elements, because \\(\\Lk\\) is invertible and has only positive diagonal elements and because \\(q&#39;_{ijab} = p(x_i \\eq a, x_j \\eq b | \\v^*,\\w^*) \\ge 0\\). Therefore \\(\\mathbf{A}\\) is invertible: \\(\\mathbf{A}^{-1} = \\text{diag}(N_{ij} q&#39;_{ijab} + (\\Lk)_{ab,ab} )^{-1}\\). Because \\(\\sum_{a,b=1}^{20} q&#39;_{ijab} = 1\\), the denominator of the second term is \\[\\begin{equation} N_{ij}^{-1} - \\sum_{a,b=1}^{20} \\frac{{q&#39;}_{ijab}^2}{N_{ij} q&#39;_{ijab} + {(\\Lk)}_{ab,ab} } &gt; N_{ij}^{-1} - \\sum_{a,b=1}^{20} \\frac{{q&#39;}^2_{ijab}}{N_{ij} q&#39;_{ijab}} = 0 \\end{equation}\\] and therefore the inverse of \\(\\Lijk\\) in eq. (5.9) is well defined. The log determinant of \\(\\Lijk\\) is necessary to compute the ratio of Gaussians (see equation (??)) and can be computed using the matrix determinant lemma: \\[\\begin{equation} \\det(\\mathbf{A} + \\mathbf{uv}^\\mathrm{T}) = (1+\\mathbf{v}^\\mathrm{T} \\mathbf{A}^{-1} \\mathbf{u}) \\det(\\mathbf{A}) \\end{equation}\\] Setting \\(\\mathbf{A} = N_{ij} \\Qij + \\Lk\\) and \\(\\v = \\qij\\) and \\(\\mathbf{u} = - N_{ij} \\qij\\) yields \\[\\begin{equation} \\det(\\Lijk ) = \\det(\\H_{ij} - \\lambda_w \\I + \\Lk) = (1 - N_{ij}\\qij^\\mathrm{T} \\mathbf{A}^{-1}\\qij) \\det(\\mathbf{A}) \\,. \\end{equation}\\] \\(\\mathbf{A}\\) is diagonal and has only positive diagonal elements so that \\(\\log(\\det(\\mathbf{A})) = \\sum \\log \\left( \\text{diag}(\\mathbf{A}) \\right)\\). "],
["modelling-the-dependence-of-wij-on-distance.html", "5.7 Modelling the dependence of \\(\\wij\\) on distance", " 5.7 Modelling the dependence of \\(\\wij\\) on distance It is straightforward to extend the model presented in 3.8.1 for distances. The mixture weights \\(g_k(\\rij)\\) in eq. (3.13) are modelled as softmax over linear functions \\(\\gamma_k(\\rij)\\) (Figure ): \\[\\begin{align} g_k(\\rij) &amp;= \\frac{\\exp \\gamma_k(\\rij)}{\\sum_{k&#39;=0}^K \\exp \\gamma_{k&#39;}(\\rij)} \\, , \\nonumber \\\\ \\gamma_k(\\rij) &amp;= - \\sum_{k&#39;=0}^{k} \\alpha_{k&#39;} ( \\rij - \\rho_{k&#39;}) . (\\#eq:definition-mixture-weights} \\end{align}\\] (ref:caption-softmax_linear_fct) The Gaussian mixture coefficients \\(g_k(\\rij)\\) of \\(p(\\wij|\\rij)\\) are modelled as softmax over linear functions \\(\\gamma_k(\\rij)\\). \\(\\rho_k\\) sets the transition point between neighbouring components \\(g_{k-1}(\\rij)\\) and \\(g_k(\\rij)\\), while \\(\\alpha_k\\) quantifies the abruptness of the transition between \\(g_{k-1}(\\rij)\\) and \\(g_k(\\rij)\\). (#fig:softmax_linear_fct)(ref:caption-softmax_linear_fct) The functions \\(g_k(\\rij)\\) remain invariant when adding an offset to all \\(\\gamma_k(\\rij)\\). This degeneracy can be removed by setting \\(\\gamma_0(\\rij) = 0\\) (i.e., \\(\\alpha_0 = 0\\) and \\(\\rho_0=0\\)). Further, the components are ordered, \\(\\rho_1&gt; \\ldots &gt; \\rho_K\\) and it is demanded that \\(\\alpha_k &gt; 0\\) for all \\(k\\). This ensures that for \\(\\rij \\rightarrow \\infty\\) we will obtain \\(g_0(\\rij) \\rightarrow 1\\) and hence \\(p(\\w | \\X) \\rightarrow \\Gauss(0, \\sigma_0^2 \\I )\\). The parameters \\(\\rho_k\\) mark the transition points between the two Gaussian mixture components \\(k-1\\) and \\(k\\), i.e., the points at which the two components obtain equal weights. This follows from \\(\\gamma_k(\\rij) - \\gamma_{k-1}(r) = \\alpha_{t} ( \\rij - \\rho_{t})\\) and hence \\(\\gamma_{k-1}(\\rho_k) = \\gamma_k(\\rho_k)\\). A change in \\(\\rho_k\\) or \\(\\alpha_k\\) only changes the behaviour of \\(g_{k-1}(\\rij)\\) and \\(g_k(\\rij)\\) in the transition region around \\(\\rho_k\\). Therefore, this particular definition of \\(\\gamma_k(\\rij)\\) makes the parameters \\(\\alpha_k\\) and \\(\\rho_k\\) as independent of each other as possible, rendering the optimisation of these parameters more efficient. "],
["training-the-hyperparameters-muk-and-lk.html", "5.8 Training the Hyperparameters \\(\\muk\\) and \\(\\Lk\\)", " 5.8 Training the Hyperparameters \\(\\muk\\) and \\(\\Lk\\) The model parameters \\(\\mathbf{\\mu} = (\\mathbf{\\mu}_{1},\\ldots,\\mathbf{\\mu}_K)\\), \\(\\mathbf{\\Lambda} = (\\mathbf{\\Lambda}_1,\\ldots,\\mathbf{\\Lambda}_K)\\) will be trained by maximizing the logarithm of the full likelihood over a set of training MSAs \\(\\X^1,\\ldots,\\X^N\\) and associated structures with distance vectors \\(\\r^1,\\ldots,\\r^N\\) plus a regularizer \\(R(\\mathbf{\\mu}, \\mathbf{\\Lambda})\\): \\[\\begin{equation} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}) + R(\\mathbf{\\mu}, \\mathbf{\\Lambda}) = \\sum_{n=1}^N \\log p(\\X^n | \\r^n, \\mathbf{\\mu}, \\mathbf{\\Lambda} ) + R(\\mathbf{\\mu}, \\mathbf{\\Lambda}) \\rightarrow \\max \\, . \\end{equation}\\] The regulariser penalizes values that deviate too far from zero and values for: \\[\\begin{align} R(\\mathbf{\\mu}, \\mathbf{\\Lambda}) = -\\frac{1}{2 \\sigma_{\\mu}^2} \\sum_{k=1}^K \\sum_{ab=1}^{400} \\mu_{k,ab}^2 -\\frac{1}{2 \\sigma_\\text{diag}^2} \\sum_{k=1}^K \\sum_{ab=1}^{400} \\Lambda_{k,ab,ab}^2 \\tag{5.10} \\end{align}\\] Reasonable values are \\(\\sigma_{\\mu}=0.1\\), \\(\\sigma_\\text{diag} = 100\\). The log likelihood can be optimized using LBFG-S-B[???], which requires the computation of the gradient of the log likelihood. For simplicity of notation, the following calculations consider the contribution of the log likelihood for just one protein, which allows to drop the index \\(n\\) in \\(\\rij^n\\), \\((\\wij^n)^*\\) and \\(\\Hij^n\\). From eq. (3.18) the log likelihood for a single protein is \\[\\begin{equation} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}) = \\sum_{1 \\le i &lt; j \\le L} \\log \\sum_{k=0}^K g_{k}(\\rij) \\frac{\\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss(\\mathbf{0} | \\muijk, \\Lijk^{-1})} + R(\\mathbf{\\mu}, \\mathbf{\\Lambda}) + \\text{const.}\\,. \\end{equation}\\] 5.8.1 The gradient of the log likelihood with respect to \\(\\mathbf{\\mu}\\) By applying the formula \\(d f(x) / dx = f(x) \\, d \\log f(x) / dx\\) to the previous equation one obtains \\[\\begin{equation} \\frac{\\partial}{\\partial \\mu_{k,ab}} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}) = \\sum_{1\\le i&lt;j\\le L} \\frac{ g_{k}(\\rij) \\frac{ \\Gauss ( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\frac{\\partial}{\\partial \\mu_{k,ab}} \\log \\left( \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\right) } { \\sum_{k&#39;=0}^K g_{k&#39;}(\\rij) \\, \\frac{ \\Gauss(\\mathbf{0} | \\muk&#39;, \\Lk&#39;^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} } . \\tag{5.11} \\end{equation}\\] To simplify this expression, we define the responsibility of component \\(k\\) for the posterior distribution of \\(\\wij\\), the probability that \\(\\wij\\) has been generated by component \\(k\\): \\[\\begin{align} p(k|ij) = \\frac{ g_{k}(\\rij) \\frac{ \\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss(\\mathbf{0} | \\muijk, \\Lijk^{-1})} } {\\sum_{k&#39;=0}^K g_{k&#39;}(\\rij) \\frac{ \\Gauss(\\mathbf{0} | \\muk&#39;, \\Lk&#39;^{-1})}{\\Gauss( \\mathbf{0} | \\muijk&#39;, \\Lijk&#39;^{-1})} } \\,. \\tag{5.12} \\end{align}\\] By substituting the definition for responsibility, (5.11) simplifies \\[\\begin{equation} \\frac{\\partial}{\\partial \\mu_{k,ab}} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}) = \\sum_{1\\le i&lt;j\\le L} p(k | ij) \\frac{\\partial}{\\partial \\mu_{k,ab}} \\log \\left( \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\right) , \\end{equation}\\] and analogously for partial derivatives with respect to \\(\\Lambda_{k,ab,cd}\\). The partial derivative inside the sum can be written \\[\\begin{equation} \\frac{\\partial}{\\partial \\mu_{k,ab}} \\log \\left( \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\right) = \\frac{1}{2} \\frac{\\partial}{\\partial \\mu_{k,ab}} \\left( \\log | \\Lk | - \\muk^\\mathrm{T} \\Lk \\muk - \\log | \\Lijk | + \\muijk^\\mathrm{T} \\Lijk \\muijk \\right)\\,. \\end{equation}\\] Using the following formula for a matrix \\(\\mathbf{A}\\), a real variable \\(x\\) and a vector \\(\\mathbf{y}\\) that depends on \\(x\\), \\[\\begin{equation} \\frac{\\partial}{\\partial x} \\left( \\mathbf{y}^\\mathrm{T} \\mathbf{A} \\mathbf{y} \\right) = \\frac{\\partial \\mathbf{y}^\\mathrm{T}}{\\partial x} \\mathbf{A} \\mathbf{y} + \\mathbf{y}^\\mathrm{T} \\mathbf{A} \\frac{\\partial \\mathbf{y}}{\\partial x} = \\mathbf{y}^\\mathrm{T} (\\mathbf{A} + \\mathbf{A}^\\mathrm{T}) \\frac{\\partial \\mathbf{y}}{\\partial x} \\end{equation}\\] the partial derivative therefore becomes \\[\\begin{align} \\frac{\\partial}{\\partial \\mu_{k,ab}} \\log \\left( \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\right) =&amp; \\left( -\\muk^\\mathrm{T} \\Lk \\mathbf{e}_{ab} \\, + \\muijk^\\mathrm{T} \\Lijk \\Lijk^{-1} \\Lk \\mathbf{e}_{ab} \\right) \\\\ =&amp; \\mathbf{e}^\\mathrm{T}_{ab} \\Lk ( \\muijk - \\muk ) \\; . \\end{align}\\] Finally, the gradient of the log likelihood with respect to \\(\\mathbf{\\mu}\\) becomes \\[\\begin{align} \\nabla_{\\muk} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}) = \\sum_{1\\le i&lt;j\\le L} p(k|ij) \\, \\Lk \\left( \\muijk - \\muk \\right) \\; . \\tag{5.13} \\end{align}\\] "],
["training-the-hyperparameters-rho-k-and-alpha-k-for-distance-dependent-prior.html", "5.9 Training the Hyperparameters \\(\\rho_k\\) and \\(\\alpha_k\\) for distance-dependent prior", " 5.9 Training the Hyperparameters \\(\\rho_k\\) and \\(\\alpha_k\\) for distance-dependent prior "],
["abbrev.html", "A Abbreviations", " A Abbreviations APC Avarage Product Correction CASP Critical Assessment of protein Structure Prediction CD Contrastive Divergence DCA Direct Coupling Analysis DI Direct Information EM electron microscopy IDP intrinsically disordered proteins ML Maximum-Likelihood MRF Markov-Random Field MSA Multiple Sequence Alignment PCD Persistent Contrastive Divergence PDB protein data bank %%%% used as: MRF "],
["dataset-properties.html", "B Dataset Properties", " B Dataset Properties The following figures display various statistics about the dataset used throughout this thesis. See section 5.1 for information on how this dataset has been generated. "],
["alignment-diversity.html", "B.1 Alignment Diversity", " B.1 Alignment Diversity Figure B.1: Distribution of alignment diversity (\\(=\\sqrt(\\frac{N}{L})\\)) in the dataset an its ten subsets. "],
["proportion-of-gaps-in-alignment.html", "B.2 Proportion of Gaps in Alignment", " B.2 Proportion of Gaps in Alignment Figure B.2: Distribution of gap percentage of alignments in the dataset an its ten subsets. "],
["alignment-size-number-of-sequences.html", "B.3 Alignment Size (number of sequences)", " B.3 Alignment Size (number of sequences) Figure B.3: Distribution of alignment size (number of sequences N) in the dataset an its ten subsets. "],
["protein-length.html", "B.4 Protein Length", " B.4 Protein Length Figure B.4: Distribution of protein length L in the dataset an its ten subsets. -->"],
["amino-acid-interaction-preferences-reflected-in-coupling-matrices.html", "C Amino Acid Interaction Preferences Reflected in Coupling Matrices ", " C Amino Acid Interaction Preferences Reflected in Coupling Matrices "],
["pi-cation.html", "C.1 Pi-Cation interactions", " C.1 Pi-Cation interactions Figure C.1 shows a Tyrosine and a Lysine residue forming a cation-\\(\\pi\\) interaction in protein 2ayd. The corresponding coupling matrix in figure C.2 reflects the strong interaction preference. Figure C.1: Tyrosing (residue 37) and Lysine (residue 48) forming a cation-\\(\\pi\\) interaction in protein 2ayd. Figure C.2: Coupling Matrix for residue pair i=37 and j=48 of PDB 2ayd chain A domain 1. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis represent single potentials for residue i=37 and bars at the y-axis represent single potentials for residue j=48. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values. "],
["disulfide.html", "C.2 Disulfide Bonds", " C.2 Disulfide Bonds Figure C.3 shows two cysteine residues forming a covalent disulfide bond in protein 1alu. The corresponding coupling matrix in figure C.4 reflects the strong interaction preference of cysteines. Figure C.3: Two cystein residues (residues 54 and 64) forming a covalent disulfide bond in protein 1alu. Figure C.4: Coupling Matrix for residue pair i=54 and j=64 of PDB 1alu chain A. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis represent single potentials for residue i=54 and bars at the y-axis represent single potentials for residue j=64. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values. "],
["aromatic-proline.html", "C.3 Aromatic-Proline Interactions", " C.3 Aromatic-Proline Interactions Figure @ref(fig:coupling-matrix-aromatic-proline-pymol )shows a proline and a tryptophan residue forming such a CH/\\(\\pi\\) interaction in protein 1aol. The corresponding coupling matrix in figure C.6 reflects this interaction with strong positive coupling between proline and tryptophan. Figure C.5: Proline and tryptophan (residues 17 and 34) stacked on top of each otherengaging in a CH/\\(\\pi\\) interaction in protein 1alu. Figure C.6: Coupling Matrix for residue pair i=17 and j=34 of PDB 1aol chain A. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis represent single potentials for residue i=17 and bars at the y-axis represent single potentials for residue j=34. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values. "],
["aromatic-network.html", "C.4 Network-like structure of aromatic residues", " C.4 Network-like structure of aromatic residues Figure C.7: Network-like structure of aromatic residues in the protein core. 80% of aromatic residues are involved in such networks that are important for protein stability [13]. --> References "],
["references.html", "References", " References "]
]
