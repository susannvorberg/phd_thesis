[
["index.html", "PhD thesis: residue-residue contact prediction Summary", " PhD thesis: residue-residue contact prediction Susann Vorberg 2017-10-03 Summary Awesome contact prediction project abstract "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements I thank the world. "],
["general-intro.html", "1 Introduction", " 1 Introduction In his Nobel Prize speech in 1973 [1] Anfinsen postulated one of the basic principles in molecular biology, which is known as Anfinsen’s dogma: a protein’s native structure is uniquely determined by its amino acid sequence. With certain exceptions (e.g. IDP [2]), this dogma has proven to hold true for the majority of proteins. Ever since, it is regarded as the biggest challenge in structural bioinformatics [3], to realiably predict a protein’s structure given only its amino acid sequence. De-novo protein structure prediction methods use physical or knowledge based energy potentials to find a protein conformation that minimizes the protein’s energy landscape. However, these methods are limited by the complexity of the conformational space and the accuracy of the energy potentials. Considering a protein with 150 amino acids, that has approximately 450 degrees of freedom, Regarding the rotational and translational degrees of freedom of the protein chain, the complexity scales with XXX [1]. Far more successfull are template-based modelling approaches. Given the observation that structure is more conserved than sequence in a protein family [4], the structure of a target protein can be inferred from a homologue protein [5]. The degree of structural conservation is linked to the level of pairwise sequence identity [6]. Therefore, the accuracy of a model crucially depends on the sequence identity between target and template and determines the applicability of the model [7]. By definition, homology derived models are unable to capture new folds [8] and their main limitation lies in the availability of suitable templates. Figure 1.1: Yearly growth of number of solved structures in the PDB[9] and protein sequences in the Uniprot[10]. Unfortunately, the number of solved protein structures increases only slowly, as experimental methods are both time consuming and expensive [8]. The PDB[9] is the main repository for marcomolecular structures and currently (Jul 2017) holds about 120 000 atomic models of proteins. The primary technique for determining protein structures is X-ray crystallography, accounting for roughly 90% of entries in the PDB. About 9% of protein structures have been solved using NMR and less than 1% using EM (see FIG 1). All three experimental techniques have advantages and limitations with respect to certain modelling aspects. X-ray crystallography requires the protein to form crystals, which is an arduous and sometimes impossible task. Furthermore, crystal packing forces the protein into a unnatural and rigid environment preventing the observation of conformational flexibility. NMR studies the protein in an physiological environment in solution and enables the study of protein dynamics as ensembles of protein structures can be observed. However, NMR is limited to look at small proteins. Recently, EM has undergone a “resolution revolution” [11] and macromolecular structures have been solved with resolutions up to 2A[citation]. The limit of cryo-EM lies in the size of proteins. Compared to the tedious task of revealing atomic resolution of a protein tertiary structure, it has become very easy to decipher the primary sequence of proteins. With the latest sequencing technologies [examples], it takes only hours to sequence millions of basepaires at low costs [example numbers] and the number of sequenced genomes has risen tremendously. The UniProtKB [10], the leading resource for protein sequences, contains more than 80 million sequence entries (24 July 2017). Consequently, the gap between the number of protein structures and sequences is still growing and even new developments as single protein structure determination [???] are not expected to close this gap near in time. [Figure sequence structure gap] Protein structure determines protein function. Therefore, structural insights are of uttermost importance. They are essential for a detailed understanding of chemical reactions, regulatory processes and transport mechanisms. They are fundamental for the design of drugs and antibiotics. Moreover structural abnormalities can lead to misfolding and aggregation potentially causing diseases so studying them is pathologically relevant. The aformentioned trends illustrate the need of computational methods and motivate research to solve Ansinsens Dogma to reliably predict protein structures from sequence alone. “At the cost of months or years of laborious work, these methods can produce high-resolution structures at the atomic level. The structures of many proteins or protein complexes cannot be determined using these methods because of the specific physical and chemical properties of these proteins or complexes.On the other hand, the knowledge of these protein structures is very important to elucidate their biological functions due to the close relationship between structure and function.Therefore, there is a pressing need for developing new methods to both increase the speed and broaden the target spectrum of protein structure determination with the rapid growth of the number of the identified proteins from genomic1,2 and proteomic studies.”[12] References "],
["protein-structure.html", "1.1 Protein Structure", " 1.1 Protein Structure Primary: Amino Acid Sewuence Secondary: Helices, sheets, coils, repeats,.. tertiary: interaction of secondary structure elementws quartary: interaction of domains 1.1.1 Amino Acid Interactions The Venn diagram in figure 1.2 displays a typical classification of amino acids with respect to their physico-chemical properties. The aromatic amino acids tryptophan (W), tyrosine (Y), phenylalanine (F), and histidine (H) contain an aromatic ring system. Generally, aromatic ring systems are planar, and electons are shared over the whole ring structure. Interactions between aromatic residues have very constrained geometries regarding the angle between the centroid of their rings. The \\(\\pi\\)-electron systems favour T-shaped or offset stacked conformations [13]. Preferred distances between aromatic residues have been observed between 4.5\\(\\angstrom\\) and 7\\(\\angstrom\\) of their ring centroids [14]. Cysteine (C) residues can form disulphide bonds, which are the only covalent bonds between two amino acid side chains. They comprise the strongest side chain interactions in protein structures and their length varies between 3.5\\(\\angstrom\\) to 4\\(\\angstrom\\). Disulphide bonds also have a well defined geometry: there are five dihedral angles in a disulphide bond resulting in 20 different possible configurations. Only one configuration is favoured so that the dihedral angle between the carbon and sulfur atoms is close to 90 degrees [15]. They play a very important role in stabilizing protein structures. The number of disulfide bonds is negatively correlated with protein length: smaller proteins have more disulfide bonds helping to stabilize the structure in absence of strong hydrophobic packing in the core. It has also been found that disulfide bonds are more frequently observed in proteins of hyperthermophilic bacteria, being positively selected for increased stability [16]. Salt bridges are based on electrostatic interactions between positively charged residues (arginine (R) and lysine (K)) and negatively charged residues (aspartic acid (D) and glutamic acid (E)). The strength of electrostatic interactions, as described by Coulomb’s law, decreases with distance between the point charges at the functional groups. It has been found to be maximal at 4\\(\\angstrom\\) with respect to the functional groups of the both residues [17]. Hydrogen bonds can be formed between a donor residue which possesses an hydrogen atom attached to a strongly electronegative atom and an acceptor residue which possesses an electronegative atom with a lone electron pair. They are electrostatic interactions as well and thus their strength depends on distance as well. Hydrogen bonds are formed at distances of 2.4\\(\\angstrom\\) to 3.5\\(\\angstrom\\) between the non-hydrogen atoms (Berg JM, Tymoczko JL, 2002). Salt bridges as well as hydrogen bonds have strong geometric preferences (Kumar and Nussinov, 1999). The geometry of a hydrogen bond depends on the angle between the HB donor, the hydrogen atom and the HB acceptor (Torshin et al., 2002). Cation–\\(\\pi\\) interactions are formed between positively charged or partially charged amino acids with amino groups (K,R,Q,E) and aromatic residues (W,Y,F,H). The preferential distance of the amino group to the \\(\\pi\\)-electron system has been determined between 3.4\\(\\angstrom\\) and 6\\(\\angstrom\\) [18] [19] Their role in stabilizing protein structures is still under debate [20]. Proline residues are conformationally restricted, with the alpha-amino group of the backbone directly attached to the side chain. The sterical rigidity of the proline side chain restricts the backbone angle and thus affects secondary structure formation. Proline is known as a helix-breaker. Whereas other aromatic side chains are defined by their negatively charged \\(\\pi\\) faces, the face of proline side chains is partially positively charged. Thus, aromatic and proline residues can interact favorably with each other. Once due to the hydrophobic nature of the residues and also due to the interaction between the negatively charged aroamtic \\(\\pi\\) face and the polarized C-H bonds in proline, called a CH/\\(\\pi\\) interaction. Petersen et al. (2012) found clear secondary structure elements preferences for each amino acid pair. For example, residue pairs containing Alanine and Leucine are predominantly found in buried \\(\\alpha\\)-helices, whereas pairs containing Isoleucine and Valine preferentially are located in \\(\\beta\\)-sheet environments. Of course, solvent accessibility represents an important criterion for residue interactions. Hydrophobic residues are rather buried in the structure, whereas polar and charged residues are found more frequently on the protein surface and interact with water molecules. Figure 1.2: Physico-chemical properties of amino acids. The 20 naturally occuring amino acids are grouped with respect to ten physico-chemical properties. Adapted from Figure 1a in [21]. References "],
["structure-prediction.html", "1.2 Structure Prediction", " 1.2 Structure Prediction Despite the knowledge of Anfinsen’s postulate, we are not able to reliably predict the structure of a protein from its sequence alone. Generally it is assumed that a protein folds into a unique, well-defined native structure that is near the global free energy minimum (fig:folding_funnel). Levinthal’s paradox [22] describes the complexity of the folding process towards this minimum. It stresses the problem that it is not possible for a protein to exhaustively search the conformational space to get to its native fold. Due to the “combinatorial explosion” of possible conformations, an exhaustive search would take unreasonably long. Hence, it is not a feasible approach for structure prediction to scan all possible conformations. Different approaches have been developed over time to overcome or elude this problem. 1.2.1 Template-based methods Homology modeling is by far the most successful approach to structure prediction. The basic concept of this strategy relates to the fact that structure is more conserved than sequence [4]. After detecting a homologous protein of known structure, that has sufficient sequence similarity, it can be used as a template to model the structure of the target protein. The degree of structural conservation is linked to the level of pariwise sequence identity [6]. Homology Modelling is assumed to yield reliably accurate models when query and target protein share more than 30\\(\\%\\) sequence similarity, depending on the sequence length (safe homology zone) [5]. Below a threshold of ~20-35% pairwise sequence identity (twighlight-zone) the number of false positives regarding structural similarity explodes and structural inference becomes less reliable and more than 95% of structures are dissmilar [23]. Advances in remote homology detection and alignment generation have improved the quality of models, even beyond the once postulated limit of the twighlight-zone [24]. Integration of multiple templates has also proved to increase model quality [25] After the identification of a suitable template, there are different strategies that can be followed to obtain a model for the target protein. The the backbone of the model is generated by simply copying the coordinates of the target backbone atoms onto the model. Non-aligned residues due to gaps in the alignment have to be modelled \\(\\textit{de-novo}\\), meaning from scratch. This can be done by a knowledge-based search for suitable fragments in the PDB or by true energy-based \\(\\textit{de-novo}\\) modelling. When the backbone is generated, the side chains are modelled, usually by searching rotamer libraries for energetically favoured residue conformations. Finally, the model is energetically optimized in an iterative procedure. Force fields are applied to correct the backbone and side chain conformations [26]. Several automated pipelines for homology modelling are well-established (Modeller [27], 3D-Jigsaw [28], SwissModel [29]) which allow more or less manual intervention in the modelling process. Fold Recognition describes the inverse folding problem [Bowie1993]: instead of finding the compatible structure for a given sequence, one tries to find sequences that fit onto a given structure. Whether the query sequence fits a structure from the database is not determined by sequence similarities but rather energetic or environment specific measures. Thus, fold recognition methods are able to recognize structural similarity even in the absence of sequence similarity. The rationale basis for this strategy is the assumption that the fold space is limited. It has been found that seemingly unrelated proteins often adopt similar folds. This might be due to divergent evolution (proteins are related, but homology cannot be detected at the corresponding sequence level) or convergent evolution (functional requirements lead to similar folds for unrelated proteins) [Gu2009]. Early approaches include profile based methods. Here, the structural information of the protein is encoded into profiles, which subsequently are aligned to the sequences [Bowie1991,Fischer1996,Ouzounis1993]. Advanced techniques are known as “threading” techniques, describing the process of threading a sequence through a structure and determining the optimal fit via energy functions. [Jones1992,Jones1998,Lemer1995] 1.2.2 Template-free structure prediction Ab initio or de-novo modeling techniques implement Anfinsen’s Dogma most closely in mimicking the folding process based only on physico-chemical principles. Energy functions (physical or knowledge-based) are used to describe the folding landscape and are minimized to arrive at the global energy minimum corresponding to the native conformation. Since the native conformation can be found near the global energy minimum of the folding landscape, energy functions (physical or knowledge-based) have been developed to describe this landscape. With respect to the idea of a folding funnel, the energy function is minimized to mimic the folding process that automatically leads to the global minimum. Again, there exist numerous webservers that combine energy minimization, threading techniques and fragment-based approaches, e.g. Rosetta [\\citep{]Simons1999], Tasser [Zhang2004, Touchstone II Zhang2003]. Drawbacks of these methods are the time requirements due to the computational complexity of energy functions as well as their inaccuracy. Minimize a physical or knowledge-based energy function for the protein. This has huge complexity due to large conformational space that needs to be sampled. --> References "],
["introduction-to-contact-prediction.html", "2 Introduction to Contact Prediction", " 2 Introduction to Contact Prediction Contact prediction refers to the prediction of physical contacts between amino acid side chains in the 3D protein structure, given the protein sequence as input. Historically, contact prediction was motivated by the idea that compensatory mutations between spatially neighboring residues can be traced down from evolutionary records [30]. As proteins evolve, they are under selective pressure to maintain their function and correspondingly their structure. Consequently, residues and interactions between residues constraining the fold, protein complex formation, or other aspects of function are under selective pressure. Highly constrained residues and interactions will be strongly conserved [31]. Another possibility to maintain structural integrity is the mutual compensation of unbeneficial mutations. For example, the unfavourable mutation of a small amino acid residue into a bulky residue in the densely packed protein core might have been compensated in the course of evolution by a particularly small side chain in a neighboring position. Other physico-chemical quantities such as amino acid charge or hydrogen bonding capacity can also induce compensatory effects[32]. The MSA of a protein family comprises homolog sequences that have descended from a common ancestor and are aligned relative to each other. According to the hypothesis, compensatory mutations show up as correlations between the amino acid types of pairs of MSA columns and can be used to infer spatial proximity of residue pairs (see Figure 2.1). Figure 2.1: The evolutionary record of a protein family reveals evidence of compensatory mutations between spatially neighboring residues that are under selective pressure with respect to some physico-chemical constraints. Mining protein family sequence alignments for residue pairs with strong coevolutionary signals using statistical methods allows inference of spatial proximity for these residue pairs. The following sections will give an overview over important methods and developments in the field of contact prediction. References "],
["local-methods.html", "2.1 Local Statistical Models", " 2.1 Local Statistical Models Early contact prediction methods used local pairwise statistics to infer contacts that regard pairs of amino acids in a sequence as statistically independent from another. Several of these methods use correlation coefficient based measures, such as Pearson correlation between amino acid counts, properties associated with amino acids or mutational propensities at the sites of a MSA [30,32–35]. Many methods have been developed that are rooted in information theory and use MI measures to describe the dependencies between sites in the alignment [36–38]. Phylogenetic and entropic biases have been identified as strong sources of noise that confound the true coevolution signal [38–40]. Different variants of MI based approaches address these effects and improve on the signal-to-noise ratio [39,41,42]. The most prominent correction for background noises is APC that is still used by many modern methods and is discussed in section 2.4.4 [43]. Another popular method is OMES that essentially computes a chi-squared statistic to detect the differences between observed and expected pairwise amino acid frequencies for a pair of columns [44,45]. A significant shortcoming of these traditional covariance approaches is their inability to cope with transitive effects that arise from chains of correlations between multiple residue pairs [46–48]. The concept of transitve effects is illustrated in Figure 2.2. Considering three residues A, B and C, where A physically interacts with B and B with C. Strong statistical dependencies between pairs (A,B) and (B,C) can induce strong indirect signals for residues A and C, eventhough they are not physically interacting. These indirect correlations can become even larger than signals of other directly interacting pairs (D,E) and thus lead to false predictions [47]. Local statistical methods consider residue pairs independent of one another which is why they cannot distinguish between direct and indirect correlation signals. In contrast, global statistical models presented in the next section learn a joint probability distribution over all residues allowing to disentangle transitive effects [47,48]. Eventhough local statistical methods cannot compete with modern predictors, OMES and MI based scores often serve as a baseline in performance benchmarks for contact prediction [49,50]. Figure 2.2: Effects of chained covariation obscure signals from true physical interactions. Consider residues A through E with physical interactions between the residue pairs A-B, B-C and D-E. The thickness of blue lines between residues reflects the strength of statistical dependencies between the corresponding alignment columns. Strong statistical dependencies between residue pairs (A,B) and (B,C) can induce a strong dependency between the spatially distant residues A and C. Covariation signals arising from transitive effects can become even stronger than other direct covariation signals and lead to false positive predictions. References "],
["global-methods.html", "2.2 Global Statistical Models", " 2.2 Global Statistical Models A huge leap forward was the development of sophisticated statistical models that make predictions for a single residue pair while considering all other pairs in the protein. These global models allow for the distinction between transitive and causal interactions which has been referred to in the literature as DCA [46,48]. In 1999 Lapedes et al. were the first to propose a global statistical approach for the prediction of residue-residue contacts in order to disentangle transitive effects [46]. They consider a Pott’s model that can be derived under a maximum entropy assumption and use the model specific coupling parameters to infer interactions. At that time the wider implications of this advancement went unnoted, but meanwhile the Pott’s Model has become the most prominent statistical model for contact prediction. Section 2.4 deals extensively with the derivation and properties of the Pott’s model, its application to contact prediction and its numerous realizations. A global statistical model not motivated by the maximum entropy approach was proposed by Burger and Nijmwegen in 2010 [47,51]. Their fast Bayesian network model incorporates additional prior information and phylogenetic correction via APC but cannot compete with the pseudo-likelihood approaches presented in section 2.4.3.1. References "],
["meta-predictors.html", "2.3 Machine Learning Methods and Meta-Predictors", " 2.3 Machine Learning Methods and Meta-Predictors With the steady increase in protein sequence data, machine learning based methods have emerged that extract features from MSAs in order to learn associations between input features and residue-residue contacts. Sequence features typically include predicted solvent accessibility, predicted secondary structure, contact potentials, conservation scores, global protein features, pairwise coevolution statistics and averages of certain features over sequence windows. Numerous sequence-based methods have been developed using machine learning algorithms, such as support support vector machines (SVMCon [52], SVM-SEQ [53]), random forests (ProC_S3 [54], TMhhcp [55], PhyCMap [56]), neural networks (NETCSS [57], SAM [58], [59], SPINE-2D [60], NNCon [61]) deep neural networks (DNCon [62], CMAPpro [63]) and ensembles of genetic algorithm classfiers (GaC [64]). Different contact predictors, especially when rooted in distinct principles like sequence-based and coevolution methods, provide orthogonal information on the likelihood that a pair of residues makes a contact [52,65]. The next logical step in method development therefore constitutes the combination of several base predictors and classical sequence-derived features in the form of meta-predictors. The first published meta-predictor was PconsC in 2013, combining sequence features and predictions from the coevolution methods PSICOV and plmDCA [66]. In a follow-up version PSICOV has been replaced with gaussianDCA and the sequence-based method PhyCMap [67]. EPC-MAP was published in 2014 integrating GREMLIN as a coevolution feature with physicochemical information from predicted ab initio protein structures [68]. In 2015, MetaPSICOV was released combining predictions from PSICOV, mfDCA and CCMpred with other sequence derived feautures [69]. RaptorX uses CCMpred as coevolution feature and other standard contact prediction features within an ultra-deep neural network [70]. The newest developments EPSILON-CP and NeBcon both comprise the most comprehensive usage of contact prediction methods so far, combining five and eight state-of-the-art contact predictors, respectively [71,72]. Another conceptual advancement besides the combination of sources of information is based on the fact that contacts are not randomly or independently distributed. DiLena and colleagues found that over 98% of long-range contacts (sequence separation &gt; 24 positions) are in close proximity of other contacts, compared to 30% for non-contacting pairs [63]. The distribution of contacts is governed by local structural elements, like interactions between helices or \\(\\beta\\)-sheets, leading to characteristic patterns in the contact map that can be recognised [73]. Deep learning provides the means to model higher level abstractions of data and several methods apply multi-layered algorithms to refine predictions by learning patterns that reflect the local neighborhood of a contact [63,69,70,74]. Eventhough a benchmark comparing the recently developed meta-predictors is yet to be made, it becomes clear from the recent CASP experiments, that meta-predictors outperform pure coevolution methods [75]. As coevolution scores comprise the most informative feautures among the set of input features, it is clear that meta-predictors will benefit from further improvements of pure coevolution methods [70,71]. References "],
["maxent.html", "2.4 Modelling Protein Families with Potts Model", " 2.4 Modelling Protein Families with Potts Model Infering contacts from a joint probability distribution over all residues in a protein sequence instead of using simple pairwise statistics has been proven to enable the distinction of direct statistical dependencies between residues from indirect dependencies mediated through other residues. The global statistical model that is commonly used to describe this joint probability distribution is the Potts model. It is a well-established model in statistical mechanics and can be derived from a maximum entropy assumption which is explained in the following. The principle of maximum entropy, proposed by Jaynes in 1957 [76,77], states that the probability distribution which makes minimal assumptions and best represents observed data is the one that is in agreement with measured constraints (prior information) and has the largest entropy. In other words, from all distributions that are consistent with measured data, the distribution with maximal entropy should be chosen. A protein family is represented by a MSA \\(\\X = \\{ \\seq_1, \\ldots, \\seq_N \\}\\) of \\(N\\) protein sequences. Every protein sequence of the protein family represents a sample drawn from a target distribution \\(p(\\seq)\\), so that each protein sequence is associated with a probability. Every sequence \\(\\seq = (x_1, \\ldots, x_L)\\) is of length \\(L\\) and every position in the sequences constitutes a categorical variables \\(x_{i}\\) that can take one of \\(q=21\\) values representing the 20 naturally occuring amino acids and a gap (‘-’). The measured constraints are given by the empirically observed single and pairwise amino acid frequencies that can be calculated as \\[\\begin{equation} f_i(a) = f(x_i\\eq a) = \\frac{1}{N}\\sum_{n=1}^N I(x_{ni} \\eq a) \\; , \\end{equation}\\] \\[\\begin{equation} f_{ij}(a,b) = f(x_i\\eq a, x_j\\eq b) = \\frac{1}{N} \\sum_{n=1}^N I(x_{ni} \\eq a, x_{nj} \\eq b) \\; . \\tag{2.1} \\end{equation}\\] According to the maximum entropy principle, the distribution \\(p(\\seq)\\) should have maximal entropy and reproduce the empirically observed amino acid frequencies, so that \\[\\begin{align} f(x_i\\eq a) &amp;\\equiv p(x_i\\eq a) \\nonumber\\\\ &amp;= \\sum_{\\seq\\prime_1, \\ldots, \\seq\\prime_L = 1}^{q} p(x\\prime) I(x\\prime_i \\eq a) \\\\ f(x_i\\eq a, x_j\\eq b) &amp;\\equiv p(x_i\\eq a, x_j \\eq b) \\nonumber \\\\ &amp;= \\sum_{\\seq\\prime_1, \\ldots, \\seq\\prime_L = 1}^{q} p(x\\prime) I(x\\prime_i\\eq a, x\\prime_j \\eq b) \\; . \\tag{2.2} \\end{align}\\] Solving for the distribution \\(p(\\seq)\\) that maximizes the Shannon entropy \\(S= -\\sum_{\\seq\\prime} p(\\seq\\prime) \\log p(\\seq\\prime)\\) while satisfying the constraints given by the empircial amino acid frequencies in eq. (2.2) by introducing Lagrange multipliers \\(\\wij\\) and \\(\\vi\\), results in the formulation of the Potts model, \\[\\begin{equation} p(\\seq | \\v, \\w ) = \\frac{1}{Z(\\v, \\w)} \\exp \\left( \\sum_{i=1}^L v_i(x_i) \\sum_{1 \\leq i &lt; j \\leq L}^L w_{ij}(x_i, x_j) \\right) \\; . \\tag{2.3} \\end{equation}\\] The Lagrange multipliers \\(\\wij\\) and \\(\\vi\\) remain as model parameters to be fitted to data. \\(Z\\) is a normalization constant also known as partition function that ensures the total probabilty adds up to one by summing over all possible assignments to \\(\\seq\\), \\[\\begin{equation} Z(\\v, \\w) = \\sum_{\\seq\\prime_1, \\ldots, \\seq\\prime_L = 1}^{q} \\exp \\left( \\sum_{i=1}^L v_i(x_i) \\sum_{1 \\leq i &lt; j \\leq L}^L w_{ij}(x_i, x_j) \\right) \\; . \\tag{2.4} \\end{equation}\\] 2.4.1 Model Properties The Potts model is specified by singlet terms \\(\\via\\) which describe the tendency for each amino acid a to appear at position \\(i\\), and pair terms \\(\\wijab\\), also called couplings, which describe the tendency of amino acid a at position \\(i\\) to co-occur with amino acid b at position \\(j\\). In contrast to mere correlations, the couplings explain the causative dependence structure between positions by jointly modelling the distribution of all positions in a protein sequence and thus account for transitive effects. By doing so, a major source of noise in contact prediction methods is eliminated. To get some intuition for the coupling coefficients, note that \\(\\wijab = 1\\) corresponds to a 2.7-fold higher probability for a and b to occur together than what is expected from the singlet frequencies if a and b were independent. Pairs of residues that are not in contact tend to have negligable couplings, \\(\\wij \\approx 0\\), whereas pairs in contact tend to have vectors significantly different from 0. For contacting residues \\(i\\) and \\(j\\) in real world MSAs typical coupling strengths are on the order of \\(||\\wij || \\approx 0.1\\) (regularization dependent). Maximum entropy models naturally give rise to exponential family distributions that express useful properties for statistical modelling, such as the convexity of the likelihood function which consequently has a unique, global minimum [78,79]. The Potts model is a discrete instance of what is referred to as a pairwise Markov random field in the statistics community. MRFs belong to the class of undirected graphical models, that represent the probability distribution in terms of a graph with nodes and edges characterizing the variables and the dependence structure between variables, respectively. 2.4.1.1 Gauge Invariance As every variable \\(x_{ni}\\) can take \\(q=21\\) values, the model has \\(L \\! \\times \\! q + L(L-1)/2 \\! \\times \\! q^2\\) parameters. But the parameters are not uniquely determined and multiple parametrizations yield identical probability distributions. For example, adding a constant to all elements in \\(v_i\\) for any fixed position \\(i\\) or similarly adding a constant to \\(\\via\\) for any fixed position \\(i\\) and amino acid \\(a\\) and subtracting the same constant from the \\(qL\\) coefficients \\(\\wijab\\) with \\(b \\in \\{1, \\ldots, q\\}\\) and \\(j \\in \\{1, \\ldots, L \\}\\) leaves the probabilities for all sequences under the model unchanged, since such a change will be compensated by a change of \\(Z(\\v, \\w)\\) in eq. (2.4). The overparametrization is referred to as gauge invariance in statistical physics literature and can be eliminated by removing parameters [48,80]. An appropriate choice of which parameters to remove, referred to as gauge choice, reduces the number of parameters to \\(L \\! \\times \\! (q-1) + L(L-1)/2 \\! \\times \\! (q-1)^2\\). Popular gauge choices are the zero-sum gauge or Ising-gauge used by Weigt et al. [48] imposed by the restraints, \\[\\begin{equation} \\sum_{a=1}^{q} v_{ia} = \\sum_{a=1}^{q} \\wijab = \\sum_{a=1}^{q} w_{ijba} = 0 \\tag{2.5} \\end{equation}\\] for all \\(i,j,b\\) or the lattice-gas gauge used by Morcos et al [80] and Marks et al [81] imposed by restraints \\[\\begin{equation} \\wij(q,a) = \\wij(a,q) = \\vi(q) = 0 \\tag{2.6} \\end{equation}\\] for all \\(i,j,a\\) [82]. Alternatively, the indeterminacy can be fixed by including a regularization prior (see next section). The regularizer selects for a unique solution among all parametrizations of the optimal distribution and therefore eliminates the need to choose a gauge [83–85]. 2.4.2 Inferring Parameters for the Potts Model Typically, parameter estimates are obtained by maximizing the log-likelihood function of the parameters over observed data. For the Potts model, the log-likelihood function is computed over sequences in the alignment \\(\\mathbf{X}\\): \\[\\begin{align} \\text{LL}(\\v, \\w | \\mathbf{X}) =&amp; \\sum_{n=1}^N \\log p(\\seq_n) \\nonumber\\\\ =&amp; \\sum_{n=1}^N \\left[ \\sum_{i=1}^L v_i(x_{ni}) + \\sum_{1 \\leq i &lt; j \\leq L}^L w_{ij}(x_{xn}, x_{nj}) - \\log Z \\right] \\tag{2.7} \\end{align}\\] The number of parameters in a Potts model is typically larger than the number of observations, i.e. the number of sequences in the MSA. Considering a protein of length \\(L=100\\), there are approximately \\(2 \\times 10^6\\) parameters in the model whereas the largest protein families comprise only around \\(10^5\\) sequences (see Figure 2.7). An underdetermined problem like this renders the use of regularizers neccessary in order to prevent overfitting. Typically, an L2-regularization is used that pushes the single and pairwise terms smoothly towards zero and is equivalent to the logarithm of a zero-centered Gaussian prior, \\[\\begin{align} R(\\v, \\w) &amp;= \\log \\left[ \\mathcal{N}(\\v | \\mathbf{0}, \\lambda_v^{-1} I) \\mathcal{N}(\\w | \\mathbf{0}, \\lambda_w^{-1} I) \\right] \\nonumber \\\\ &amp;= -\\frac{\\lambda_v}{2} ||\\v||_2^2 - \\frac{\\lambda_w}{2} ||\\w||_2^2 + \\text{const.} \\; , \\tag{2.8} \\end{align}\\] where the strength of regularization is tuned via the regularization coefficients \\(\\lambda_v\\) and \\(\\lambda_w\\) [86–88]. However, optimizing the log-likelihood requires computing the partition function \\(Z\\) given in eq. (2.4) that sums \\(q^L\\) terms. Computing this sum is intractable for realistic protein domains with more than 100 residues. Consequently, evaluating the likelihood function at each iteration of an optimization procedure is infeasible due to the exponential complexity of the partition function in protein length \\(L\\). Many approximate inference techniques have been developed to sidestep the infeasible computation of the partition function for the specific problem of predicting contacts that are briefly explained in the next section. 2.4.3 Solving the Inverse Potts Problem In 1999 Lapedes et al. were the first to propose maximum entropy models for the prediction of residue-residue contacts in order to disentangle transitive effects [46]. They used an iterative Monte Carlo procedure to obtain estimates of the partition function. As the calculations involved were very time-consuming and at that time required supercomputing resources, the wider implications were not noted yet. Ten years later Weight et al proposed an iterative message-passing algorithm, here referred to as mpDCA, to approximate the partition function [48]. Eventhough their approach is computationally very expensive and in practice only applicable to small proteins, they obtained remarkable results for the two-component signaling system in bacteria. Balakrishnan et al were the first to apply pseudo-likelihood approximations to the full likelihood in 2011 [89]. The pseudo-likelihood optimizes a different objective and replaces the global partition function \\(Z\\) with local estimates. Balakrishnan and colleagues applied their method GREMLIN to learn sparse graphical models for 71 protein families. In a follow-up study in 2013, the authors proposed an improved version of GREMLIN that uses additional prior information [88]. Also in 2011, Morcos et al. introduced a naive mean-field inversion approximation to the partition function, named mfDCA [80]. This method allows for drastically shorter running times as the mean-field approach boils down to inverting the empirical covariance matrix calculated from observed amino acid frequencies for each residue pair \\(i\\) and \\(j\\) of the alignment. This study performed the first high-throughput analysis of intradomain contacts for 131 protein families and facilitated the prediction of protein structures from accurately predicted contacts in [81]. The initial work by Balakrishnan and collegueas went almost unnoted as it was not primarily targeted to the problem of contact prediction. Ekeberg and collegueas independently developed the pseudo-likelihood method plmDCA in 2013 and showed its superior precision over mfDCA [84]. A related approach to mean-field approximation is sparse inverse covariance estimation, named PSICOV, developed by Jones et al. (2012) [50]. PSICOV uses an L1-regularization, known as graphical Lasso, to invert the correlation matrix and learn a sparse graphical model [90]. Both procedures, mfDCA and PSICOV, assume the model distribution to be a multivariate Gaussian. It has been shown by Banerjee et al. (2008)that this dual optimization solution also applies to binary data, as is the case in this application, where each position is encoded as a 20-dimensional binary vector [91]. Another related approach to mfDCA and PSICOV is gaussianDCA, proposed in 2014 by Baldassi et al. [92]. Similar to the other both approaches, they model the data as multivariate Gaussian but within a simple Bayesian formalism by using a suitable prior and estimating parameters over the posterior distribution. So far, pseudo-likelihood has proven to be the most successful approximation of the likelihood with respect to contact prediction performance. Currently, there exist several implementations of pseudo-likelihood maximization that vary in slight details, perform similarly and thus are equally popular in the community, such as CCMpred [86], plmDCA[87] and GREMLIN [88]. 2.4.3.1 Maximum Likelihood Inference for Pseudo-Likelihood The pseudo-likelihood is a rather old estimation principle that was suggested by Besag already in 1975 [93]. It represents a different objective function than the full likelihood and approximates the joint probability with the product over conditionals for each variable, i.e. the conditional probability of observing one variable given all the others: \\[\\begin{align} p(\\seq | \\v,\\w) \\approx&amp; \\prod_{i=1}^L p(x_i | \\seq_{\\backslash xi}, \\v,\\w) \\nonumber \\\\ =&amp; \\prod_{i=1}^L \\frac{1}{Z_i} \\exp \\left( v_i(x_i) \\sum_{1 \\leq i &lt; j \\leq L}^L w_{ij}(x_i, x_j) \\right) \\end{align}\\] Here, the normalization term \\(Z_i\\) sums only over all assignments to one position \\(i\\) in sequence: \\[\\begin{equation} Z_i = \\sum_{a=1}^{q} \\exp \\left( v_i(a) \\sum_{1 \\leq i &lt; j \\leq L}^L w_{ij}(a, x_j) \\right) \\tag{2.9} \\end{equation}\\] Replacing the global partition function in the full likelihood with local estimates of lower complexity in the pseudo-likelihood objective resolves the computational intractability of the parameter optimization procedure. Hence, it is feasible to maximize the pseudo-log-likelihood function, \\[\\begin{align} \\text{pLL}(\\v, \\w | \\mathbf{X}) =&amp; \\sum_{n=1}^N \\sum_{i=1}^L \\log p(x_i | \\seq_{\\backslash xi}, \\v,\\w) \\nonumber \\\\ =&amp; \\sum_{n=1}^N \\sum_{i=1}^L \\left[ v_i(x_{ni}) + \\sum_{j=i+1}^L w_{ij}(x_{ni}, x_{nj}) - \\log Z_{ni} \\right] \\;, \\end{align}\\] plus an additional regularization term in order to prevent overfitting and to fix the gauge to arrive at a MAP estimate of the parameters, \\[\\begin{equation} \\hat{\\v}, \\hat{\\w} = \\underset{\\v, \\w}{\\operatorname{argmax}} \\; \\text{pLL}(\\v, \\w | \\mathbf{X}) + R(\\v, \\w) \\; . \\end{equation}\\] Eventhough the pseudo-likelihood optimizes a different objective than the full-likelihood, it has been found to work well in practice for many problems, including contact prediction [79,83–85]. The pseudo-likelihood function retains the concavity of the likelihood and it has been proven to be a consistent estimator in the limit of infinite data for models of the exponential family [83,93,94]. That is, as the number of sequences in the alignment increases, pseudo-likelihood estimates converge towards the true full likelihood parameters. 2.4.4 Computing Contact Maps Model inference as described in the last section yields MAP estimates of the couplings \\(\\hat{\\w}_{ij}\\). In order to obtain a scalar measure for the coupling strength between two residues \\(i\\) and \\(j\\), all available methods presented in section 2.4.3 heuristically map the \\(21 \\! \\times \\! 21\\) dimensional coupling matrix \\(\\wij\\) to a single scalar quantity. mpDCA [48] and mfDCA [80,81] employ a score called DI, that essentially computes the MI for two positions \\(i\\) and \\(j\\) using the couplings \\(\\wij\\) instead of pairwise amino acid frequencies. Most pseudo-likelihood methods (plmDCA [84,87], CCMpred [86], GREMLIN [88]) compute the Frobenius norm of the coupling matrix \\(\\wij\\) to obtain a scalar contact score \\(C_{ij}\\), \\[\\begin{equation} C_{ij} = ||\\wij||_2 = \\sqrt{\\sum_{a,b=1}^q \\wijab^2} \\; . \\tag{2.10} \\end{equation}\\] The Frobenius norm improves prediction performance over DI and further improvements can be obtained by computing the Frobenius norm only on the \\(20 \\times 20\\) submatrix thus ignoring contributions from gaps [84,92,95]. PSICOV [50] uses an L1-norm on the \\(20 \\times 20\\) submatrix instead of the Frobenius norm. Furthermore it should be noted that the Frobenius norm is gauge dependent and is minimized by the zero-sum gauge [48]. Therefore, the coupling matrices should be transformed to zero-sum gauge before computing the Frobenius norm \\[\\begin{equation} \\w^{\\prime}_{ij} = \\wij - \\wij(\\cdot, b) - \\wij(a, \\cdot) + \\wij(\\cdot, \\cdot) \\; , \\tag{2.11} \\end{equation}\\] where \\(\\cdot\\) denotes average over the respective indices [84,86,87,92]. Another commonly applied heuristic known as APC has been introduced by Dunn et al. in order to reduce background noise arising from correlations between positions with high entropy or phylogenetic couplings [43]. APC is a correction term that is computed from the raw contact map as the product over average row and column contact scores \\(\\overline{C_i}\\) divided by the average contact score over all pairs \\(\\overline{C_{ij}}\\). The corrected contact score \\(C_{ij}^{APC}\\) is obtained by subtracting the APC term from the raw contact score \\(C_{ij}\\), \\[\\begin{equation} C_{ij}^{APC} = C_{ij} - \\frac{\\overline{C_i} \\; \\overline{C_j}}{\\overline{C_{ij}}}\\; . \\tag{2.12} \\end{equation}\\] Visually, APC creates a smoothing effect on the contact maps that is illustrated in Figure 2.3 and it has been found to substantially boost contact prediction performance [43,88]. It was first adopted by PSICOV [50] but is now used by most methods to adjust raw contact scores. It was long under debate why APC works so well and how it can be interpreted. Zhang et al. showed that APC essentially approximates the first principal component of the contact matrix and therefore removes the highest variability in the matrix that is assumed to arise from background biases [96]. Furthermore, they studied an advanced decomposition technique, called LRS matrix decomposition, that decomposes the contact matrix into a low-rank and a sparse component, representing background noise and true correlations, respectively. Inferring contacts from the sparse component works astonishing well, improving precision further over APC independent of the underlying statistical model. Dr Stefan Seemayer could show that the main component of background noise can be attributed to entropic effects and that a substantial part of APC amounts to correcting for these entropic biases (unpublished). In his doctoral thesis, he developed an entropy correction, computed as the geometric mean of per-column entropies, that correlates well with the APC correction term and yields similar precision for predicted contacts. The entropy correction has the advantage that it is computed from input statistics and therefore is independent of the statistical model used to infer the couplings. In contrast, APC and other denoising techniques such as LRS [96] discussed above, estimate a background model from the final contact matrix, thus depending on the statistical model used to infer the contact matrix. Figure 2.3: Contact maps computed from pseudo-likelihood couplings. Subplot on top of the contact maps illustrates the normalized Shannon entropy (pink line) and percentage of gaps for every position in the alignment (brown line). Left: Contact map computed with Frobenius norm as in eq. (2.10). Overall coupling values are dominated by entropic effects, i.e. the amount of variation for a MSA position, leading to striped brightness patterns. For example, positions with high column entropy (e.g. positions 7, 12 or 31) have higher overall coupling values than positions with low column entropy (e.g. positions 11, 24 or 33). b: previous contact map but corrected for background noise with the APC as in eq. (2.12). References "],
["applications.html", "2.5 Applications", " 2.5 Applications The most popular and successful application for contact prediction is contact-guided denovo structure prediction. In the last years many studies employed predicted contacts to build three-dimensional models for single proteins or even thousand of protein families at once []. In 2011, first structures predicted with contacts (Marks et al., 2011). Protein fold reconstruction from contact maps has a long history. The idea to reconstruct the three-dimensional structure of a protein given distance constraints is not new as constraints from experiments such as cross-linking/mass specttrometry or fluorescence resonance energy transfer (FRET) allow for the determination of inter-residue proximity constraints. It has been shown that is possible to reconstruct the fold from native contacts alone and many studies have been dealing with questions like how many constraints are necesarry to determine the fold and what is the structural requirements to these constraints. It was shown that a subset of native contacts alone is sufficient and to nativelike conformation of a protein and that rational strategies can be applied to select a minimal subset [12,97–99]. In [97] is was shown that selecting 8% of native contact according to a rational strategy helps reconstruct the structure to 4.8Ca RMSD, and 20-30% of randomly selected contacts. However, the model quality depends on many factors such as the secondary structure comnposition, additional information, the distance definitions and most of all the structure reconstruction protocol, the quality of the constaints and their distribution and structural properties. Long-range contacts define the overal fold and packing of tertiary structure whereas short-range contacts define secondary structure. Long-range contacts are more important than short-range contacts for structure reconstruction, even though their information alone is not sufficient [97] large-scale contact-guided folding study has shown that contact predictions should have an accuracy of at least 22% to generate a positive effect to the ab initio struc- ture prediction (Zhang, et al., 2003). 2.5.1 contact assisted de-novo predictions “”“Assessing Predicted Contacts for Building Protein Three-Dimensional Models”“!! Structure Reconstruction from true contacts maps works well. Even a small number of contacts is sufficient to reconstruct the fold of the protein. The number of contacts which can be accurately predicted increases with the number of effective sequences: to robustly generate accurate three-dimensional structure models requires roughly fivefold more sequences (Ovchinnikov et al., 2017) Distance maps work even better. What is the optimal distance cutoff to define a contact? Duarte et al 2010: between 8 and 12A Dyrka et al 2016 Konopka et al 2014 Sathyapriya et al 2009 Many studies that successfuly predict structures denovo with the help of predicted contact.[Ovchinnikov2015a] Vice versa, because contacts at large primary distances are rare, they are most informative for protein structure prediction: Izarzugaza J, Gran O, Tress M, Valencia A, Clarke N (2007) Assessment of intramolecular contact predictions for CASP7 only one correct contact for every 12 residues in the protein to allow accurate topology level modeling[100] also nonlocal and broadly distributed observe that false positives are not equal in terms of their detrimental effect on structure prediction[81] unicon3d: http://sysbio.rnet.missouri.edu/UniCon3D/ -Saint 2 (not published yet) https://github.com/sauloho/SAINT2 Saint 2 was used in the Oliveira 2016 study - CoinFold (Wang 2016) unsatisfied constraints are filtered - RASREC (Braun 2015) using contacts from EVFOLD-plm and EVFOLD-mf (other contact maps can be used) Rosetta RASREC protocol distance restraints function: shallow sigmoidal potential https://www.rosettacommons.org/demos/latest/protocol_capture/rasrec_evolutionary_restraints/README - CONFOLD (Cheng 2015) principal idea behind CONFOLD is to build models in two stages to detect self- conflicting contacts can provide own contacts, validation in paper using EVfold predictions two stage modelling: 1) SSE and contact predictions are transformed to distance restraints and bond and angle restraints and models are build using CNS 2) filter out unsatisfied contacts http://protein.rnet.missouri.edu/confold/ - RBO_ALEPH (Mabrouk 2015) automatically decides on either template-based modelling or ab initio modelling in case of ab initio modelling: uses EPC for contact prediction (users can upload there own predictions: server linearly combines user defined contacts with its own predicted contacts by EPC-map. Currently, both contact lists have equal weight in the linear) uses model-based search (MBS) for structure prediction http://compbio.robotics.tu-berlin.de/rbo_aleph/ - GDFuzz3D (Pietal 2015) contact map as input predict 2D distance map from contact map use distance map to generate coarse-grained Calpha model refine with Modeller + REFINER using distance restraints http://iimcb.genesilico.pl/gdserver/GDFuzz3D/ - C2S_pipeline (Konopka 2014) contact map as input (for validation they use real contact maps + derived erroneous CMaps) - FT-COMAR to construct Calpha trace SABBAC for backbone reconstruction SCWRL for side-chain prediction and optimization http://comprec-lin.iiar.pwr.edu.pl/c2sInput/ - PconsFold (Michel 2014) smoothed square well restraint scoring function https://github.com/ElofssonLab/pcons-fold - Fragfold + coevolutionary Info (Kosciolek 2014) square well function with exponential decay - EVFold (Marks 2011) - CNS Moreover, many studies employ residue-residue contacts to successfully predict structures of unsolved proteins (Wang and Barth, 2015; Hayat et al., 2015; Ovchinnikov et al., 2015), to elucidate the binding interface between proteins (Dos Santos et al., 2015; Ovchinnikov et al., 2014; Hopf et al., 2014) or to analyse conformational changes (Parisi et al., 2015; Morcos et al., 2013). Only in 2015, ve studies (Adhikari et al., 2015; Mabrouk et al., 2015; Pietal et al., 2015; Tang et al., 2015; Wang and Barth, 2015) present protocols for contact assisted de novo protein structure prediction which highlights the immense interest to tap the full potential of evolutionary derived information protein-protein interactions generation of the multiple sequence alignment for EC methods is challenging because orthologous interacting pairs of sequences must reliably be identified from a large number of species Typically, sequences from individual alignments are paired using bacterial genome coordinates, i.e. the closer their location in the genome the more likely their co-expression and physical interaction (Ovchinnikov et al., 2014; Hopf et al., 2014; Skerker et al., 2008). A second, simpler method: matches orthologous sequence pairs using genome BLAST scores (Iserte et al., 2015; Yu et al., 2016; Ochoa &amp; Pazos, 2010) and new methods may well improve the accuracy for these organisms (Gueudre´ et al., 2016; Bitbol et al., 2016) case study: Feinauer2016a alternative conformations Sfriso2016 Parisi2015a rna structure prediction “Structural biology: RNA structure from sequence” tal nawy, 2016 nature mutational landscapes EVmutation [101]: unsupervised statistical method for predicting the effects of mutations that explicitly captures residue dependencies between positions assess the quantitative effects of mutations in genes of any organism “Maximum-Entropy Models of Sequenced Immune Repertoires Predict Antigen-Antibody Affinity.” protein design Franceus2016 gene interactions [102] genomeDCA identify epistatic loci == polymorphic loci under strongest coevolutionary pressure = identify coevolving locus pairs (underlying ppi that is under evolutionart pressure) result: &gt; 3/4 coevolution loci found in genes that determine beta-lactam (antibiotic) resistence network deconvolution domain parsing [sadowski 2013] Predicting domain boundaries using predicted contacts (Fig. 1) is based on a very simple idea: that native contacts, and hence predicted contacts, are more abundant within domains than between domains distinguish properly folded proteins from crap (“Correlated mutations distinguish misfolded and properly folded proteins”): DCA correctly predicts significantly more contacts for properly folded structures than for misfolded ones properly oriented model from mirror models: Kurczynska2016 model wuality assessment: “QAcon: single model quality assessment using protein structural and contact information with machine learning techniques” Simkovic2017 review!!! Tang2015 NMR Fox2016 : turn it around: uses DCA to evaluate MSA quality; rationale: better alignments give better contact preictions References "],
["intro-cp-evaluation.html", "2.6 Evaluating Contact Prediction Methods", " 2.6 Evaluating Contact Prediction Methods Choosing an appropriate benchmark for contact prediction is determined by the further utilization of the predictions. Most prominently, predicted contacts are used to assist structure prediction as outlined in the last section 2.5.1. Therefore, one could assess the quality of structural models computed with the help of predicted contacts. However, predicting structural models adds not only another layer of computational complexity but also raises questions about implementation details of the folding protocol. It has been found that in general a small number of accurate contacts is sufficient to constrain the overal protein fold as already discussed. From these considerations emerged various standard benchmarks that have been established by the CASP community over many years [75,103,104]. CASP, the well-respected and independent competition for the structural bioinformatic’s community introduced the contact prediction category in 1996. Taking place every two years, the progress in the field is assessed in a blind competition and the community discusses the outcome in a subsequent meeting. According to the CASP regulations, a pair of residues is defined to be in physical contact when the distance between their \\(\\Cb\\) atoms (\\(C_{\\alpha}\\) in case of glycine) is less than \\(8 \\angstrom\\) in the reference protein structure. The overall performance of a contact predictor is evaluated by the mean precision over a testset of proteins with known high quality 3D structures against the top scoring predictions from every protein. The number of top scoring predictions per protein is typically normalized with respect to protein length \\(L\\) and precision is defined as the number of true contacts among the top scoring predicted contacts. A popular variant of this benchmark plot shows the mean precision of a certain fraction of top ranked predictions (e.g. L/5 top ranked predictions) against specific properties of the test proteins such as protein length or alignment depth [105]. During CASP11 further evaluation metrics have been introduced, such as Matthews correlation coefficient, area under the precision-recall curve or F1 measure but they are rarely used in studies [75]. Currently best methods perform in the range XXX. Sequence feature based methods: Their performance is less dependent on the number of available sequence homologs compared to coevolution methods and therefore they can outperform pure coevolution methods in low data ranges [56,106]. TODOOOPLOT 2.6.1 Sequence Separation Local residue pairs separated by only some positions in sequence (e.g \\(|i-j| &lt; 6\\)) are usually filtered out for evaluating contact prediction methods. They are trivial to predict as they typically correspond to contacts within secondary structure elements and reflect the local geometrical constraints. Figure 2.4 shows the distribution of \\(\\Cb\\) distances for various minimal sequence separation thresholds. Without filtering local residue pairs (sequence separation 1), there are several additional peaks in the distribution around \\(5.5\\angstrom\\), \\(7.4\\angstrom\\) and \\(10.6\\angstrom\\) that can be attributed to local interactions in e.g. helices (see Figure 2.5). Figure 2.4: Distribution of residue pair \\(\\Cb\\) distances over 6741 proteins in the dataset (see Methods 8.1) at different minimal sequence separation thresholds. Figure 2.5: \\(\\Cb\\) distances between neighboring residues in \\(\\alpha\\)-helices. Left: Direct neighbors in \\(\\alpha\\)-helices have \\(\\Cb\\) distances around \\(5.4\\angstrom\\) due to the geometrical constraints from \\(\\alpha\\)-helical architecture. Right: Residues separated by two positions (\\(|i-j| = 2\\)) are less geometrically restricted to \\(\\Cb\\) distances between \\(7\\angstrom\\) and \\(7.5\\angstrom\\). Commonly, sequence separation bins are applied to distuinguish short (\\(6 &lt; |i-j| \\le 12\\)), medium (\\(12 &lt; |i-j| \\le 24\\)) and long range (\\(|i-j| &gt; 24\\)) contacts [75,104]. Especially long range contacts are of importance for structure prediction as they are the most informative and able to constrain the overal fold of a protein [103]. 2.6.2 Interpretation of Evaluation Results There are certain subtleties to be considered when interpreting contact prediction evaluation results. First of all, the rigid \\(\\Cb\\) distance definition of a contact is a very rough measure of true physical interactions between amino acid sidechains. More importantly, interactions between sidechains depend on their physico-chemical properties, on their orientation and different environments within proteins (see section 1.1.1) [107]. A simple \\(\\Cb\\) distance threshold not only misses to reflect biological interaction preferences of amino acids but also provides a questionable gold-standard for benchmarking. Other distance thresholds and definitions for physical contacts (e.g minimal atomic distances or distance between functional groups) have been studied as well. In fact, Duarte and colleagues found that using a \\(\\Cb\\) distance threshold between 9\\(\\angstrom\\) and 11\\(\\angstrom\\) yields optimal results when predicting the 3D structure from the respective contacts [108]. Anishchenko and colleagues analysed false positive predictions with respect to a minimal atom distance threshold \\(&lt; 5 \\angstrom\\), as they found that this cutoff optimally defines direct physical interactions of residue pairs [109]. Definitely, choosing different distance cutoffs and reference atoms for defining a true contact changes the evaluation outcome. Another issue concerns structural variation within a protein family. Evolutionary couplings are inferred from all family members in the MSA and therefore predicted contacts might be physical contacts in one family member but not in another. Anishchenko et al. could show that more than \\(80\\%\\) of false positives at intermediate distances (minimal heavy atom distance 5-15\\(\\angstrom\\)) are true contacts in at least one homolog structure [109]. Therefore, choosing the right trade-off between sensitivity and specificity when generating alignments is a crucial step as well as choosing the target protein structure for evaluation. Finally, an important aspect not considered in the standard benchmarks is the spread of predicted contacts. It is perfectly possible to improve precison of predicted contacts without translating this improvement to better structural models. The reason being that structurally redundant contacts, that is contacts in the immediate sequence neighborhood of other contacts, do not give additional information to constrain the fold [65,81,100]. For example, given a contact between residues \\(i\\) and \\(j\\), there is hardly an added value knowing that there is a contact between residues \\(i\\!+\\!1\\) and \\(j\\!+\\!1\\) when it comes to predicting the overal topology. This observation is highly relevant for deep learning methods due to their unique ability to abstract higher order interactions and recognize contact patterns. Several measures of the contact spread have been developed, like the mean euclidian distance between true and predicted contacts, but are not commonly evaluated yet [81,110]. References "],
["challenges.html", "2.7 Challenges for Coevolutionary Inference", " 2.7 Challenges for Coevolutionary Inference Coevolution methods face several challenges when interpreting the covariation signals obtained from a MSA. Some of these challenges have been successfully met (e.g. disentangling transitive effects with global statistical models), others are still open or open up new perspectives, such as dissecting different sources of coevolution signals. 2.7.1 Phylogenetic Effects as a Source of Noise Sequences in MSAs do not represent independent samples of a protein family. In fact, there is selection bias from sequencing species of special interest (e.g human pathogens) or sequencing closely related species, e.g multiple strains. This uneven sampling of a protein family’s sequence space leaves certain regions unexplored whereas others are statistically overrepresented [80,82,111]. Furthermore, due to their evolutionary relationship, sequences of a protein family have a complicated dependence structure. Closely related sequences can cause spurious correlations between positions, as there was not sufficient time for the sequences to diverge from their common ancestor [42,46,47]. Figure 2.6 illustrates a simplified example, where dependence of sequences due to phylogeny leads to a covariation signal. To reduce the effects of redundant sequences, a popular sequence reweighting strategy has been found to improve contact prediction performance, where every sequence receives a weight that is the inverse of the number of similar sequences according to an identity threshold (see section 8.2.3) [50,80,82,112]. Figure 2.6: The phylogenetic dependence of closely related sequences can produce covariation signals. Here, two independent mutation events (highlighted in red) in two branches of the tree result in a perfect covariation signal for two positions. 2.7.2 Entropic Effects as a Source of Noise Another source for noise is entropy bias that is closely linked to phylogenetic effects. By nature, methods detecting signals from correlated mutations rely on a certain degree of covariation between sequence positions [47]. Highly conserved interactions pose a conceptual challenge, as changes from one amino acid to another cannot be detected if sequences do not vary. This results in generally higher co-evolution signals from positions with high entropy and underestimated signals for highly conserved interactions [40]. Several heuristics have been proposed to reduce entropy effects, such as Row-Column-Weighting (RCW) [42] or Average Product Correction (APC) [43] (see section 2.4.4). 2.7.3 Finite Sampling Effects Spurious correlations can arise from random statistical noise and blur true co-evolution signals especially in low data scenarios. Consequently, false positive predictions attributable to random noise accumulate for protein families comprising low numbers of homologous sequences. This relationship was confirmed in many studies and as a rule of thumb it has been argued that proteins with \\(L\\) residues need at least 5L sequences in order to obtain confident predictions that can bet used for protein structure prediction [88,111]. Recently it was shown that precision of predicted contacts saturates for protein families with more than \\(10^3\\) diverse sequences and that precision is only dependent on protein length for families with small number of sequences [109]. Interesting targets for contact prediction are protein families without any associated structural information. As can be seen in Figure 2.7, those protein families generally comprise low numbers of homologous sequences with a median of 185 sequences per family and are thus susceptible to finite sampling effects. With the rapidly increasing size of protein sequence databases (see section 1) the number of protein families with enough sequences for accuarate contact predictions will increase steadily [10,88]. Nevertheless, because of the already mentioned sequencing biases, better and more sensitive statistical models are indespensible to extend the applicability domain of coevolutionary methods. Figure 2.7: Distribution of PFAM family sizes. Less than half of the families in PFAM (7990 compared to 8489 families) do not have an annotated structure. The median family size in number of sequences for families with and without annotated structures is 185 and 827 respectively. Data taken from PFAM 31.0 (March 2017, 16712 entries) [113]. 2.7.4 Multiple Sequence Alignments A correct MSA is the essential starting point for coevolution analysis as incorrectly aligned residues will confound the true signal. Highly sensitive and accurate alignment tools such as HHblits generate high quality alignments suitable for contact prediction [114]. However, there are certain subtleties to be kept in mind when generating alignments. For example, proteins with repeated stretches of amino acids or with regions of low complexity are notoriously hard to align. Especially, repeat proteins have been found to produce many false positive contact predictions [109]. Therefore, MSAs need to be generated with great care and covariation methods need to be tailored to these specific types of proteins [115,116]. Furthermore, sensitivity of sequence search is critically dependent on the research question at hand and on the protein family under study. Many diverse sequences in general increase precision of predictions [105,117]. However, deep alignments can capture coevolutionary signals from different subfamilies [118]. If only a specific subfamily is of interest, many false predictions might arise from strong coevolutionary signals specific to another subfamily that constitutes a prominent subset in the alignment. Therefore, a trade-off between specificity and diversity of the alignment is required to reach optimal results [119]. Another intrinsic characteristic of MSAs are repeated stretches of gaps that result from commonly utilized gap-penalty schemes assigning large penalties to insert a gap and lower penalties to gap extensions. Most statistical coevolution models for contact prediction treat gaps as the 21st amino acid. This introduces an imbalance as gaps and amino acids express different behaviours which can result in gap-induced artefacts [95]. 2.7.5 Alternative Sources of Coevolution Coevolutionary signals can not only arise from intra-domain contacts, but also from other sources, like homo-oligomeric contacts, alternative conformations, ligand-mediated interactions or even contacts over hetero-oligomeric interfaces (see Figure 2.8) [111]. With the objective to predict physical contacts it is therefore necessary to identify and filter these alternative sources of coevolutionary couplings. Figure 2.8: Possible sources of coevolutionary signals. a) Physical interactions between intra-domain residues. b) Interactions across the interface of predominantly homo-oligomeric complexes. c) Interactions mediated by ligands or metal atoms. d) Transient interactions due to conformational flexibility. Many proteins form homo-oligomers with evolutionary conserved interaction surfaces (Figure 2.8 b). Currently it is hard to reliably distinguish intra- and inter-molecular contacts [118]. Anishchenko et al. found that approximately one third of strong co-evolutionary signals between residue pairs at long distances (minimal heavy atom distance &gt;15\\(\\angstrom\\)) can be attributed to interactions across homo-oligomeric interfaces [109]. Several studies specifically analysed co-evolution across homo-oligomeric interfaces for proteins of known structure by filtering for residue pairs with strong couplings at long distances [118–123] or used co-evolutionary signals to predict homo-dimeric complexes [124]. It has been proposed that co-evolutionary signals can also arise from ligand or atom mediated interactions between residues or from critical interactions in intermediate folding states (Figure 2.8 c) [112,125]. Confirming this hypothesis, a study showed that the cumulative strength of couplings for a particular residue can be used to predict functional sites [111,119]. Another important aspect is conformational flexibility (Figure 2.8 c). PDB structures used to evaluate coevolution methods represent only rigid snapshots taken in an unnatural crystalline environment. Yet proteins possess huge conformational plasticity and can adopt distinct alternative conformations or adapt shape when interacting with other proteins in an induced fit manner [126]. Several studies demonstrated successfully that coevolutionary signals can capture interactions specific to different distinct conformations [80,119,123,127]. --> References "],
["developing-a-bayesian-model-for-contact-prediction.html", "3 Developing a Bayesian Model for Contact Prediction", " 3 Developing a Bayesian Model for Contact Prediction Coevolution methods ad The most popular and successfull methods for contact prediction optimize the pseudo-log-likelihood of the MSA and use several heuristics to calculate a contact score (see section 2.4.4). By doing so valuable information in contact matrices is lost. Analyses in section 1 shows what information is contained in coupling matrices and that the signal in coupling matrices varies with \\(\\Cb\\) distance. This thesis introcudes a principled Bayesian statistical approach that eradicates these heuristics to fully exploit the information in coupling matrices. Instead of transforming the model parameters \\(\\w\\) into heuristic contact scores, one can compute the posterior probability distributions of the distances \\(r_{ij}\\) between \\(\\Cb\\) atoms of all residues pairs \\(i\\) and \\(j\\), given the MSA \\(\\X\\). The coupling parameters \\(\\w\\) are treated as hidden variables that will be integrated out analytically. This approach also allows for extraction of information contained in the particular types of amino acids, since each pair of amino acids will have a different preference to be coupled at certain distances. TODO Figure ! ! In section 2 introduces max ent model for protein families that will produce the model parameters for the Bayesian model. In section 3 describes in detail how the posterior distribution of distances can be computed. Section 4 presents the optimizaton of the coupling prior. And the Bayesian model will be evalutated in section 5. The outlook describes an extension of the model to predict inter-residue distances. Development is ongoing. "],
["interpreting-coupling-matrices.html", "4 Interpretation of Coupling Matrices", " 4 Interpretation of Coupling Matrices Contact prediction methods learning a Potts model for the MSA of a protein familiy, map the inferred 20 x 20 coupling matrices \\(w_{ij}\\) onto scalar values to obtain contact scores for each residue pair as outlined in section 2.4.4. By doing so, the full information contained in coupling matrices is lost: the contribution of individual couplings \\(\\wijab\\) the direction of couplings (positive or negative) the correlation between couplings \\(\\wijab\\) and \\(\\wijcd\\) inherent biological meaning The following analyses give some intuition for the information contained in coupling matrices. "],
["correlation-between-couplings-and-class.html", "4.1 Single Coupling Values Carry Evidence of Contacts", " 4.1 Single Coupling Values Carry Evidence of Contacts Given the success of DCA methods, it is clear that the inferred couplings \\(\\wij\\) are good indicators of spatial proximity for residue pairs. As described in section 2.4.4, a contact score for a residue pair is commonly computed as the Frobenius norm over the coupling matrix: \\(||\\wij||_2 = \\sqrt{\\sum_{a,b=1}^{20} \\wijab}\\) The left plot in Figure 4.1 shows the correlation between squared coupling values \\((\\wijab)^2\\) and binary contact class (contact=1, non-contact=0) for approximately 100.000 residue pairs per class (for details see methods section 8.3.1). All couplings have a positive class correlation, meaning the stronger the squared coupling value, the more likely a contact can be inferred. Generally, couplings that involve any aliphatic amino acid (I, L, V) or alanine express the strongest class correlation. In contrast, C-C or pairs involving charged residus (R, E, K, D) or tryptophane correlate only weakly with contact class. Interestingly, C-C and couplings involving charged residues have the highest standard-deviation among all couplings as can be seen in the right plot in Figure 4.1. It can be hypothesized that these couplings considerably contribute to false positive predictions when using the Frobenius norm as a contact score; they can have high squared values (high standard deviation) that do not correlate well with being a contact. Figure 4.1: Left Pearson correlation of squared coupling values \\((\\wijab)^2\\) with contact class (contact=1, non-contact=0). Right Standard deviation of squared coupling values. Dataset contains 100.000 residue pairs per class (for details see methods section 8.3.1). Contacts are defined as residue pairs with \\(\\Cb &lt; 8 \\angstrom\\) and non-contacts as residue pairs with \\(\\Cb &gt; 25 \\angstrom\\). Apparantly, different couplings are of varying importance for contact inference and have distinct characteristics. But looking at the raw coupling values (without squaring), these charateristics become even more pronounced. The left plot in Figure 4.2 shows the correlation of raw coupling values \\(\\wijab\\) with contact class. Interestingly, in contrast to the findings for squared coupling values, couplings for charged residue pairs (R, E, K, D) have the strongest class correlation (positive and negative), whereas aliphatic couplings correlate to a much lesser extent. This implies that absolute (squared) coupling strength for aliphatic couplings is a better indicator for contacts than the raw signed coupling value. On the contrary, the raw signed coupling values for charged residue pairs are much more indicative of a contact than the sheer magnitude of their squared values. Raw couplings for aromatic pairs or C-C pairs correlate only weakly with contact class. For these pairs neither coupling strength, nor the sign of the coupling value seems to be a good indicator for a contact. Figure 4.2: Left Pearson correlation of raw signed coupling values \\(\\wijab\\) with contact class (contact=1, non-contact=0). Right Standard deviation of coupling values. Dataset contains 100.000 residue pairs per class (for details see section 8.3.1). Contacts are defined as residue pairs with \\(\\Cb &lt; 8 \\angstrom\\) and non-contacts as residue pairs with \\(\\Cb &gt; 25 \\angstrom\\). Of course, looking only at correlations can be misleading if there are non-linear patterns in the data, for example higher order dependencies between couplings. For this reason it is advisable to take a more detailed view at coupling matrices and the distributions of their values. "],
["physico-chemical-fingerprints-in-coupling-matrices.html", "4.2 Physico-Chemical Fingerprints in Coupling Matrices", " 4.2 Physico-Chemical Fingerprints in Coupling Matrices The correlation analysis of coupling matrices in the last section revealed that certain couplings are more indicative of a contact than others. Individual coupling matrices for a residue pair that is in physcial contact often display striking patterns that agree with the previous findings. Most often, these patterns allow a biological interpreation of the coupling values that reveal details of the interdependency between both residues. Figure 4.3 visualizes the inferred coupling matrix for a residue pair using the pseudo-likelihood method. Clearly visible is a cluster of strong coupling values for charged and polar residues (E,D,K,R,Q). Positive coupling values can be observed between positively charged residues (R,K) and negatively charged residues (E,D), whereas couplings between equally charged residues have negative values. The coupling matrix perfectly reflects the interaction preference for residues forming salt bridges. Indeed, in the protein structure the first residue (glutamic acid) forms a salt bridge with the second residue (lysine) as can be seen in the left Figure 4.5. Figure 4.3: Coupling matrix computed with pseudo-likelihood for residues 6 and 82 in protein 1awq chain A. Size of the bubbles represents coupling strength and color represents positive (red) and negative (blue) coupling values. Bars at the x-axis and y-axis represent the corresponding single potentials for both residue positions. Height of the bars stands for potential strength and color for positive (red) and negative (blue) values. Figure 4.4 visualizes the coupling matrix for a pair of hydrophobic residues. Hydrophobic pairings have strong coupling values but the couplings also reflect a sterical constraint. Alanine as a small hydrophobic residue is favoured at either of both residue positions as it has strong positive couplings with isoleucine, leucine and methionine. But alanine is disfavoured to appear at both positions at the same time as the A-A coupling is negative. Figure 4.5 illustrates the location of the two residues in the protein core. Here, hydrophobic residues are densely packed and the limited space allows for only small hydrophobic residues. Figure 4.4: Coupling matrix computed with pseudo-likelihood for residues 29 and 39 in protein 1ae9 chain A. Size of the bubbles represents coupling strength and color represents positive (red) and negative (blue) coupling values. Bars at the x-axis and y-axis represent the corresponding single potentials for both residues. Height of the bars stands for potential strength and color for positive (red) and negative (blue) values. --> Figure 4.5: Interactions between protein side chains. Left: residue 6 (glutamic acid) forms a salt bridge with residue 82 (lysine) in protein 1awq, chain A. Right: residue 29 (alanine) and residue 39 (leucine) within the hydrophobic core of protein 1ae9 chain A. Many more biological interpretable signals can be identified from coupling matrices, including pi-cation interactions (see Appendix C.1), aromatic-proline interactions (see Appendix C.3), sulfur-aromatic interactions or disulphide bonds (see Appendix C.2). Coucke and collegues performed a thorough quantitative analysis of coupling matrices selected from confidently predicted residue pairs [128]. They showed that eigenmodes obtained from a spectral analysis of averaged coupling matrices are closely related to physico-chemical properties of amino acid interactions, like electrostaticity, hydrophobicity, steric interactions or disulphide bonds. By looking at specific populations of residues, like buried and exposed residues or residues from specific protein classes (small, mainly \\(\\alpha\\), etc), the eigenmodes of corresponding coupling matrices are found to capture very characteristic interactions for each class, e.g. rare disulfide contacts within small proteins and hydrophilic contacts between exposed residues. Their study confirms the qualitative observations presented above that amino acid interactions can leave characteristic physico-chemical fingerprints in coupling matrices. References "],
["coupling-profiles-vary-with-distance.html", "4.3 Coupling Profiles Vary with Distance", " 4.3 Coupling Profiles Vary with Distance Analyses in the previous sections showed that certain coupling values correlate more or less strong with contact class and that coupling matrices for contacts express biological meaningfull patterns. More insights can be obtained by looking at the distribution of distinct coupling values for contacts, non-contacts and arbitrary populations of residue pairs. Figure 4.6 shows the distribution of selected couplings for filtered residue pairs within a \\(\\Cb\\) distance \\(&lt; 5\\angstrom\\) (see methods section 8.3.2 for details). The distribution of R-E and E-E coupling values is shifted and skewed towards positive and negative values respectively. This is in accordance with attracting electrostatic interactions between the positively charged side chain of arginine and the negatively charged side chain of gluatamic acid and also with repulsive interactions between the two negatively charged gluatamic acid side chains. Coupling values for cysteine pairs (C-C) have a broad distribution that is skewed towards positive values, reflecting the strong signals obtained from covalent disulphide bonds. The broad distribution for C-C, R-E and E-E agrees with the observation in section 4.1 that these specific coupling values have large standard deviations and that for charged residue pairings the signed coupling value is a strong indicator of a contact. Hydrophobic pairs like V-I have an almost symmetric coupling distribution, confirming the finding that the direction of coupling is not indicative of a true contact whereas the strength of the coupling is. The hydrophobic effect that determines hydrophobic interactions is not specific or directed. Therefore, hydrophobic interaction partners can commonly be substituted by other hydrophobic residues, which explains the not very pronounced positive coupling signal compared to more specific interactions, e.g ionic interactions. The distribution of aromatic coupling values like F-W is slightly skewed towards negative values, accounting for steric hindrance of their large side chains at small distances. Figure 4.6: Distribution of selected couplings for filtered residue pairs with \\(\\Cb\\) distance \\(&lt; 5\\angstrom\\) (see methods section 8.3.2 for details). Number of coupling values used to determine the distribution is given in brackets in the legend. R-E = coupling for pairings of arginine and glutamic acid, C-C = coupling for pairings between cystein residues, V-I = coupling for pairings of valine and isoleucine, F-W = coupling for pairing sof phenylalanine and tryptophane, E-E = coupling for pairings between glutamic acid residues. In an intermediate \\(\\Cb\\) distance range between \\(8\\angstrom\\) and \\(12\\angstrom\\) the distributions for all coupling values are centered close to zero and are less broad. The distributions are still shifted and skewed, but much less pronounced as it has been observed for the distributions at \\(\\Cb\\) distance \\(&lt; 5\\angstrom\\). For aromatic pairs like F-W, the distribution of coupling values has very long tails, suggesting rare but strong couplings for aroamtic side chains at this distance. Figure 4.7: Distribution of selected couplings for filtered residue pairs with \\(\\Cb\\) distances between \\(8\\angstrom\\) and \\(12 \\angstrom\\) (see methods section 8.3.2 for details). Number of coupling values used to determine the distribution is given in brackets in the legend. R-E = coupling for pairings of arginine and glutamic acid, C-C = coupling for pairings between cystein residues, V-I = coupling for pairings of valine and isoleucine, F-W = coupling for pairing sof phenylalanine and tryptophane, E-E = coupling for pairings between glutamic acid residues. Figure 4.8 shows the distribution of selected couplings for residue pairs far apart in the protein structure (\\(\\Cb\\) distance \\(&gt; 20\\angstrom\\)). The distribution for all couplings is centered at zero and has small variance. Only for C-C coupling values, the distribution has a long tail for positve values, presumably arising from the fact that the maximum entropy model cannot distuinguish highly conserved signals of multiple disulphide bonds within a protein. This observation also agrees with the previous finding in section 4.1 that C-C coupling values, albeit having large standard-deviations, correlate only weakly with contact class. The same arguments apply to couplings of aromatic pairs that have a comparably broad distribution and do not correlate strongly with contact class. The strong coevolution signals for aromatic pairs even at high distance ranges might be a result of insufficient disentangling of transitive effects, as aromatic residues are known to form network-like structures in the protein core that stabilize protein structure (see Figure C.7 in Appendix)[14]. Figure 4.8: Distribution of selected couplings for filtered residue pairs with \\(\\Cb\\) distances between \\(20\\angstrom\\) and \\(50 \\angstrom\\) (see methods section 8.3.2 for details). Number of coupling values used to determine the distribution is given in brackets in the legend. R-E = coupling for pairings of arginine and glutamic acid, C-C = coupling for pairings between cystein residues, V-I = coupling for pairings of valine and isoleucine, F-W = coupling for pairing sof phenylalanine and tryptophane, E-E = coupling for pairings between glutamic acid residues. References "],
["higher-order-dependencies-between-couplings.html", "4.4 Higher Order Dependencies Between Couplings", " 4.4 Higher Order Dependencies Between Couplings The analyses in the previous sections focused on single coupling values picked from the \\(20 \\times 20\\)-dimensional coupling matrices \\(\\wij\\). As mentioned before, analysing only single dimension might be misleading and further insights might be concealed in a higher order relationships. Unfortunately, it is not possible to reasonably visualize the high dimensional coupling matrices. But taking a look at two dimensional coupling scatter plots already reveals some further insights and confirms the previous observations that couplings reflect biological relevant amino acid interactions. Figure 4.9 and 4.10 illustrate the distribution of couplings at \\(\\Cb\\) distances less than \\(8\\angstrom\\) between the attractive ionic pairings of E-R and R-E and between the ionic pairing R-E and the repulsive pair of equally charged residues E-E, respectively. Whereas coupling values for R-E and E-R are positively correlated, coupling values for R-E and E-E are negatively correlated. Figure 4.9: Two-dimensional distribution of coupling values R-E and E-R for approximately 10000 residue pairs with \\(\\Delta\\Cb &lt; 8\\angstrom\\). The distribution is almost symmetric and the coupling values are positively correlated. Residue pairs have been filtered for sequence separation, percentage of gaps and evidence in alignment as explained in methods section 8.3.2. Figure 4.10: Two-dimensional distribution of coupling values R-E and E-E for approximately 10000 residue pairs with \\(\\Delta\\Cb &lt; 8\\angstrom\\). The distribution is almost symmetric and the coupling values are negatively correlated. Residue pairs have been filtered for sequence separation, percentage of gaps and evidence in alignment as explained in methods section 8.3.2. Figure 4.11 shows distributions between couplings for hydrophobic pairings that are almost symmetric and broadly centered around zero. Coupling distributions for residue pairs that are not physically interacting (\\(\\Cb &gt;&gt; 8 \\angstrom\\)) resemble the distribution for hydrophobic pairings in that there is no correlation, but at high distance the distributions are much tighter centered around zero. Figure 4.11: Two-dimensional distribution of coupling values R-E and E-E for approximately 10000 residue pairs with \\(\\Delta\\Cb &lt; 8\\angstrom\\). The coupling values are symmetrically distributed around zero without visible correlation. Residue pairs have been filtered for sequence separation, percentage of gaps and evidence in alignment as explained in methods section 8.3.2. -->"],
["optimizing-full-likelihood.html", "5 Optimizing the Full-Likelihood", " 5 Optimizing the Full-Likelihood Section 2.4 introduced the Potts model for contact prediction that is able to distinguish between directly and indirectly coupled residue pairs by jointly modelling the probabilty of a protein sequence over all residues. Maximum-likelihood inference of the model parameters is numerically challenging due to the exponential complexity of the partition function that normalizes the probability distribution. Several approximate inference techniques for the full likelihood have been developed trying to sidestep the exact computation of the partition function. At this point in time, pseudo-likelihood is the most successful approximate solution with regard to the specific problem of predicting residue-residue contacts. It has been shown that the pseudo-likelihood is a consistent estimator to the full likelihood in the limit of large amounts of data, however, it is unclear whether it represents a good approximation when there is only little data, in other words for small protein families that are typical of contact prediction. Computing the gradient of the likelihood analytically is infeasible, because computing \\(p(x_i \\eq a, x_j \\eq b | \\v, \\w) = \\sum_{y_1, \\dots, y_L =1}^{20} p(y_1, \\dots, y_L | \\v, \\w) I(y_i \\eq a, y_j \\eq b)\\) would require summing over \\(20^L\\) sequences \\((y_1,\\ldots,y_L)\\). Several approaches have been used to get around this problem as described in section ??. The most popular one for protein contact prediction is to optimize the pseudo likelihood instead (see section 2.4.3.1). Its gradient involves a sum over just the 20 amino acids instead of over all possible sequences of length \\(L\\). While the function value of the full likelihood cannot efficiently be computed, it is possible to approximate the gradient of the full likelihood with an approach called contrastive divergence that makes use of MCMC sampling techniques [129]. In the next sections I am going to specify the exact model that differs from the Potts model as it is used for pseudo-likelihood inference as explained in section @ref(pseudo-likelihood}) in some small details. Then I will discuss how the gradient of the likelihood can be approximated with contrastive divergence and present the results for this optimization stratedgy. References "],
["the-likelihood-of-the-sequences-as-a-potts-model.html", "5.1 The Likelihood of the Sequences as a Potts Model", " 5.1 The Likelihood of the Sequences as a Potts Model The \\(N\\) sequences of the MSA \\(\\X\\) are denoted as \\({\\seq_1, ..., \\seq_N}\\). Each sequence \\(\\seq_n = (\\seq_{n1}, ..., \\seq_{nL})\\) is a string of \\(L\\) letters from an alphabet indexed by \\(\\{0, ..., 20\\}\\), where 0 stands for a gap and \\(\\{1, ... , 20\\}\\) stand for the 20 types of amino acids. As already described in detail in section 2.4, the likelihood of the sequences in the MSA of the protein family is modelled with a Potts Model: \\[\\begin{align} p(\\X | \\v, \\w) &amp;= \\prod_{n=1}^N p(\\seq_n | \\v, \\w) \\nonumber \\\\ &amp;= \\prod_{n=1}^N \\frac{1}{Z(\\v, \\w)} \\exp \\left( \\sum_{i=1}^L v_i(x_{ni}) \\sum_{1 \\leq i &lt; j \\leq L} w_{ij}(x_{ni}, x_{nj}) \\right) \\end{align}\\] The coefficients \\(\\via\\) and \\(\\wijab\\) are referred to as single potentials and couplings, respectively that describe the tendency of an amino acid a (and b) to (co-)occur at the respective positions in the MSA. \\(Z(\\v, \\w)\\) is the partition function that normalizes the probability distribution \\(p(\\seq_n |\\v, \\w)\\): \\[\\begin{equation} Z(\\v, \\w) = \\sum_{y_1, ..., y_L = 1}^{20} \\exp \\left( \\sum_{i=1}^L v_i(y_i) \\sum_{1 \\leq i &lt; j \\leq L} w_{ij}(y_i, y_j) \\right) \\end{equation}\\] "],
["gap-treatment.html", "5.2 Treating Gaps as Missing Information", " 5.2 Treating Gaps as Missing Information Treating gaps explicitly as 0’th letter of the alphabet will lead to couplings between columns that are not in physical contact. To see why, imagine a hypothetical alignment consisting of two sets of sequences as it is illustrated in Figure 5.1. The first set has sequences covering only the left half of columns in the MSA, while the second set has sequences covering only the right half of columns. The two blocks could correspond to protein domains that were aligned to a single query sequence. Now consider couplings between a pair of columns \\(i, j\\) with \\(i\\) from the left half and \\(j\\) from the right half. Since no sequence (except the single query sequence) overlaps both domains, the empirical amino acid pair frequencies \\(q(x_i = a, x_j = b)\\) will vanish for all \\(a, b \\in \\{1,... , L\\}\\). Figure 5.1: Hypothetical MSA consisting of two sets of sequences: the first set has sequences covering only the left half of columns, while the second set has sequences covering only the right half of columns. The two blocks could correspond to protein domains that were aligned to a single query sequence. Empirical amino acid pair frequencies \\(q(x_i \\eq a, x_j \\eq b)\\) will vanish for positions \\(i\\) from the left half and \\(j\\) from the right half of the alignment. The gradient of the log likelihood for couplings is \\[\\begin{align} \\frac{\\partial LL}{\\partial \\wijab} &amp;= \\sum_{n=1}^N I(x_{ni}=a, x_{nj}=b) - N \\frac{\\partial}{\\partial \\wijab} \\log Z(\\v,\\w) \\\\ &amp;= \\sum_{n=1}^N I(x_{ni} \\eq a, x_{nj} \\eq b) \\\\ &amp; - N \\sum_{y_1,\\ldots,y_L=1}^{20} \\!\\! \\frac{ \\exp \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L} w_{ij}(y_i,y_j) \\right)}{Z(\\v,\\w)} I(y_i \\eq a, y_j \\eq b) \\\\ &amp;= N q(x_{i} \\eq a, x_{j} \\eq b) - N \\sum_{y_1,\\ldots,y_L=1}^{20} p(y_1, \\ldots, y_L | \\v,\\w) \\, I(y_i \\eq a, y_j \\eq b) \\\\ &amp;= N q(x_{i} \\eq a, x_{j} \\eq b) - N p(x_i \\eq a, x_j \\eq b | \\v,\\w) \\tag{5.1} \\end{align}\\] Note that the empirical frequencies \\(q(x_{i} \\eq a, x_{j} \\eq b)\\) are equal to the model probabilities \\(p(x_i \\eq a, x_j \\eq b | \\v,\\w)\\) at the maximum of the likelihood when the gradient vanishes. Therefore, \\(p(x_i \\eq a, x_j \\eq b | \\v, \\w)\\) would have to be zero in the optimum when the empirical amino acid frequencies \\(q(x_i \\eq a, x_j \\eq b)\\) vanish for pairs of columns as described above. However, \\(p(x_i \\eq a, x_j \\eq b | \\v, \\w)\\) can only become zero, when the exponential term is zero, which would only be possible if \\(\\wijab\\) goes to \\(−\\infty\\). This is clearly undesirable, as physical contacts will be deduced from the size of the couplings. The solution is to treat gaps as missing information. This means that the normalisation of \\(p(\\seq_n | \\v, \\w)\\) should not run over all positions \\(i \\in \\{1,... , L\\}\\) but only over those \\(i\\) that are not gaps in \\(\\seq_n\\). Therefore, the set of sequences \\(\\Sn\\) used for normalization of \\(p(\\seq_n | \\v, \\w)\\) in the partition function will be defined as: \\[\\begin{equation} \\Sn := \\{(y_1,... , y_L): 0 \\leq y_i \\leq 20 \\land (y_i \\eq 0 \\textrm{ iff } x_{ni} \\eq 0) \\} \\end{equation}\\] and the partition function becomes: \\[\\begin{equation} Z_n(\\v, \\w) = \\sum_{\\mathbf{y} \\in \\Sn} \\exp \\left( \\sum_{i=1}^L v_i(y_i) \\sum_{1 \\leq i &lt; j \\leq L} w_{ij}(y_i, y_j) \\right) \\end{equation}\\] To ensure that the gaps in \\(x_n\\) do not contribute anything to the sums, the parameters associated with a gap will be fixed to 0: \\[ \\vi(0) = \\wij(0, b) = \\wij(a, 0) = 0 \\; , \\] for all \\(i, j \\in \\{1, ..., L\\}\\) and \\(a, b \\in \\{0, ..., 20\\}\\). Furthermore, the empirical amino acid frequencies \\(q_{ia}\\) and \\(q_{ijab}\\) need to be redefined such that they are normalised over \\(\\{1, ..., 20\\}\\): \\[\\begin{align} N_i :=&amp; \\sum_{n=1}^N I(x_{ni} \\!\\ne\\! 0) &amp; q_{ia} = q(x_i \\eq a) :=&amp; \\frac{1}{N_i} \\sum_{n=1}^N I(x_{ni} \\eq a) \\\\ N_{ij} :=&amp; \\sum_{n=1}^N I(x_{ni} \\!\\ne\\! 0, x_{nj} \\!\\ne\\! 0) &amp; q_{ijab} = q(x_i \\eq a, x_j \\eq b) :=&amp; \\frac{1}{N_{ij}} \\sum_{n=1}^N I(x_{ni} \\eq a, x_{nj} \\eq b) \\end{align}\\] With this definition, empirical amino acid frequencies are normalized without gaps, so that \\[\\begin{equation} \\sum_{a=1}^{20} q_{ia} = 1 \\; , \\; \\sum_{a,b=1}^{20} q_{ijab} = 1. \\tag{5.2} \\end{equation}\\] "],
["the-regularized-log-likelihood-and-its-gradient.html", "5.3 The Regularized Log Likelihood and its Gradient", " 5.3 The Regularized Log Likelihood and its Gradient As with pseudo-likelihood based methods, Gaussian priors \\(\\mathcal{N}( \\v | \\v^*, \\lambda_v^{-1} \\I)\\) and \\(\\mathcal{N}( \\w |\\boldsymbol 0, \\lambda_w^{-1} \\I)\\) will be used to constrain the parameters \\(\\v\\) and \\(\\w\\) and to fix the Gauge (see section 2.4.3.1). The choice of \\(v^*\\) will be discussed in section 5.4. By including the logarithm of this prior into the log likelihood using the gap treatment described in the last section, the regularised likelihood is obtained, \\[\\begin{equation} \\LLreg(\\v,\\w) = \\log \\left[ p(\\X | \\v,\\w) \\; \\Gauss (\\v | \\v^*, \\lambda_v^{-1} \\I) \\; \\Gauss( \\w | \\boldsymbol 0, \\lambda_w^{-1} \\I) \\right] \\end{equation}\\] or explicitely, \\[\\begin{align} \\LLreg(\\v,\\w) =&amp; \\sum_{n=1}^N \\left[ \\sum_{i=1}^L v_i(x_{ni}) + \\sum_{1\\le i&lt;j\\le L} w_{ij}(x_{ni},x_{nj}) - \\log Z_n(\\v,\\w) \\right] \\\\ &amp; - \\frac{\\lambda_v}{2} \\!\\! \\sum_{i=1}^L \\sum_{a=1}^{20} (\\via - \\via^*)^2 - \\frac{\\lambda_w}{2} \\sum_{1 \\le i &lt; j \\le L} \\sum_{a,b=1}^{20} \\wijab^2 . \\end{align}\\] The gradient of the regularized log likelihood has single components \\[\\begin{align} \\frac{\\partial \\LLreg}{\\partial \\via} =&amp; \\sum_{n=1}^N I(x_{ni}=a) - \\sum_{n=1}^N \\frac{\\partial}{\\partial \\via} \\, \\log Z_n(\\v,\\w) - \\lambda_v (\\via - \\via^*)\\\\ =&amp; \\; N_i q(x_i \\eq a) \\\\ &amp; - \\sum_{n=1}^N \\sum_{\\mathbf{y} \\in \\Sn} \\frac{ \\exp \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i&lt;j \\le L}^L w_{ij}(y_i,y_j) \\right) }{Z_n(\\v,\\w)} I(y_i=a) \\\\ &amp; - \\lambda_v (\\via - \\via^*) \\tag{5.3} \\end{align}\\] and pair components \\[\\begin{align} \\frac{\\partial \\LLreg}{\\partial \\wijab} =&amp; \\sum_{n=1}^N I(x_{ni} \\eq a, x_{nj} \\eq b) - \\sum_{n=1}^N \\frac{\\partial}{\\partial \\wijab} \\log Z_n(\\v,\\w) - \\lambda_w \\wijab \\\\ =&amp; \\; N_{ij} q(x_i \\eq a, x_j=b) \\\\ &amp; - \\sum_{n=1}^N \\sum_{\\mathbf{y} \\in \\Sn} \\frac{ \\exp \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i&lt;j \\le L}^L w_{ij}(y_i,y_j) \\right) }{Z_n(\\v,\\w)} I(y_i \\eq a, y_j \\eq b) \\\\ &amp; - \\lambda_w \\wijab \\tag{5.4} \\end{align}\\] Note that (without regulariation \\(\\lambda_v = \\lambda_w = 0\\)) the empirical frequencies \\(q(x_i \\eq a)\\) and \\(q(x_i \\eq a, x_j=b)\\) are equal to the model probabilities at the maximum of the likelihood when the gradient becomes zero. "],
["prior-v.html", "5.4 The prior on \\(\\v\\)", " 5.4 The prior on \\(\\v\\) Most previous approaches chose a prior around the origin, \\(p(\\v) = \\Gauss ( \\v| \\mathbf{0}, \\lambda_v \\I)\\). This choice has an obvious draw-back. Taking the sum over \\(b=1,\\ldots, 20\\) at the optimum of the gradient of couplings in eq. (5.10), yields \\[\\begin{equation} 0 = N_{ij}\\, q(x_i \\eq a, x_j \\ne 0) - N_{ij}\\, p(x_i \\eq a | \\v, \\w) - \\lambda_w \\sum_{b=1}^{20} \\wijab. \\tag{5.5} \\end{equation}\\] Incidentally, by taking the sum over \\(a\\) it follows that, \\[\\begin{equation} \\sum_{a,b=1}^{20} \\wijab = 0. \\tag{5.6} \\end{equation}\\] At the optimum the gradient with respect to \\(\\via\\) vanishes and \\(p(x_i=a|\\v,\\w) = q(x_i=a) - \\lambda_v (\\via - \\via^*) / N_i\\) can be substituted into equation (5.5), yielding \\[\\begin{equation} 0 = N_{ij} \\, q(x_i \\eq a, x_j \\ne 0) - N_{ij} \\, q(x_i=a) + \\frac{N_{ij}}{N_i}\\lambda_v (\\via - \\via^*) - \\lambda_w \\sum_{b=1}^{20} \\wijab \\; , \\tag{5.7} \\end{equation}\\] for all \\(i,j \\in \\{1,\\ldots,L\\}\\) and all \\(a \\in \\{1,\\ldots,20\\}\\). Considering a MSA without gaps, it can be shown how the choice \\(\\v^*= \\mathbf{0}\\) leads to undesirable results. The first two terms \\(N_{ij} \\, q(x_i \\eq a, x_j \\ne 0) - N_{ij} \\, q(x_i=a)\\) cancel out, leaving \\[\\begin{equation} 0 = \\lambda_v (\\via - \\via^*) - \\lambda_w \\sum_{b=1}^{20} \\wijab . \\tag{5.8} \\end{equation}\\] Consider a column \\(i\\) that is not coupled to any other and assume that amino acid \\(a\\) was frequent in column \\(i\\) and therefore \\(\\via\\) would be large and positive. Then according to eq. (5.8), for any other column \\(j\\) the 20 coefficients \\(\\wijab\\) for \\(b \\in \\{1,\\ldots,20\\}\\) would have to take up the bill and deviate from zero! This unwanted behaviour can be corrected by instead choosing a Gaussian prior centered around \\(\\v^*\\) obeying \\[\\begin{equation} \\frac{\\exp(\\via^*)}{\\sum_{a&#39;=1}^{20} \\exp(v_{ia&#39;}^*)} = q(x_i=a) . \\end{equation}\\] This choice ensures that if no columns are coupled, i.e. \\(p(\\seq | \\v,\\w) = \\prod_{i=1}^L p(x_i)\\), \\(\\v=\\v^*\\) and \\(\\w= \\mathbf{0}\\) gives the correct probability model for the sequences in the MSA. Furthermore imposing the restraint \\(\\sum_{a=1}^{20} \\via \\eq 0\\) to fix the gauge of the \\(\\via\\) (i.e. to remove the indeterminacy), yields \\[\\begin{align} \\via^* = \\log q(x_i \\eq a) - \\frac{1}{20} \\sum_{a^{\\prime}=1}^{20} \\log q(x_i \\eq a^{\\prime}) . \\tag{5.9} \\end{align}\\] For this choice, \\(\\via - \\via^*\\) will be approximately zero and will certainly be much smaller than \\(\\via\\), hence the sum over coupling coefficients in eq. (5.8) will be close to zero, as it should be. Another way to understand the choice of \\(\\v^*\\) in eq. (5.9) as opposed to \\(\\v^*=\\mathbf{0}\\) is by noting that in that case \\(q(x_i \\eq a) \\approx p(x_i \\eq a|\\v^*,\\w^*)\\). Therefore, if \\(q(x_i \\eq a,x_j \\eq b) = q(x_i \\eq a) \\, q(x_j \\eq b)\\) it follows that \\(p(x_i \\eq a, x_j \\eq b | \\v,\\w) \\approx q(x_i \\eq a, x_j \\eq b) = p(x_i \\eq a | \\v^*,\\w^*)\\, p(x_j \\eq b | \\v^*,\\w^*)\\), i.e. we would correctly conclude that \\(\\wijab=0\\) and \\((i,a)\\) and \\((j,b)\\) are not coupled. Regarding (5.10): Note that the couplings between columns \\(i\\) and \\(j\\) in the hypothetical MSA presented in the last section 5.2 will now vanish since \\(N_{ij} \\eq 0\\) and the gradient with respect to \\(\\wijab\\) is equal to \\(-\\lambda_w \\wijab\\). "],
["full-likelihood-gradient.html", "5.5 Approximation of the Gradient with Contrastive Divergence", " 5.5 Approximation of the Gradient with Contrastive Divergence If the proportion of gap positions in \\(\\X\\) is small (e.g. \\(&lt;5\\%\\), also compare percentage of gaps in dataset in Appendix Figure B.2), the sums over \\(\\mathbf{y} \\in \\Sn\\) in eqs. (5.3) and (5.4) can be approximated by \\(p(x_i=a | \\v,\\w) I(x_{ni} \\ne 0)\\) and \\(p(x_i=a, x_j=b | \\v,\\w) I(x_{ni} \\ne 0, x_{nj} \\ne 0)\\), respectively, and the partial derivatives become \\[\\begin{align} \\frac{\\partial \\LLreg}{\\partial \\via} =&amp; \\; N_i q(x_i \\eq a) - N_i \\; p(x_i \\eq a | \\v,\\w) - \\lambda_v (\\via - \\via^*) \\\\ \\frac{\\partial \\LLreg}{\\partial \\wijab} =&amp; \\; N_{ij} q(x_i \\eq a, x_j=b) - N_{ij} \\; p(x_i \\eq a, x_j \\eq b | \\v,\\w) - \\lambda_w \\wijab \\tag{5.10} \\end{align}\\] The gradients consist of the empirical single and pairwise amino acid counts, which are constant and can be computed once from the alignment and the model probability terms that cannot be computed analytically. MCMC algorithms are typically used in Bayesian statistics to generate samples from probability distributions that involve the computation of complex integrals and therefore cannot be computed analytically. Samples are generated from an distribution as the current state of a running Markov chain. The equilibrium statistics identical to true prob distribution statistics if MCMC is run long enough Lapedes et al. applied such a scheme in 1999 but running the Markov Chain until reaching its stationary distribution is only feasible for small proteins. Hinton suggests CD as an approximation to MCMC methods. - represents a general framework that is applicable to all kinds of likelihoods evenhtough originally proposed for products of experts models - approach that is applicable to all kinds of ML problems and that makes use of another class of MCMC algorithms: Gibbs sampling. Instead of starting a Markov Chain from a random point, it is initialized from a data sample. It is then evolved for only one Gibbs step and the newly generated samples are used to compute estimate marginal distribution from simple statistics. \\(N\\) Markov chains will be initialized with the \\(N\\) sequences from the MSA and \\(N\\) new samples will be generated by a single step of Gibbs sampling from each of the \\(N\\) sequences. Single and pairwise amino acid counts can be computed from the newly generated sample that correspond to an estimate of the marginal probabilies from the model. Gibbs sampling works only when conditional probabilites of the target distribution can be exactly computed. This is the case and the conditionals look like this: One step of Gibbs sampling samples a new amino acid according to the conditional probability for each position in the sequence (randomly selected). When enough sampling steps are made, samples can be considered independent of one another. gradient is really noisy but hinton showed empirically and later it was shown proven that CD gradients becomes zero when full likelihood is at optimum -Just performing a single Gibbs sampling step away from the original data vectors r n moves the sampled r n,t into the direction of the equilibrium posterior distribution, and therefore promises to yield a similar direction of change as the true gradient of the likelihood. The intuition is that one obtains a rough estimate of the gradient even though the sampler has not reached the equilibrium distribution Even though the approximation for the second term is very bad, it can be seen that this approximate gradient will become zero approximately where the true gradient of the likelihood also becomes zero. To see this, imagine \\((\\v^*, \\w^*)\\) is the maximum of the likelihood. Then, starting from the sequences in the MSA, the Gibbs sampling step should not lead away from the empirical distribution, because the parameters \\((\\v^*, \\w^*)\\) already describe the empirical distribution correctly. This equality of the two maxima is accurate to the extent that the empirical distribution with its finite number of sequences \\(N\\) can represent the true distribution given by parameters \\((\\v^*, \\w^*)\\). Therefore, the larger \\(N\\), the better CD will optimise into the maximum of the true likelihood. It can be shown that CD using one step of Gibbs sampling for on only one variable for fully visible Boltzman machines is exactly equivalent to optimising the pseudo likelihood. In general CD can be considered an approximation to pLL [130,131]. it was shown that CD10 works better than CD 1, but of course more computational complex PCD is a variation of CD in that the markov chain is NOT reinitialized at every iteration tieleman showed improved convergence properties For PCD, the Markov chains are not restarted from the \\(N\\) sequences in the MSA every time a new gradient is computed. Instead the Markov chains are evolved between successive gradient computations without resetting them. This ensures that, as we approach the maximum \\((\\v^*, \\w^*)\\), we acquire more and more samples from the distribution corresponding to parameters \\((\\v,\\w)\\) near the optimum. Hence our approximation to the gradient of the likelihood gets better the longer we sample, independent of the number of sequences \\(N\\) in the MSA. Dr Stefan Seemayer provided a Python implementation of CCMpred that was extended to optimize the full-likelihood of the MRF. CD is about the difference between the original data set and a perturbed data set perturbed data set : The contrasting data set needs to represent A data sample characteristic of the current PARAMETERS –&gt; Gibbs Sampling starting from data Note: as contrasting dataset towards true_parameters, the elements of the gradient converge to the gradient of the max log likelihood – At the limit of the Markov chain, the CD converges to the actual MLE References "],
["full-likelihood-optimization.html", "5.6 Optimizing the Full Likelihood", " 5.6 Optimizing the Full Likelihood Given the likelihood gradient estimates obtained with CD, the full negative log likelihood can now be minimized using a gradient descent optimization algorithm. Gradient descent algorithms are used to find the minimum of an objective function with respect to its parametrization by iteratively updating the parameters values in the opposite direction of the gradient of the objective function with respect to these parameters. SGD is a variant thereof that uses an stochastic estimate of the gradient whose average over many updates approaches the true gradient. The stochasticity is commonly obtained by evaluating a random subsample of the data at each iteration. For CD stochasticity additionally arises from the Gibbs sampling process in order to obtain a gradient estimate in the first place. As a consequence of stochasticity, the gradient estimates are noisy, resulting in parameter updates with high variance and strong fluctuations of the objective function. These fluctuations enable stochastic gradient descent to escape local minima but also complicate finding the exact minimum of the objective function. By slowly decreasing the step size, which is also called learning rate, of the parameter updates at every iteration, stochastic gradient descent most likely will converge to the global minimum for convex objective functions [132,133]. However, choosing an optimal step size for parameter updates as well as finding the optimal annealing schedule offers a challenge and needs manual tuning [134]. If the step size is chosen too small, progress will be unnecessarily slow, if it is chosen too large, the optimum will be overshot and parameter estimates might diverge. Another key challenge is that one specific step size often is not an optimal choice for all parameters. Because the magnitudes of gradients might vary considerably for different parameters, e.g. because of sparse data, different parameters require different optimal step sizes. Another disadvantage of SGD methods is that it is not easy to identify suitable convergence criteria. Unfortunately, it is neither possible to use second order optimization algorithms nor sophisticated first order algorithms like conjugate gradients to optimize the full likelihood. While the former class of algorithms requires (approximate) computation of the second partial derivatives, the latter requires evaluating the objective function in order to identify the optimal step size via linesearch, both being computationally too demanding. There exist many variants of stochastic gradient descent algorithms that deal with the aforementioned challenges e.g. speeding up convergence rates using momentum or defining adaptive learning rates for each parameter [132]. One of these SGD variants is Adaptive Moment Estimation (ADAM) [135], an algorithm that computes adaptive learning rates for each parameter including momentum (see methods section 8.4.1 for details). A major advantage is that ADAM does not require tuning many hyperparameters as the default values have been found to work quite well. 5.6.1 Optimizing with ADAM explain adam with its advantage of adaptive learning rates define optimal learning rate and decay dependent on neff –&gt; see methods violates sum_wijab condition, sum_wijab could be used as convergence criterium but practice shows that it is esp hard to reach for huge Neff, L? use change in parameter values as criteria instead benchmark plot with PLL correlation plot for some proteins with pLL 5.6.2 Optimizing with vanilla stochastic gradient descent need to manually tune learning rate and decay no adaptivity –&gt; harder to choose learning rate and decay than for adam however sum_wijab condition is not violated convergence criteria change of parameter values benchmark plot with pLL correlation plot with pLL 5.6.3 Altering Gibbs sampling Schemes until now: cd-1 but cd-k has been found to improve performance pcd nr of sequences that are sampled.. --> References "],
["a-bayesian-statistical-model-for-residue-residue-contact-prediction.html", "6 A Bayesian Statistical Model for Residue-Residue Contact Prediction", " 6 A Bayesian Statistical Model for Residue-Residue Contact Prediction All methods so far predict contacts by finding the one solution of parameters \\(\\via\\) and \\(\\wijab\\) that maximizes a regularized version of the log likelihood of the MSA and in a second step transforming the MAP estimates of the couplings \\(\\w^*\\) into heuristic contact scores (see Introduction 2.4.3.1). Apart from the heuristic transformation that omits meaningful information comprised in the coupling matrices \\(\\wij\\) as discussed in section 4, using the MAP estimate of the parameters instead of the true distribution has the decisive disadvantage of concealing the uncertainty of the estimates. The next sections present the derivation of a principled Bayesian statistical approach for contact prediction eradicating these deficiencies. The model provides estimates of the probability distributions of the distances \\(\\rij\\) between \\(\\Cb\\) atoms of all residues pairs \\(i\\) and \\(j\\), given the MSA \\(\\X\\). The parameters \\((\\v, \\w)\\) of the MRF model describing the probability distribution of the sequences in the MSA are treated as hidden parameters that can be integrated out using an approximation to the posterior distribution of couplings \\(\\w\\). This approach also allows to explictely model the distance-dependence of coupling coeffcients \\(\\wij\\) as a mixture of Gaussians with distance-dependent mixture weights and thus can even learn correlations between couplings. "],
["overview-posterior-distances.html", "6.1 Computing the Posterior Distribution of Distances \\(p(\\r | \\X)\\)", " 6.1 Computing the Posterior Distribution of Distances \\(p(\\r | \\X)\\) The joint probability of distances and MRF model parameters \\((\\v, \\w)\\) given the MSA \\(\\X\\) and a set of sequence derived features \\(\\phi\\) (described in detail in section 7), can be written as a hierarchical Bayesian model of the form: \\[\\begin{align} p(\\r, \\v, \\w | \\X, \\phi) &amp;\\propto p(\\X | \\v, \\w) p(\\v, \\w | \\r) \\, p(\\r | \\phi ) \\, . \\tag{6.1} \\end{align}\\] The ultimate goal is to compute the posterior probability of the distances, \\(p(\\r | \\X, \\phi)\\), that can be obtained by treating the parameters \\((\\v, \\w)\\) as hidden variables and marginalizing over these parameters, \\[\\begin{align} p(\\r | \\X , \\phi) &amp;\\propto p(\\X | \\r) p(\\r | \\phi)\\\\ p(\\X | \\r) &amp;= \\int \\int p(\\X | \\v,\\w) \\, p(\\v, \\w | \\r) \\,d\\v\\,d\\w \\; . \\tag{6.2} \\end{align}\\] The single potentials \\(\\v\\) will be fixed at their best estimate \\(\\v^*\\) (see section 5.4) by using a very tight prior \\(p(\\v) = \\Gauss(\\v|\\v^*,\\lambda_v^{-1} \\I) \\rightarrow \\delta(\\v-\\v*)\\) for \\(\\lambda_v \\rightarrow \\infty\\) that acts as a delta function. This allows the replacement of the intergral over \\(\\v\\) with the value of the integrand at its mode \\(\\v^*\\). Computing the integral over \\(\\w\\) can be achieved by factorizing the integrand into factors over \\((i,j)\\) and performing each integration over the coupling coefficients \\(\\wij\\) for \\((i,j)\\) separately. For that account, the prior over \\(\\w\\) will be modelled as a product over independent contributions over \\(\\wij\\) with \\(\\wij\\) depending only on the distance \\(\\rij\\), which is described in detail in the next section 6.2. The prior over MRF model parameters then yields, \\[\\begin{equation} p(\\v,\\w|\\r) = \\Gauss(\\v|\\v^*,\\lambda_v^{-1} \\I) \\, \\prod_{1\\le i&lt;j\\le L} p(\\wij|\\rij) \\; . \\tag{6.3} \\end{equation}\\] Furthermore, section 6.3 proposes an approximation to the regularised likelihood, \\(p(\\X | \\v,\\w) \\, p(\\v, \\w)\\), with a Gaussian distribution that facilitates the analytical solution of the integral in eq. (6.2) and is covered in section 6.4. Finally, the marginals \\(p(\\rij | \\X, \\phi) = \\int p(\\r | \\X, \\phi) d \\r_{\\backslash ij}\\), where \\(\\r_{\\backslash ij}\\) is the vector containing all coordinates of \\(\\r\\) except \\(\\rij\\) will be computed in 6.5. "],
["coupling-prior.html", "6.2 Modelling the prior over couplings with dependence on \\(\\rij\\)", " 6.2 Modelling the prior over couplings with dependence on \\(\\rij\\) The prior over couplings \\(p(\\wij|\\rij)\\) will be modelled as a mixture of \\(K\\!+\\!1\\) 400-dimensional Gaussians, with means \\(\\muk \\in \\mathbb{R}^{400}\\), precision matrices \\(\\Lk \\in \\mathbb{R}^{400\\times 400}\\), and distance-dependent, normalised weights \\(g_k(\\rij)\\), \\[\\begin{align} p(\\wij | \\rij) = \\sum_{k=0}^K g_k(\\rij) \\, \\Gauss(\\wij | \\muk, \\Lk^{-1}) \\,. \\tag{6.4} \\end{align}\\] The mixture weights \\(g_k(\\rij)\\) in eq. (6.4) are modelled as softmax: \\[\\begin{equation} g_k(\\rij) = \\frac{\\exp \\gamma_k(\\rij)}{\\sum_{k&#39;=0}^K \\exp \\gamma_{k&#39;}(\\rij)} \\tag{6.5} \\end{equation}\\] The functions \\(g_k(\\rij)\\) remain invariant when adding an offset to all \\(\\gamma_k(\\rij)\\). This degeneracy can be removed by setting \\(\\gamma_0(\\rij)=1\\). "],
["laplace-approx.html", "6.3 Gaussian approximation to the posterior of couplings", " 6.3 Gaussian approximation to the posterior of couplings From sampling experiments done by Markus Gruber we know that the regularized pseudo-log-likelihood for realistic examples of protein MSAs obeys the equipartition theorem. The equipartition theorem states that in a harmonic potential (where third and higher order derivatives around the energy minimum vanish) the mean potential energy per degree of freedom (i.e. per eigendirection of the Hessian of the potential) is equal to \\(k_B T/2\\), which is of course equal to the mean kinetic energy per degree of freedom. Hence we have a strong indication that in realistic examples the pseudo log likelihood is well approximated by a harmonic potential. We assume here that this will also be true for the regularized log likelihood. The posterior distribution of couplings \\(\\w\\) is given by \\[\\begin{equation} p(\\w | \\X , \\v^*) = p(\\X | \\v^*, \\w) \\Gauss (\\w | \\mathbf{0}, \\lambda_w^{-1} \\I) \\end{equation}\\] where the single potentials \\(\\v\\) are set to the target vector \\(\\v^*\\) as discussed in section 6.1. The posterior distribution can be approximated with a so called “Laplace Approximation”[79] as follows. By performing a second order Taylor expansion around the mode \\(\\w^*\\) of the log posterior it can be written as \\[\\begin{align} \\log p(\\w | \\X , \\v^*) \\overset{!}{\\approx} &amp; \\; \\log p(\\w^* | \\X , \\v^*) \\\\ &amp; + \\nabla_\\w \\log p(\\w | \\X , \\v^*)|_{\\w^*}(\\w-\\w^*) \\\\ &amp; - \\frac{1}{2} (\\w-\\w^*)^{\\mathrm{T}} \\H (\\w-\\w^*) \\; . \\end{align}\\] where \\(\\H\\) signifies the negative Hessian matrix with respect to the components of \\(\\w\\), \\[\\begin{equation} (\\H)_{klcd, ijab} = - \\left. \\frac{\\partial^2 \\log p(\\w | \\X , \\v^{*})}{\\partial \\w_{klcd} \\, \\partial \\wijab } \\right|_{(\\w^{*})} \\; . \\end{equation}\\] The mode \\(\\w^*\\) will be determined with the CD approach described in detail in section 5. Since the gradient vanishes at the mode maximum, \\(\\nabla_\\w \\log p(\\w | \\X , \\v^*)|_{\\w^*} = 0\\), the second order approximation can be written as \\[\\begin{equation} \\log p(\\w | \\X , \\v^*) {\\approx} \\log p(\\w^* | \\X , \\v^*) - \\frac{1}{2} (\\w-\\w^*)^{\\mathrm{T}} \\, \\H \\, (\\w-\\w^*) \\;. \\end{equation}\\] Hence, the posterior of couplings can be approximated with a Gaussian \\[\\begin{align} p(\\w | \\X , \\v^*) &amp;\\approx p(\\w^* | \\X , \\v^*) \\exp \\left( - \\frac{1}{2} (\\w-\\w^*)^{\\mathrm{T}} \\H (\\w -\\w^*) \\right) \\nonumber \\\\ &amp;= p(\\w^* | \\X , \\v^*) \\frac{(2 \\pi)^\\frac{D}{2}} { |\\H|^\\frac{D}{2}} \\times \\Gauss (\\w | \\w^*, \\H^{-1} ) \\\\ &amp;\\propto \\Gauss (\\w | \\w^*, \\H^{-1}) \\,, \\tag{6.6} \\end{align}\\] with proportionality constant that depends only on the data and with a precision matrix equal to the negative Hessian matrix. The surprisingly easy computation of the Hessian can be found in Methods section 8.5.1. 6.3.1 Iterative improvement of Laplace approximation The quality of the Gaussian approximation to the posterior distribution of couplings \\(p(\\w | \\X , \\v^*)\\) depends on two points, how well is the posterior distribution of couplings approximated by a Gaussian how closely does the mode of the posterior distribution of couplings lie near the mode of the integrand in equation (??). The second point can be addressed quite effectively in the following way. (see Murphy page 658 eq. 18.137 and eq 18.138) Supppose the optimal prior parameters \\((\\tilde{\\muk}, \\tilde{\\Lk})\\) have been trained as described in Methods section 8.5.3, using the standard isotropic regularisation prior \\(\\Gauss(\\w_{ij} | \\mathbf{0}, \\lambda_w^{-1} \\I)\\). An improved regularisation prior \\(\\Gauss( \\wij | \\mu(r_{ij}), \\mathbf{\\Sigma}(r_{ij}))\\) can then be selected using the knowledge of the true, optimised prior, by matching the mean and variance of the improved regularisation with those of the true prior from the first optimisation: \\[\\begin{align} \\mathbf{\\mu}(r_{ij}) &amp;= \\operatorname{E}_{p( \\wij | \\rij, \\tilde{\\mathbf{\\mu}}, \\tilde{\\Lambda})} \\left[ \\wij \\right] \\\\ &amp;= \\int \\wij \\, p( \\wij | \\rij, \\tilde{\\mathbf{\\mu}}, \\tilde{\\Lambda}) d \\w \\\\ &amp;= \\int \\wij \\sum_{k=0}^K g_k(\\rij) \\, \\Gauss(\\wij | \\tilde{\\muk}, \\tilde{\\Lambda}_k^{-1}) d \\w \\\\ &amp;= \\sum_{k=0}^K g_k(\\rij) \\int \\wij \\, \\Gauss(\\wij | \\tilde{\\muk}, \\tilde{\\Lambda}_k^{-1}) d \\w \\\\ \\mathbf{\\mu}(r_{ij}) &amp;= \\sum_{k=0}^K g_k(\\rij) \\, \\tilde{\\muk} \\end{align}\\] and similarly, \\[\\begin{align} \\mathbf{\\Sigma}(r_{ij}) &amp;= \\operatorname{var}_{ p(\\wij | \\rij, \\tilde{\\mathbf{\\mu}}, \\tilde{\\Lambda} )} \\left[ \\wij \\right] \\\\ &amp;= \\int (\\wij - \\mathbf{\\mu}(r_{ij})) (\\wij - \\mathbf{\\mu}(r_{ij}))^\\mathrm{T} \\, p( \\wij | \\rij, \\tilde{\\mathbf{\\mu}}, \\tilde{\\Lambda}) d \\w \\\\ &amp;= \\sum_{k=0}^K g_k(\\rij) \\int (\\wij - \\mathbf{\\mu}(r_{ij})) (\\wij - \\mathbf{\\mu}(r_{ij}))^\\mathrm{T} \\, \\Gauss(\\wij | \\tilde{\\muk}, \\tilde{\\Lk}^{-1}) d \\w \\\\ &amp;= \\sum_{k=0}^K g_k(\\rij) \\int (\\wij - \\mathbf{\\mu}(r_{ij}) + \\tilde{\\muk}) (\\wij - \\mathbf{\\mu}(r_{ij}) + \\tilde{\\muk})^\\mathrm{T} \\, \\Gauss(\\wij | \\mathbf{0} , \\tilde{\\Lk}^{-1}) d \\w \\\\ \\mathbf{\\Sigma}(r_{ij}) &amp;= \\sum_{k=0}^K g_k(\\rij) \\left( \\tilde{\\Lk}^{-1} + (\\mathbf{\\mu}(r_{ij}) - \\tilde{\\muk}) (\\mathbf{\\mu}(r_{ij}) - \\tilde{\\muk})^\\mathrm{T}\\right) \\,. \\end{align}\\] We can now run a second optimisation with better regularisation prior, in which the \\(\\tilde{\\mathbf{\\mu}}\\) and \\(\\tilde{\\Lambda}\\) are fixed and will not be optimised. Instead we optimise the marginal likelihood as a function of \\(\\muk\\) and \\(\\Lk\\). Since the new regularisation prior will be very close to the mode of the integrand in the marginal likelihood, our approximation for the second iteration has improved in comparison to the first iteration. In principle, a third iteration can be done in which our regularisation prior derived from the prior that was found by optimisation in the second iteration. However this is unlikely to further improve the predictions. References "],
["likelihood-fct-distances.html", "6.4 Computing the likelihood function of distances \\(p(\\X | \\r)\\)", " 6.4 Computing the likelihood function of distances \\(p(\\X | \\r)\\) In order to compute the likelihood function of the distances, one needs to solve the integral over \\((\\v, \\w)\\), \\[\\begin{equation} p(\\X | \\r) = \\int \\int p(\\X | \\v,\\w) \\, p(\\v, \\w | \\r) \\,d\\v\\,d\\w \\; . \\tag{6.7} \\end{equation}\\] Inserting the prior over parameters \\(p(\\v, \\w | \\r)\\) from eq. (6.3) into the previous equation and performing the integral over \\(\\v\\), as discussed earlier in section 6.1, yields \\[\\begin{eqnarray} p(\\X | \\r) &amp;=&amp; \\int \\left( \\int p(\\X | \\v,\\w) \\, \\Gauss(\\v|\\v^*,\\lambda_v^{-1} \\I) \\,d\\v \\right) \\, \\prod_{1\\le i&lt;j\\le L} p(\\wij|\\rij) \\, d\\w \\\\ p(\\X | \\r) &amp;=&amp; \\int p(\\X | \\v^*,\\w) \\, \\prod_{1\\le i&lt;j\\le L} p(\\wij|\\rij) \\, d\\w \\label{eq:in_over_w_1} \\end{eqnarray}\\] Next, the likelihood will be multiplied with the regularisation prior and the distance-dependent prior will be divided by the regularisation prior again: \\[\\begin{eqnarray} p(\\X | \\r) &amp;=&amp; \\int p(\\X | \\v^*,\\w) \\, \\Gauss(\\w|\\mathbf{0}, \\lambda_w^{-1} \\I) \\, \\prod_{1\\le i&lt;j\\le L} \\frac{p(\\wij|\\rij)}{\\Gauss(\\wij|\\mathbf{0}, \\lambda_w^{-1} \\I)} \\,d\\w \\, . \\end{eqnarray}\\] Now the crucial advantage of our likelihood regularisation is borne out: We can chose the strength of the regularisation prior, \\(\\lambda_w\\), such that the mode \\(\\w^*\\) of the regularised likelihood is near to the mode of the integrand in the last integral. The regularisation prior \\(\\Gauss(\\wij|\\mathbf{0}, \\lambda_w^{-1} \\I)\\) is then a simpler, approximate version of the real, distance-dependent prior \\(\\prod_{1\\le i&lt;j\\le L} p(\\wij|\\rij)\\). This allows us to approximate the regularised likelihood with a Gaussian distribution (eq. (6.6)), because this approximation will be fairly accurate in the region around its mode, which is near the region around the mode of the integrand and this again is in the region that contributes most to the integral: \\[\\begin{eqnarray} p(\\X | \\r) &amp;\\propto&amp; \\int \\Gauss (\\w | \\w^*, \\H^{-1} ) \\, \\prod_{1 \\le i&lt;j \\le L} \\frac{p(\\wij | \\rij)}{\\Gauss(\\wij|\\mathbf{0}, \\lambda_w^{-1} \\I)} d\\w \\,. \\tag{6.8} \\end{eqnarray}\\] The matrix \\(\\H\\) has dimensions \\((L^2 \\times 20^2) \\times (L^2 \\times 20^2)\\). Computing it is obviously infeasible, even if there was a way to compute \\(p(x_i \\eq a, x_j \\eq b| \\v^*,\\w^*)\\) efficiently. In Methods section ?? is shown that in practice, the off-diagonal block matrices with \\((i,j) \\ne (k,l)\\) are negligible in comparison to the diagonal block matrices. For the purpose of computing the integral in eq. (6.8), it is therefore a good approximation to simply set the off-diagonal block matrices (case 3 in (8.8)) to zero! The first term in the integrand of eq. (6.8) now factorizes over \\((i,j)\\), \\[\\begin{equation} \\Gauss (\\w | \\w^{*}, \\H^{-1}) \\approx \\prod_{1 \\le i &lt; j \\le L} \\Gauss (\\wij | \\wij^{*}, \\H_{ij}^{-1}) , \\end{equation}\\] with the diagonal block matrices are \\((\\H_{ij})_{ab,cd} := (\\H)_{ijab,ijcd}\\). Now the product over all residue indices can be moved in front of the integral and each integral can be performed over \\(\\wij\\) separately, \\[\\begin{eqnarray} p(\\X | \\r) &amp;\\propto&amp; \\int \\prod_{1 \\le i &lt; j \\le L} \\Gauss (\\wij | \\wij^{*}, \\H_{ij}^{-1}) \\prod_{1 \\le i&lt;j \\le L} \\frac{p(\\wij | \\rij)}{\\Gauss(\\wij|\\mathbf{0}, \\lambda_w^{-1} \\I)} d\\w \\\\ p(\\X | \\r) &amp;\\propto&amp; \\int \\prod_{1\\le i&lt;j\\le L} \\left( \\Gauss (\\wij | \\wij^*, \\H_{ij}^{-1}) \\, \\frac{p(\\wij | \\rij)}{\\Gauss(\\wij | \\mathbf{0}, \\lambda_w^{-1} \\I)} \\right) d\\w \\\\ p(\\X | \\r) &amp;\\propto&amp; \\prod_{1\\le i&lt;j\\le L} \\int \\Gauss (\\wij | \\wij^*, \\H_{ij}^{-1}) \\frac{p(\\wij | \\rij)}{\\Gauss (\\wij | \\mathbf{0}, \\lambda_w^{-1} \\I)} d \\wij \\tag{6.9} \\end{eqnarray}\\] Inserting the distance-dependent coupling prior defined in eq. (6.4) yields \\[\\begin{eqnarray} p(\\X | \\r) &amp;\\propto&amp; \\prod_{1\\le i&lt;j\\le L} \\int \\Gauss (\\wij | \\wij^*, \\H_{ij}^{-1}) \\frac{\\sum_{k=0}^K g_{k}(\\rij) \\Gauss(\\wij | \\muk, \\Lk^{-1})}{\\Gauss (\\wij | \\mathbf{0}, \\lambda_w^{-1} \\I)} d \\wij \\\\ p(\\X | \\r) &amp;\\propto&amp; \\prod_{1\\le i&lt;j\\le L} \\sum_{k=0}^K g_{k}(\\rij) \\int \\frac{\\Gauss (\\wij | \\wij^*, \\H_{ij}^{-1})}{\\Gauss (\\wij | \\mathbf{0}, \\lambda_w^{-1} \\I)} \\Gauss(\\wij | \\muk, \\Lk^{-1}) d\\wij \\; . \\tag{6.10} \\end{eqnarray}\\] The integral can be carried out using the following formula: \\[\\begin{equation} \\int d\\seq \\, \\frac{ \\Gauss( \\seq | \\mathbf{\\mu}_1, \\mathbf{\\Lambda}_1^{-1}) }{\\Gauss(\\seq|\\mathbf{0},\\mathbf{\\Lambda}3^{-1})} \\, \\Gauss(\\seq|\\mathbf{\\mu}_2,\\mathbf{\\Lambda}_2^{-1}) = \\\\ \\frac{\\Gauss(\\mathbf{0}| \\mathbf{\\mu}_1, \\mathbf{\\Lambda}_{1}^{-1}) \\Gauss(\\mathbf{0}| \\mathbf{\\mu}_2, \\mathbf{\\Lambda}_{2}^{-1})}{\\Gauss(\\mathbf{0}|\\mathbf{0}, \\mathbf{\\Lambda}_{3}^{-1}) \\Gauss(\\mathbf{0}| \\mathbf{\\mu}_{12}, \\mathbf{\\Lambda}_{123}^{-1})} \\end{equation}\\] with \\[\\begin{eqnarray} \\mathbf{\\Lambda}_{123} &amp;:=&amp; \\mathbf{\\Lambda}_1 - \\mathbf{\\Lambda}_3 + \\mathbf{\\Lambda}_2 \\\\ \\mathbf{\\mu}_{12} &amp;:=&amp; \\mathbf{\\Lambda}_{123}^{-1}(\\mathbf{\\Lambda}_1 \\mathbf{\\mu}_1 + \\mathbf{\\Lambda}_2 \\mathbf{\\mu}_2). \\end{eqnarray}\\] We define \\[\\begin{align} \\Lijk &amp;:= \\H_{ij} - \\lambda_w \\I + \\Lk \\\\ \\muijk &amp;:= \\Lijk^{-1}(\\H_{ij} \\wij^* + \\Lk \\muk) \\,. \\tag{6.11} \\end{align}\\] and obtain \\[\\begin{align} p(\\X | \\r) \\propto \\prod_{1 \\le i &lt; j \\le L} \\sum_{k=0}^K g_{k}(\\rij) \\frac{\\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss(\\mathbf{0} | \\muijk, \\Lijk^{-1})} \\,. \\tag{6.12} \\end{align}\\] \\(\\Gauss( \\mathbf{0} | \\mathbf{0}, \\lambda_w^{-1} \\I)\\) and \\(\\Gauss( \\mathbf{0} | \\wij^*, \\H_{ij}^{-1})\\) are constants that depend only on \\(\\X\\) and \\(\\lambda_w\\) and can be omitted. "],
["posterior-of-rij.html", "6.5 The posterior probability distribution for \\(\\rij\\)", " 6.5 The posterior probability distribution for \\(\\rij\\) The posterior distribution for \\(r_{ij}\\) can be computed by marginalizing over all other distances, which are summarized in the vector \\(\\r_{\\backslash ij}\\): \\[\\begin{eqnarray} p(\\rij | \\X, \\phi) &amp;=&amp; \\int d \\r_{\\backslash ij} \\, p(\\r |\\X, \\mathbf{\\phi})\\\\ &amp;\\propto &amp; \\int d \\r_{\\backslash ij} \\, p(\\X|\\r) \\, p(\\r | \\phi) \\\\ &amp;\\propto &amp; \\int d \\r_{\\backslash ij} \\prod_{i&#39;&lt;j&#39;} \\sum_{k=0}^K g_{k}(r_{i&#39;j&#39;}) \\, \\frac{\\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss(\\mathbf{0} | \\muijk, \\Lijk^{-1})} \\, \\prod_{i&#39;&lt;j&#39;} p(r_{i&#39;j&#39;} |\\phi_{i&#39;j&#39;}) \\,, \\end{eqnarray}\\] and, by pulling out of the integral over \\(\\r_{\\backslash ij}\\) the term depending only on \\(\\rij\\), \\[\\begin{eqnarray} p(\\rij | \\X, \\phi) &amp; \\propto &amp; p(\\rij |\\phi_{ij}) \\, \\sum_{k=0}^K g_{k}(\\rij) \\, \\frac{\\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss(\\mathbf{0} | \\muijk, \\Lijk^{-1})} \\\\ &amp; \\times &amp; \\prod_{i&#39;&lt;j&#39;, (i&#39;,j&#39;) \\ne (i,j)} \\int d r_{i&#39;j&#39;} \\, p(r_{i&#39;j&#39;} |\\phi_{i&#39;j&#39;}) \\, \\sum_{k=0}^K g_{k}(r_{i&#39;j&#39;}) \\, \\frac{\\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss(\\mathbf{0} | \\muijk, \\Lijk^{-1})} \\end{eqnarray}\\] Since the second factor involving the integrals over \\(r_{i&#39;j&#39;}\\) is a constant with respect to \\(\\rij\\), we find \\[\\begin{equation} p(\\rij | \\X, \\phi) \\propto p(\\rij |\\phi_{ij}) \\, \\sum_{k=0}^K g_{k}(\\rij) \\, \\frac{\\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss(\\mathbf{0} | \\muijk, \\Lijk^{-1})} \\, . \\tag{6.13} \\end{equation}\\] -->"],
["contact-prior.html", "7 Contact Prior", " 7 Contact Prior The wealth of successful meta-predictors presented in section 2.3 highlights the importance to exploit other sources of information apart from coevolution statistics. Much information about residue interactions is typically contained in features of 1D properties at positions \\(i\\) and \\(j\\) predicted from local sequence profiles, such as secondary structure, solvent accessibility or contact number, and in features of predicted 2D properties such as the contact prediction scores for \\((i,j)\\) from a profile-based method. For example, predictions of secondary structure elements and solvent accessibility are used by almost all modern machine learning predictors, such as MetaPsicov [69], NeBCon [72], EPSILON-CP [71], PconsC3 [67]. Other frequently used sequence derived features include pairwise contact potentials, sequence separation and conservation measures such as column entropy [69,72,136]. In the following sections I present a random forest classifier that uses sequence derived features to distinguish contacts from non-contacts. Methods section 8.7.1 lists all features used to train the classifier including the aforementioned standard features as well as some novel features. The probabilistic predictions of the random forest model can be introduced directly as prior information into the Bayesian statistical model presented in the last section ?? to improve the overall prediction accuracy in terms of posterior probabilities. Furthermore, contact scores from coevolution methods can be added as an additional feature to the random forest model in order to elucidate how much the combined information improves prediction accuracy over the single methods. References "],
["random-forest-classifiers.html", "7.1 Random Forest Classifiers", " 7.1 Random Forest Classifiers Random Forests are supervised machine learning methods that belong to the class of ensemble methods [137–139]. They are easy to implement, fast to train and can handle large numbers of features due to implicit feature selection [140]. Ensemble methods combine the predictions of several independent base estimators with the goal to improve generalizability over a single estimator. Random forests are ensembles of decision trees where randomness is introduced in two ways: every tree is build on a random sample that is of the same size but drawn with replacement from the training set (i.e., a bootstrap sample) every split of a node is performed on a random subset of features Predictions from all trees are averaged to obtain the final prediction. A single decision tree, especially when it is grown very deep is highly susceptible to noise in the training set and therefore prone to overfitting which results in poor generalization ability. As a consequence of randomness and averaging over many decision trees, the variance of a random forest predictor decreases and therefore the risk of overfitting. Random forests are capable of regression and classification tasks. For classification, predictions for new data are obtained by running a data sample down every tree in the forest and then either apply majority voting over single class votes or averaging the probabilistic class predictions. Probabilistic class predictions of single trees are computed as the fraction of training set samples of the same class in a leaf whereas the single class vote refers to the majority class in a leaf. Typically, Gini impurity, which is a computationally efficient approximation to the entropy, is used as a split criterion to estimate the quality of a split of a node. It measures the degree of purity in a data set regarding class labels as \\(GI = (1 - \\sum_{k=1}^K p_k^2)\\), where \\(p_k\\) is the proportion of class \\(k\\) in the data set. The feature \\(f\\) with the highest decrease in Gini impurity \\(\\Delta GI_f\\) over the two resulting child node subsets will be used to split the data set at the given node \\(N\\), \\[ \\Delta GI_f(N_{\\textrm{parent}}) = GI_f(N_{\\textrm{parent}}) - p_{\\textrm{left}} GI(N_{\\textrm{left}}) - p_{\\textrm{right}} GI(N_{\\textrm{left}}) \\] where \\(p_{\\textrm{left}}\\) and \\(p_{\\textrm{right}}\\) refers to the fraction of samples ending up in the left and right child node respectively [140]. Summing the decrease in Gini impurity for a feature \\(f\\) over all trees whenever \\(f\\) was used for a split yields the Gini importance measure, which can be used as an estimate of general feature relevance. Random forests therefore are popular methods for feature selection and it is common practice to remove the least important features from a data set to reduce the complexity of the model. However, feature importance measured with respect to Gini importance needs to be interpreted with care. The random forest model cannot distinguish between correlated features and it will choose any of the correlated features for a split, thereby reducing the importance of the other features and introducing bias. Furthermore, it has been found that feature selection based on Gini importance is biased towards selecting features with more categories as they will be chosen more often for splits and therefore tend to obtain higher scores [141]. References "],
["evaluating-random-forest-model-as-contact-predictor.html", "7.2 Evaluating Random Forest Model as Contact Predictor", " 7.2 Evaluating Random Forest Model as Contact Predictor I trained a random forest classifier on the feature set described in methods section 8.7.1 and optimized model hyperparameters as well as some data set specific settings (e.g window size and class ratios) with 5-fold cross-validation as described in methods section 8.7.2. Figure 7.1 shows the ranking of the ten most important features according to Gini importance. Both local statistical contact scores, OMES [142] and MI (mutual information between amino acid counts), constitute the most important features besides the mean pair potentials acording to Miyazawa &amp; Jernigan [143] and Li&amp;Fang[54]. Further important features include the relative solvent accessibility at both pair positions, the total percentage of gaps at both positions, the correlation between mean isoelectric point property at both positions, sequence separation and the beta-sheet propensity in a window of size five around position i. Figure 7.1: Top ten features ranked according to Gini importance. Features are described in detail in methods section 8.7.1. Many features have low Gini importance scores which means they are rarely considered for splitting a node and can likely be removed from the dataset. Removing irrelevant features from the dataset is a convenient procedure to reduce model complexity. As described in methods section 8.7.3, I performed feature selection by evaluating model performance on subsets of features of decreasing importance. Most models trained on subsets of the total feature space perform nearly identical compared to the model trained on all features, as can be seen in Figure ??. When using only the 26 most relevant features, performance of the model drops compared to using at least 75 features. The model using the 75 most important features has been selected for further analysis. Figure 7.2: Average precision of random forest models trained on subsets of sequence derived features. Subsets of features have been selected as described in methods section 8.7.3. Figure ?? shows the mean precision for the random forest model trained on the 75 most important feautures and for several contact scores. The pseudo-likelihood contact score (APC corrected Frobenius norm) is shown as a reference method. The random forest model has a mean precision of 0.33 for the top 5L contacts compared to a precision of 0.47 for pseudo-likelihood. Furthermore, the random forest model improves approximately ten percentage points in precision over the local statistical contact scores, OMES and mutual information (MI). Both methods comprise important features of the random forest model as can be seen in Figure 7.1. Figure 7.3: Mean precision for top ranked contacts on a test set of 761 proteins. omes_fodoraldrich+apc = OMES score with APC as described in section 8.7.1.3. mi_pc + APC = mutual information with APC as described in section 8.7.1.3. rf_contact_prior = random forest model using only sequence derived features. pLL-L2normapc-RF = random forest model using sequence derived features and pseudo-likelihood contact score (L2norm + APC). ccmpred-pll-centerv+apc = conventional pseudo-likelihood contact score (L2norm + APC) When analysing performance with respect to alignment size it can be found that the random forest model outperforms the pseudo-likelihood score for small alignments (see Figure 7.4). Both, local statistial models OMES and MI also perform weak on small alignments, leading to the conclusion that the remaining sequence derived features are highly relevant when the alignment contains only few sequences. This finding is expected, as it is well known that models trained on simple sequence features perform almost independent of alignment size [71]. Figure 7.4: Mean precision for top ranked contacts on a test set of 761 proteins splitted into four equally sized subsets according to Neff. Subsets are defined according to quantiles of Neff values. Upper left: Subset of proteins with Neff &lt; Q1. Upper right: Subset of proteins with Q1 &lt;= Neff &lt; Q2. Lower left: Subset of proteins with Q2 &lt;= Neff &lt; Q3. Lower right: Subset of proteins with Q3 &lt;= Neff &lt; Q4. omes_fodoraldrich+apc = OMES score with APC as described in section 8.7.1.3. mi_pc + APC = mutual information with APC as described in section 8.7.1.3. rf_contact_prior = random forest model using only sequence derived features. pLL-L2normapc-RF = random forest model using sequence derived features and pseudo-likelihood contact score (APC corrected Frobenius norm of the couplings \\(\\wij\\)). ccmpred-pll-centerv+apc = APC corrected Frobenius norm of the couplings \\(\\wij\\) computed with pseudo-likelihood. References "],
["additional-coevolution-features-improves-precision.html", "7.3 Additional Coevolution Features improves Precision", " 7.3 Additional Coevolution Features improves Precision Figure 7.4 showed that the random forest predictor improves over the pseudo-likelihood coevolution method when the alignment consists of only few sequences. In order to assess this improvement in a more direct manner, it is possible to build a combined random forest predictor that is not only trained on the sequence derived features but also on the pseudo-likelihood contact score as an additional feature (see methods section 8.7.4 for details). As expected, the pseudo-likelihood score comprises the most important feature in the model (Figure 8.16). Finally, the comparison of the extended random forest model (“pLL-L2normapc-RF”) to the pseudo-likelihood score in Figure ?? reveals that the additional coevolutionary feature indeed improves performance over the score without prior information. Especially for small alignments, the improvment is substantial as can be seen in Figure ??. In contrast, the improvement on large alignments is small, as the gain from simple sequence features compared to the much more powerful coevolution signals is neglectable. -->"],
["methods.html", "8 Methods", " 8 Methods all you need to know "],
["dataset.html", "8.1 Dataset", " 8.1 Dataset A protein dataset has been constructed from the CATH (v4.1) [144] database for classification of protein domains. All CATH domains from classes 1(mainly \\(\\alpha\\)), 2(mainly \\(\\beta\\)), 3(\\(\\alpha+\\beta\\)) have been selected and filtered for internal redundancy at the sequence level using the pdbfilter script from the HH-suite[114] with an E-value cutoff=0.1. The dataset has been split into ten subsets aiming at the best possible balance between CATH classes 1,2,3 in the subsets. All domains from a given CATH topology (=fold) go into the same subsets, so that any two subsets are non-redundant at the fold level. Some overrepresented folds (e.g. Rossman Fold) have been subsampled ensuring that in every subset each class contains at max 50% domains of the same fold. Consequently, a fold is not allowed to dominate a subset or even a class in a subset. In total there are 6741 domains in the dataset. Multiple sequence alignments were built from the CATH domain sequences (COMBS) using HHblits [114] with parameters to maximize the detection of homologous sequences: hhblits -maxfilt 100000 -realign_max 100000 -B 100000 -Z 100000 -n 5 -e 0.1 -all hhfilter -id 90 -neff 15 -qsc -30 The COMBS sequences are derived from the SEQRES records of the PDB file and sometimes contain extra residues that are not resolved in the structure. Therefore, residues in PDB files have been renumbered to match the COMBS sequences. The process of renumbering residues in PDB files yielded ambigious solutions for 293 proteins, that were removed from the dataset. Another filtering step was applied to remove 80 proteins that do not hold the following properties: more than 10 sequences in the multiple sequence alignment (\\(N&gt;10\\)) protein length between 30 and 600 residues (\\(30 \\leq L \\leq 600\\)) less than 80% gaps in the multiple sequence alignment (percent gaps &lt; 0.8) at least one residue-pair in contact at \\(C_\\beta &lt; 8\\AA\\) and minimum sequence separation of 6 positions The final dataset is comprised of 6368 proteins with almost evenly distributed CATH classes over the ten subsets (Figure 8.1). Figure 8.1: Distribution of CATH classes (1=mainly \\(\\alpha\\), 2=mainly \\(\\beta\\), 3=\\(\\alpha-\\beta\\)) in the dataset and the ten subsets. References "],
["optimizing-pseudo-likelihood.html", "8.2 Optimizing Pseudo-Likelihood", " 8.2 Optimizing Pseudo-Likelihood Dr Stefan Seemayer has reimplementated the open-source software CCMpred [86] in Python. Based on a fork of his private github repository I continued development and extended the software, which is now called CCMpredPy. It will soon be available at https://github.com/soedinglab/CCMpredPy. All computations in this thesis are performed with CCMpredPy unless stated otherwise. 8.2.1 Pseudo-Likelihood Objective Function and its Gradients CCMpred optimizes the regularized negative pseudo-log-likelihood using conjugate gradients optimizer. The negative pseudo-log-likelihood, abbreviated \\(\\mathcal{npll}\\), is defined as: \\[\\begin{equation} \\mathcal{npll}(\\mathbf{X} | \\v,\\w) = - \\sum_{n=1}^N \\sum_{i=1}^L \\left( v_i(x_i^{(n)}) + \\sum_{\\substack{j=1 \\\\ j \\neq i}}^L w_{ij}(x_i^{(n)}, x_j^{(n)}) - \\log Z_i^{(n)} \\right) \\end{equation}\\] The normalization term \\(Z_i\\) sums over all assignments to one position \\(i\\) in sequence: \\[\\begin{equation} Z_i^{(n)} = \\sum_{a=1}^{20} \\exp \\left( v_i(a) + \\sum_{\\substack{j=1 \\\\ j \\neq i}}^L w_{ij}(a, x_j^{(n)}) \\right) \\end{equation}\\] 8.2.2 Differences between CCMpred and CCMpredpy CCMpredPy differs from CCMpred [86] which is available at https://github.com/soedinglab/CCMpred in several details: Initialization of potentials \\(\\v\\) and \\(\\w\\) CCMpred initializes single potentials \\(\\v_i(a) = \\log f_i(a) - \\log f_i(a= &quot;-&quot;)\\) with \\(f_i(a)\\) being the frequency of amino acid a at position i and \\(a=&quot;-&quot;\\) representing a gap. A single pseudo-count has been added before computing the frequencies. Pair potentials \\(\\w\\) are intialized at 0. CCMpredPy initializes single potentials \\(\\v\\) with the ML estimate of single potentials (see section ??) using amino acid frequencies computed as described in section 8.2.4. Pair potentials \\(\\w\\) are initialized at 0. Regularization CCMpred uses a Gaussian regularization prior centered at zero for both single and pair potentials. The regularization coefficient for single potentials \\(\\lambda_v = 0.01\\) and for pair potentials \\(\\lambda_w = 0.2 * (L-1)\\) with \\(L\\) being protein length. CCMpredPy uses a Gaussian regularization prior centered at zero for the pair potentials. For the single potentials the Gaussian regularization prior is centered at the ML estimate of single potentials (see section ??) using amino acid frequencies computed as described in section 8.2.4. The regularization coefficient for single potentials \\(\\lambda_v = 10\\) and for pair potentials \\(\\lambda_w = 0.2 * (L-1)\\) with \\(L\\) being protein length. Default settings for CCMpredPy have been chosen to best reproduce CCMpred results. A benchmark over a subset of approximately 3000 proteins confirms that performance measured as PPV for both methods is almost identical (see Figure 8.2). Figure 8.2: Benchmark for CCMpred and CCMpredPy on a dataset of 3124 proteins. ccmpred-vanilla+apc: CCMpred [86] with APC. ccmpred-pll-centerv+apc: CCMpredPy with APC. Specific flags that have been used to run both methods are described in detail in the text (see section 8.2.2). The benchmark in Figure 8.2 as well as all contacts predicted with CCMpred and CCMPredPy (using pseudo-likelihood) in my thesis have been computed using the following flags: Flags used with CCMpredPy (using pseudo-likelihood objective function): --maxit 250 # Compute a maximum of MAXIT operations --center-v # Use a Gaussian prior for single potentials centered at ML estimate v* --reg-l2-lambda-single 10 # regularization coefficient for single potentials --reg-l2-lambda-pair-factor 0.2 # regularization coefficient for pairwise potentials computed as reg-l2-lambda-pair-factor * (L-1) --pc-uniform # use uniform pseudocounts (1/21 for 20 amino acids + 1 gap state) --pc-count 1 # defining pseudo count admixture coefficient rho = pc-count/( pc-count+ Neff) --epsilon 1e-5 # convergence criterion for minimum decrease in the last K iterations --ofn-pll # using pseudo-likelihood as objective function --alg-cg # using conjugate gradient to optimize objective function Flags used with CCMpred: -n 250 # NUMITER: Compute a maximum of NUMITER operations -l 0.2 # LFACTOR: Set pairwise regularization coefficients to LFACTOR * (L-1) -w 0.8 # IDTHRES: Set sequence reweighting identity threshold to IDTHRES -e 1e-5 # EPSILON: Set convergence criterion for minimum decrease in the last K iterations to EPSILON 8.2.3 Sequence Reweighting As discussed in section 2.7, sequences in a MSA do not represent independent draws from a probabilistic model. To reduce the effects of overrepresented sequences, typically a simple weighting strategy is applied that assigns a weight to each sequence that is the inverse of the number of similar sequences according to an identity threshold [85]. It has been found that reweighting improves contact prediction performance [50,80,112] significantly but results are robust against the choice of the identity threshold in a range between 0.7 and 0.9 [80]. We chose an identity threshold of 0.8. Every sequence \\(x_n\\) of length \\(L\\) in an alignment with \\(N\\) sequences has an associated weight \\(w_n = 1/m_n\\), where \\(m_n\\) represents the number of similar sequences: \\[\\begin{equation} w_n = \\frac{1}{m_n}, m_n = \\sum_{m=1}^N I \\left( ID(x_n, x_m) \\geq 0.8 \\right) \\\\ ID(x_n, x_m)=\\frac{1}{L} \\sum_{i=1}^L I(x_n^i = x_m^i) \\tag{8.1} \\end{equation}\\] The number of effective sequences \\(\\mathbf{\\neff}\\) of an alignment is then the number of sequence clusters computed as: \\[\\begin{equation} \\neff = \\sum_{n=1}^N w_n \\tag{8.2} \\end{equation}\\] TODO: Plot Performance for Seq weighting 8.2.4 Computing Amino Acid Frequencies Single and pairwise amino acid frequencies are computed from the alignment by weighting amino acid counts (see section 8.2.3) and adding pseudocounts for numerical stability. Let \\(a,b \\in \\{1,\\ldots,20\\}\\) be amino acids, \\(q(x_i=a), q(x_i=a, x_j=b)\\) and \\(q_0(x_i=a), q_0(x_i=a,x_j=b)\\) be the empirical single and pair frequencies with and without pseudocounts, respectively. We define \\[\\begin{align} q(x_i \\eq a) :=&amp; (1-\\tau) \\; q_0(x_i \\eq a) + \\tau \\tilde{q}(x_i\\eq a) \\\\ q(x_i \\eq a, x_j \\eq b) :=&amp; (1-\\tau)^2 \\; [ q_0(x_i \\eq a, x_j \\eq b) - q_0(x_i \\eq a) q_0(x_j \\eq b) ] + \\\\ &amp; q(x_i \\eq a) \\; q(x_j \\eq b) \\tag{8.3} \\end{align}\\] with \\(\\tilde{q}(x_i \\eq a) := f(a)\\) being background amino acid frequencies and \\(\\tau \\in [0,1]\\) is a pseudocount admixture coefficient, which is a function of the diversity of the multiple sequence alignment: \\[\\begin{equation} \\tau = \\frac{N_\\mathrm{pc}}{(N_\\mathrm{eff} + N_\\mathrm{pc})} \\tag{8.4} \\end{equation}\\] where \\(N_{pc} &gt; 0\\). The formula for \\(q(x_i \\eq a, x_j \\eq b)\\) in the second line in eq (8.3) was chosen such that for \\(\\tau \\eq0\\) we obtain \\(q(x_i \\eq a, x_j \\eq b) = q_0(x_i \\eq a, x_j \\eq b)\\), and furthermore \\(q(x_i \\eq a, x_j \\eq b) = q(x_i \\eq a) q(x_j \\eq b)\\) exactly if \\(q_0(x_i \\eq a, x_j \\eq b) = q_0(x_i \\eq a) q_0(x_j \\eq b)\\). References "],
["analysis-of-coupling-matrices.html", "8.3 Analysis of Coupling Matrices", " 8.3 Analysis of Coupling Matrices 8.3.1 Correlation of Couplings with Contact Class Approximately 100000 residue pairs have been filtered for contacts and non-contacts respectively according to the following criteria: consider only residue pairs separated by at least 10 positions in sequence minimal diversity (\\(=\\frac{\\sqrt{N}}{L}\\)) of alignment = 0.3 minimal number of non-gapped sequences = 1000 \\(\\Cb\\) distance threshold for contact: \\(&lt;8\\AA\\) \\(\\Cb\\) distance threshold for noncontact: \\(&gt;25\\AA\\) 8.3.2 Coupling Distribution Plots For one-dimensional coupling distribution plots the residue pairs and respective pseudo-log-likelihood coupling values \\(\\wijab\\) have been selected as follows: consider only residue pairs separated by at least 10 positions in sequence discard residues that have more than 30% gaps in the alignment discard residue pairs that have insufficient evidence in the alignment: \\(N_{ij} \\cdot q_i(a) \\cdot q_j(b) &lt; 100\\) with: \\(N_{ij}\\) is the number of sequences with neither a gap at position i nor at position j \\(q_i(a)\\) and \\(q_j(b)\\) are the frequencies of amino acids a and b at positions i and j (computed as described in section 8.2.4) These criteria ensure that uninformative couplings are neglected, e.g. sequence neighbors albeit being contacts according to the \\(\\Cb\\) contact definition cannot be assumed to express biological meaningful coupling patterns, or couplings for amino acid pairings that do not have statistical relevant counts in the alignment. The same criteria have been applied for selecting couplings for the two-dimensional distribution plots with the difference that evidence for a single coupling term has to be \\(N_{ij} \\cdot q_i(a) \\cdot q_j(b) &lt; 80.\\) "],
["methods-optimizing-full-likelihood.html", "8.4 Optimizing the Full-Likelihood", " 8.4 Optimizing the Full-Likelihood The following sections will describe the hyperparameter tuning for the stochastic gradient descent optimization as well as tuning different aspects of the Gibbs sampling scheme used to approximate the gradient with CD. In theory the algorithm has converged and the optimum of the objective function has been reached when the gradient becomes zero. In practice the gradients will never be exactly zero, especially due to the stochasticity of the gradient estimates when using stochastic gradient descent. For this reason, usually some kind of convergence criterion is designed and convergence is assumed whenever the criterion is met. A common criterion is the when the relative change of objective function value between iterations is close to zero. However, because the evaluation of the full likelihood function is too expensive, the function value cannot be used to define a criterion. Another possibility is to stop learning when the norm of the gradient is close to zero [145]. For CD however, the gradients will be far off zero depending on how many sequences are used for sampling. Only when sampling large number of sequences, the gradients will eventually be close to zero (see section @ref(sampling more seq)). This is however achieved at the expense of runtine which increases linearly in the number of sequences for sampling. An alternative is to check the relative change over the norm of gradients between iterations and stop the algorithm whenever it falls below a small threshold \\(\\epsilon\\), \\[\\begin{equation} \\frac{|\\nabla_\\theta f(\\theta_{t-1}) - \\nabla_\\theta f(\\theta_{t})|}{|\\nabla_\\theta f(\\theta_{t-1})|} &lt; \\; \\epsilon \\; . \\tag{8.5} \\end{equation}\\] However, as gradient estimates are very noisy for stochastic gradient descent, gradient fluctuations complicate the proper assessment of this criterion. It is also possible to monitor the relative change over the norm of parameter estimates between iterations, \\[\\begin{equation} \\frac{|\\theta_{t-1} - \\theta_t|}{|\\theta_t|} &lt; \\; \\epsilon \\; . \\tag{8.6} \\end{equation}\\] This criterion is more stable as parameter updates are dampened by the step size are not quite as noisy compared to subsequent gradient estimates. Another idea is to monitor the direction of gradients. Once getting close to the optimum, gradients will start to fluctuate as the optimizer will oscillate around the true optimum. From my experience, it is hard to find a general threshold that applies to all proteins, as every protein reflects a problem of varying complexity (number of parameters scales with \\(L^2\\), L being protein length). Furthermore this analysis is complicated when using momentum, as the computed gradient is not actually used to change the parameters but it is combined with previous gradient estimates. A necessary but not sufficient criterion for convergence is given by \\(\\sum_{a,b=1}^{20} \\wijab = 0\\) as derived in section 5.4. When using plain stochastic gradient descent without momentum and without adaptive learning rates, this criterion is never violated when parameters are initialized uniformly. This is due to the fact that the 400 gradients \\(\\wijab\\) for \\(a,b \\in {1, ..., 20\\}\\) are not independent. The sum over the 400 pairwise amino acid counts for two positions i and j is identical for the observed and sampled alignment, \\[\\begin{equation} \\sum_{a,b=1}^{20} N_{ij} q(x_i \\eq a, q_j \\eq b) = N_{ij} \\; , \\end{equation}\\] and therefore the gradients, computed as the difference of pairwise counts between observed and sampled alignment, are symmetrical. Considering residue pair (i,j) and assuming amino acid pair (a,b) has higher counts in the sampled alignment compared to the true alignment, then this differnce in counts must be compensated by other amino acid pairs (c,d) having less counts in the sampled alignment compared to the true alignment. This symmetry is translated into parameter updates when the same learning rate is used to update all parameters. However, when using adaptive learning rates, this symmetry is broken and the condition \\(\\sum_{a,b=1}^{20} \\wijab = 0\\) can be violated during the optimization processs. It is therefore interesting to monitor \\(\\sum_{1 \\le 1 &lt; j \\le L} \\sum_{a,b=1}^{20} \\wijab\\). Finally, the simplest strategy is to specify a maximum number of iterations for the optimization procedure. This also ensures that the algorithm will stop eventually if none of the other convergence criteria is met. In the following, I will set the maximum number of iterations to 5000 and I will stop the optimization when the relative change over the norm of parameter estimates falls below the threshold of \\(\\epsilon = 1e-8\\). Furthermore, I will follow the pragmatic standard strategy and run the optimization algorithm with many hyperparameter settings and pick the model that gives the best performance on a validation set [146]. The performance will be evaluated as the mean precision of the top ranked contact predictions over a benchmark set of 300 proteins, that is a subset of the data set described in methods section 8.1. Pseudo-likelihood couplings are computed with the tool CCMpredPy that is introduced in methods section 8.2.2. Contact scores for couplings obtained with pseudo-likelihood and CD are computed as the APC corrected Frobenius norm as explained in section 2.4.4. Coupling parameters are initialized at 0. 8.4.1 Full Likelihood Optimization with ADAM ADAM [135] stores an exponentially decaying average of past gradients and squared gradients, \\[\\begin{align} m_t &amp;= \\beta_1 m_{t−1} + (1 − \\beta_1) g \\\\ v_t &amp;= \\beta_2 v_{t−1} + (1 − \\beta_2) g^2 \\; , \\end{align}\\] with \\(g = \\nabla_w \\LLreg(\\v,\\w)\\) and the rate of decay being determined by hyperparameters \\(\\beta_1\\) and \\(\\beta_2\\). Both terms \\(m_t\\) and \\(v_t\\) represent estimates of the first and second moments of the gradient, respectively. Because \\(m_t\\) and \\(v_t\\) are initialized as vectors of zeros, the following bias correction terms are supposed to counteract the initialization bias towards zero, \\[\\begin{align} \\hat{m_t} &amp;= \\frac{m_t}{1-\\beta_1^t} \\\\ \\hat{v_t} &amp;= \\frac{v_t}{1-\\beta_2^t} \\; . \\end{align}\\] Parameters are then updated using step size \\(\\alpha\\), a small noise term \\(\\epsilon\\) and the corrected moment estimates \\(\\hat{m_t}\\), \\(\\hat{v_t}\\), according to \\[\\begin{equation} x_{t+1} = x_t - \\alpha \\cdot \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon} \\end{equation}\\] Kingma et al. proposed the default values \\(\\beta_1=0.9\\), \\(\\beta_2=0.999\\) and \\(\\epsilon=1e−8\\) and a constant learning rate \\(\\alpha_0=1e-3\\), because ADAM performs a kind of step size annealing by nature. However, popular implementations of ADAM in the Keras [147] and Lasagne [148] packages allow the use of a linear annealing schedule for the learning rate \\(\\alpha\\) of the form \\(\\alpha = \\frac{\\alpha_0}{1 + \\gamma t}\\), for an initial learning rate \\(\\alpha_0\\), decay rate \\(\\gamma\\) and timestep \\(t\\). I first tested three different constant learning rates \\(\\alpha \\in \\{1e-4, 1e-3, 5e-3\\}\\) and default parameters \\(\\beta_1=0.9\\), \\(\\beta_2=0.999\\) and \\(\\epsilon=1e−8\\). PLOT As can be seen in the performance: not goot. Looking at individual proteins: 8.4.2 Full Likelihood Optimization with Stochastic Gradient Descent The coupling parameters \\(\\w\\) will be updated in each iteration \\(t\\) by taking a step of size \\(\\alpha\\) along the direction of the negative gradient of the regularized full log likelihood \\(- \\nabla_w \\LLreg(\\v,\\w)\\) that has been approximated with CD, \\[\\begin{equation} \\w_{t+1} = \\w_t - \\alpha \\cdot \\nabla_w \\LLreg(\\v,\\w) \\; . \\end{equation}\\] In order to get a first intuition of the optimization problem, I tested initial learning rates \\(\\alpha_0 \\in \\{1\\mathrm{e}{-4}, 5\\mathrm{e}{-4}, 1\\mathrm{e}{-3}, 5\\mathrm{e}{-3}\\}\\) with a standard learning rate annealing schedule, \\(\\alpha = \\frac{\\alpha_0}{1 + \\gamma \\cdot t}\\) with decay rate \\(\\gamma=0.01\\) and timestep \\(t\\) [133]. Figure 8.3 shows the mean precision for top ranked contacts computed from pseudo-likelihood couplings and the CD couplings optimized with stochastic gradient descent using the four different learning rates. Overall, mean precision for CD contacts is smaller than for pseudo-likelihood contacts. Especially the smallest (\\(1\\mathrm{e}{-4}\\)) and biggest learning (\\(5\\mathrm{e}{-3}\\)) rate perform bad. Figure 8.3: Mean precision for top ranked contact predictions over 286 proteins. Contact scores are computed as the APC corrected Frobenius norm of the couplings \\(\\wij\\). pseudo-likelihood: Contact scores computed from pseudo-likelihood. The other methods derive contact scores from couplings computed from CD using stochastic gradient descent with different initial learning rates \\(\\alpha_0\\) as specified in the legend. Looking at individual proteins it turns out that the optimal learning rate depends on alignment size. Figure 8.4 shows convergence plots for two different proteins, where convergence is measured as the L2-norm of coupling parameters \\(||\\w||_2\\). For an exemplary protein with a small alignment (left plot), a small learning rate \\(1\\mathrm{e}{-4}\\) lets the optimization run very slowly and not reach convergence within 5000 iterations. A large learning rate of \\(5\\mathrm{e}{-3}\\) overshoots the optimum at the beginning of the optimization but as the learning rate decays over time the parameter estimates converge. In contrast, for a protein with a big alignment (right plot) the effects of the learning rate choice are much more pronounced. With a small initial learning rate of \\(1\\mathrm{e}{-4}\\) the optimization runs slowly but almost converges within 5000 iterations. Using a big initial learning rate of \\(5\\mathrm{e}{-3}\\), the parameter estimates diverged too far from the optimum and never reach the optimum. With learning rates \\(5\\mathrm{e}{-4}\\) and \\(1\\mathrm{e}{-3}\\), the optimum is well overshot at the beginning of the optimization but the parameter estimates eventually converge as the learning rates decreases over time. These observations are explained by the fact that the magnitude of the gradient scales with the number of sequences in the alignment. Because the gradient is computed from amino acid counts (see section 5.5), alignments with many sequences will generally produce larger gradients compared to alignments with few sequences, especially at the beginning of the optimization procedure when the difference in amino acid counts between sampled and observed sequences is largest. Following the observations I defined the initial learning rate \\(\\alpha_0\\) as a function of Neff. Aiming to obtain values for \\(\\alpha_0\\) around 5e-3 for small Neff and values for \\(\\alpha_0\\) around 1e-4 for large Neff, the initial learning rate is defined as \\[\\begin{equation} \\alpha_0 = \\frac{5\\mathrm{e}{-2}}{\\sqrt{N_{\\text{eff}}}} \\; . \\tag{8.7} \\end{equation}\\] For small Neff \\(\\approx 50\\) this yields \\(\\alpha_0 \\approx 7\\mathrm{e}{-3}\\) and for big Neff \\(\\approx 20000\\) this yields \\(\\alpha_0 \\approx 3.5\\mathrm{e}{-4}\\). Using this learning rate defined as a function of Neff, precision improves over the previous fixed learning rates (brown line in Figure 8.3). All following analyses are conducted using the Neff dependent learning rate. Figure 8.4: L2-norm of the coupling parameters \\(||\\w||_2\\) during stochastic gradient descent optimization with different learning rates. Linear learning rate annealing schedule has been used with decay rate \\(\\gamma=0.01\\) and initial learning rates \\(\\alpha_0\\) as stated in the legend. Left Convergence plot for protein 1mkc_A_00 having protein length L=43 and 142 sequences in the alignment (Neff=96). Right Convergence plot for protein 1c75_A_00 having protein length L=71 and 28078 sequences in the alignment (Neff=16808). Figure is cut at the yaxis at \\(||\\w||_2=1500\\), but learning rate of \\(5\\mathrm{e}{-3}\\) reaches \\(||\\w||_2 \\approx 13000\\). In a next step, I tried to find an optimal learning rate annealing schedule and an optimal decay rate. I evaluated the following learning rate schedules and decay rates using an initial learning rate defined as a funciton of Neff as in eq. (8.7). default linear learning rate schedule \\(\\alpha = \\frac{\\alpha_0}{1 + \\gamma \\cdot t}\\) with \\(\\gamma \\in \\{1\\mathrm{e}{-2}, 1\\mathrm{e}{-1}, 1\\}\\) square root learning rate schedule \\(\\alpha = \\frac{\\alpha_0}{\\sqrt{1 + \\gamma \\cdot t}}\\) with \\(\\gamma \\in \\{1\\mathrm{e}{-2}, 1\\mathrm{e}{-1}, 1\\}\\) sigmoidal learning rate schedule \\(\\alpha_{t+1} = \\frac{\\alpha_{t}}{1 + \\gamma \\cdot t}\\) with \\(\\gamma \\in \\{1\\mathrm{e}{-5}, 1\\mathrm{e}{-4}, 1\\mathrm{e}{-3}\\}\\) The decay schedules with different decay rates are visualized in Figure 8.5 Figure 8.5: Value of learning rate against the number of iterations for different learning rate schedules. Red legend group represents the default learning rate schedule \\(\\alpha = \\alpha_0 / (1 + \\gamma \\cdot t)\\). Blue legend represents the sigmoidal learning rate schedule \\(\\alpha_{t+1} = \\alpha_{t} / (1 + \\gamma \\cdot t)\\) with \\(\\gamma\\). Green legend represents the square root learning rate schedule \\(\\alpha = \\alpha_0 / \\sqrt{1 + \\gamma \\cdot t}\\). The iteration number is given by \\(t\\). Initial learning rate \\(\\alpha_0\\) is set to 1e-4 and \\(\\gamma\\) is the decay rate and its value is given in brackets in the legend. Only the sigmoidal learning rate schedule achieves precision comparable to the pseudo-likelihood score while improving convergence speed measured by the number of iterations. Appendix D.2 shows benchmarks as well as the distribution over the number of iterations until convergence for all learning rate schedules that have been evaluated. Whereas none of the proteins except one did converge within 5000 iterations using the default learning rate schedule with decay rate \\(\\gamma = 1\\mathrm{e}{-1}\\) (blue box plot in Figure 8.6), all proteins converged within 2000 or 1000 iterations using the sigmoidal learning rate schedule with decay rates \\(\\gamma = 1\\mathrm{e}{-5}\\) and \\(\\gamma = 1\\mathrm{e}{-4}\\), respectively (orange and green box plot respectively in Figure 8.6). Because the gradient of the full likelihood scales with number of sequences in the MSA, I defined the initial learning rate \\(\\alpha_0\\) and the decay rate \\(\\gamma\\) for the sigmoidal learning rate schedule as functions of Neff. Aiming to obtain values for the initial learning rate \\(\\alpha_0\\) and the decay rate \\(\\gamma\\) around 1e-4, as these values yield good performance and fast convergence, I defined \\(\\alpha_0 = \\gamma = \\frac{3\\mathrm{e}{-4}}{\\log N_{\\text{eff}}}\\). Assuming small Neff \\(\\approx 50\\) this yields \\(\\alpha_0 = \\gamma \\approx 1.7\\mathrm{e}{-4}\\) and for big Neff \\(\\approx 20000\\) this yields \\(\\alpha_0 = \\gamma \\approx 7\\mathrm{e}{-5}\\). The distribution of the number of iterations until convergence is displayed as red box plot in Figure 8.6, and is in the range of the sigmoidal learning rate schedule with decay rates \\(\\gamma = 1\\mathrm{e}{-5}\\) and \\(\\gamma = 1\\mathrm{e}{-4}\\), as expected. All following analyses are conducted using the sigmoidal learning rate schedule with initial learning rate and decay rate defined as functions of Neff as \\(\\alpha_0 = \\gamma = \\frac{3\\mathrm{e}{-4}}{\\log N_{\\text{eff}}}\\). Figure 8.6: Distribution of the number of iterations until convergence for gradient descent optimizations of the full likelihood for different learning rate schedules. Initial learning rate \\(\\alpha_0\\) is fixed to 1e-4 and maximum number of iterations is set to 5000. Blue box plot displays default learning rate schedule with decay rate \\(\\gamma = 1\\mathrm{e}{-1}\\). Orange and green boxplot represent sigmoidal learning rate schedule with decay rates \\(\\gamma = 1\\mathrm{e}{-5}\\) and \\(\\gamma = 1\\mathrm{e}{-4}\\), respectively. Red boxplot displays sigmoidal learning rate schedule with \\(\\alpha_0\\) and decay rate \\(\\gamma\\) defined as functions of Neff. Interestingly, and in contradiction to the benchmark results, using higher initial learning rates usually leads the parameters to diverge further from the pseudo-likelihood initializations than using smaller and faster decaying learning rates. Figure ?? illustrates this effect for protein 1mkc. Determining the learning and decay rate with respect to Neff, as just described, yields \\(\\alpha_0 = \\gamma \\approx 6.6e-5\\). Optimization converges after \\(\\approx 600\\) iterations with an L2-norm over couplings \\(||\\w||_2 = 15.28\\). When using a larger learning rate \\(\\alpha_0 = 1e-3\\) the optimization has converged after \\(\\approx 560\\) iterations with an L2-norm over couplings \\(||\\w||_2 = 12.66\\). 8.4.3 Optimizing Regularization Coefficients for Contrastive Divergence Gaussian priors are put on the single potentials \\(\\v\\) and the couplings \\(\\w\\) when optimizing the full likelihood with CD just as it is done for pseudo-likelihood optimization (compare section 2.4.3.1). A difference compared to pseudo-likelihood optimization that uses zero centered priors, is the centering of the Gaussian prior for the single potentials \\(\\v\\) at \\(v^*\\) as described in section 5.4. The hyperparameter tuning for stochastic gradient descent described in the last section applied the default pseudo-likelihood regularization coefficients \\(\\lambda_v \\eq 10\\) and \\(\\lambda_w \\eq 0.2\\cdot(L-1)\\). The regularization coefficient \\(\\lambda_w\\) for couplings \\(\\w\\) is defined with respect to protein length \\(L\\) owing to the fact that the number of possible contacts in a protein increases quadratically with \\(L\\) whereas the number of observed contacts only increases linearly as can be seen in Figure 8.7. Figure 8.7: Number of contacts (\\(\\Cb &lt; 8 \\angstrom\\)) with respect to protein length and sequence separation has a linear relationship. It is possible that CD achieves optimal performance using stronger or weaker regularization coefficients compared to pseudo-likelihood. Therefore, I evaluated performance of contrastive divergence using different regularization coefficients \\(\\lambda_w \\in \\{ 1\\mathrm{e}{-2}, 1\\mathrm{e}{-1}, 0.2, 1\\} \\cdot(L-1)\\) while leaving the regularization for single potentials at the default value \\(\\lambda_v \\eq 10\\). Furthermore, I analysed whether precision is impacted by only optimizing the couplings \\(\\w\\) (with default regularization) while fixing the single potentials \\(\\vi\\) to their best estimates \\(\\vi^*\\) as described in section 5.4. As can be seen in Figure 8.8, using strong regularization for the couplings \\(\\lambda_w \\eq (L-1)\\) results in a drop of mean precision. Using weaker regularization or fixing the single potentials hardly has an impact on precision with using \\(\\lambda_w \\eq 1\\mathrm{e}{-2}(L-1)\\) yielding slightly improved precision for the top ranked contacts. Figure 8.8: Performance of contrastive divergence optimization of the full likelihood with different regularization settings compared to pseudo-likelihood (blue) for 280 proteins. Contact scores are computed as the APC corrected Frobenius norm of the couplings \\(\\wij\\). Default regularization coefficients as used with pseudo-likelihood are \\(\\lambda_v \\eq 10\\) and \\(\\lambda_w \\eq 0.2(L-1)\\). “fixed vi” (orange) uses CD to optimize only couplings with default regularization while keeping the single potentials \\(\\vi\\) fixed at their MLE optimum \\(\\vi^*\\). The other optimization runs with CD (green, red, purple, brown) use default regularization for the single potentials and a regularization coefficient for the couplings according to legend description. 8.4.4 Optimizing the Sampling Scheme for Contrastive Divergence I analysed whether choosing a different number of sequences for the approximation of the gradient via Gibbs sampling can improve performance. Randomly selecting only a subset \\(N^{\\prime}\\) of the \\(N\\) observed sequences corresponds to the stochastic gradient descent idea of a minibatch and introduces additional stochasticity over the random Gibbs sampling process. Using \\(N^{\\prime} &lt; N\\) sequences for Gibbs sampling has the further advantage of decreasing the runtime at each iteration. Note, that the reference counts from the observed sequences \\(N_{ij} q(x_i \\eq a, x_j \\eq b)\\) that are part of the gradient calculation will be kept constant. Therefore it is necessary, however to rescale the amino acid counts from the sampled sequences in a way such that the total sample counts match the total observed counts. I evaluated two different schemes for randomly selecting \\(N^{\\prime} \\eq xL\\) sequences from the \\(N\\) given sequences of the alignment at every iteration: without replacement (enforcing \\(N^{\\prime} \\eq \\min(N, xL)\\)) with replacement with \\(x \\in \\{ 1, 5, 10, 50 \\}\\). As can be seen in the Figure ??, the choice of minibatch size which corresponds to the number sequences that are selected to approximate the gradient, has no influence on precision. PLOT PEFORMANCE SMAPLING SIZE This results is somewhat unexpected, because using more samples to approximate the gradient should result in a better gradient approximation and thus in a better performance. Indeed, the magnitude of the gradient norms decreases when more sequences are used for sampling as can be seen in Figure ??. However, this does apparantly not translate into better parameter values. PLOT GRADIENT NORMS The default CD algorithm as described by Hinton in 2002 applies only one full step of Gibbs sampling on each data sample to generate a sampled data set that will be used to approximate the gradient [129]. One full step of Gibbs sampling corresponds to sampling each position in a protein sequence according to the conditional probabilities computed from the current model probabilties as described in ??. The sampled sequences obtained after only one step of Gibbs sampling will be very similar to the input sequences. It has been shown that sampling with \\(n&gt;1\\) steps gives more precise results but increases computational cost per gradient evaluation [149,150]. In the following I analysed the impact on performance when Gibbs sampling each sequence with 1, 5 and 10 full steps. As can be seen, there is hardly an impact on precision while having much longer runtimes (by a factor or 5 and 10). Figure 8.9: Performance of contrastive divergence optimization of the full likelihood with different number of Gibbs steps compared to pseudo-likelihood (blue) for 287 proteins. Contact scores are computed as the APC corrected Frobenius norm of the couplings \\(\\wij\\). pseudo-likelihood: contact scores computed from pseudo-likelihood. The other methods derive contact scores from couplings computed from CD with different number of Gibbs sampling steps. Another variant of CD that has been suggested by Tieleman in 2008 is PCD[150] that does not reset the Markov Chains at every iteration. The reason being that when using small learning rates, the model changes only slightly between iterations and the true data distribution can be better approximated However, subsequent samples of PCD will be highly correlated creating a kind of momentum effect. Furthermore is has been found that PCD should be used with smaller learning rates and higher minibatch sizes. As PCD might require samller update steps and larger minibatches, I analysed the performance of PCD for the default settings of CD and additionally for smaller learning and decay rates and larger minibatches. Note that one Markov chain is kept for every sequence of the input alignment. At each iteration a subset \\(N^{\\prime} &lt; N\\) of the Markov chains is randomly selected (without replacement) and used to for another round of Gibss sampling at the current iteration. PLOT PCD for different LEARNIGN RATEWS and SAMPLE SIZES References "],
["bayesian-model-for-residue-resdiue-contact-prediction.html", "8.5 Bayesian Model for Residue-Resdiue Contact Prediction", " 8.5 Bayesian Model for Residue-Resdiue Contact Prediction 8.5.1 Efficiently Computing the negative Hessian of the regularized log-likelihood Surprisingly, the elements of the Hessian at the mode \\(\\w^*\\) are easy to compute. Let \\(i,j,k,l \\in \\{1,\\ldots,L\\}\\) be columns in the MSA and let \\(a, b, c, d \\in \\{1,\\ldots,20\\}\\) represent amino acids. The partial derivative \\(\\partial / \\partial \\w_{klcd}\\) of the second term in the gradient of the couplings in eq. (5.4) is \\[\\begin{eqnarray} \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\wklcd \\, \\partial \\wijab } &amp;=&amp; - \\sum_{n=1}^{N} \\, \\sum_{\\mathbf{y} \\in \\Sn} \\frac{\\partial \\left( \\frac{\\exp \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L}^L w_{ij}(y_i,y_j) \\right) }{Z_n(\\v,\\w)} \\right)}{\\partial \\wklcd} I(y_i \\eq a, y_j \\eq b) \\\\ &amp;&amp;- \\lambda_w \\delta_{ijab,klcd} \\,, \\end{eqnarray}\\] where \\(\\delta_{ijab,klcd} = I(ijab=klcd)\\) is the Kronecker delta. Applying the product rule, we find \\[\\begin{eqnarray} \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\wklcd \\, \\partial \\wijab } &amp;=&amp; - \\sum_{n=1}^{N} \\, \\sum_{\\mathbf{y} \\in \\Sn} \\frac{\\exp \\left(\\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L}^L w_{ij}(y_i,y_j) \\right)}{Z_n(\\v,\\w)} I(y_i \\eq a, y_j \\eq b) \\\\ &amp; \\times &amp; \\left[ \\frac{\\partial}{\\partial \\wklcd} \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L} w_{ij}(y_i,y_j) \\right) - \\frac{1}{Z_n(\\v,\\w)} \\frac{\\partial Z_n(\\v,\\w) }{\\partial\\wklcd} \\right] \\\\ &amp;-&amp; \\lambda_w \\delta_{ijab,klcd} \\\\ \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\wklcd \\, \\partial \\wijab } &amp;=&amp; - \\sum_{n=1}^{N} \\, \\sum_{\\mathbf{y} \\in \\Sn} \\frac{\\exp \\left(\\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L}^L w_{ij}(y_i,y_j) \\right)}{Z_n(\\v,\\w)} I(y_i \\eq a, y_j \\eq b) \\\\ &amp; \\times &amp; \\left[ I(y_k \\eq c, y_l \\eq d) - \\frac{\\partial}{\\partial \\wklcd} \\log Z_n(\\v,\\w) \\right] \\\\ &amp;-&amp; \\lambda_w \\delta_{ijab,klcd} \\,. \\end{eqnarray}\\] We simplify this expression using \\[\\begin{equation} p(\\mathbf{y} | \\v,\\w) = \\frac{\\exp \\left( \\sum_{i=1}^L v_i(y_i) + \\sum_{1 \\le i &lt; j \\le L} w_{ij}(y_i,y_j) \\right)}{Z_n(\\v,\\w)} , \\end{equation}\\] yielding \\[\\begin{eqnarray} \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\wklcd \\, \\partial \\wijab} &amp;=&amp; - \\sum_{n=1}^{N} \\, \\sum_{\\mathbf{y} \\in \\Sn} p(\\mathbf{y} | \\v,\\w) \\, I(y_i \\eq a, y_j \\eq b, y_k \\eq c, y_l \\eq d) \\\\ &amp;+&amp; \\sum_{n=1}^{N} \\, \\sum_{\\mathbf{y} \\in \\mathcal{S}_n} p(\\mathbf{y} | \\v,\\w) \\, I(y_i \\eq a, y_j \\eq b ) \\sum_{\\mathbf{y} \\in \\Sn} p(\\mathbf{y} | \\v,\\w) I(y_k \\eq c, y_l \\eq d ) \\\\ &amp;-&amp; \\lambda_w \\delta_{ijab,klcd} \\,. \\end{eqnarray}\\] If \\(\\X\\) does not contain too many gaps, this expression can be approximated by \\[\\begin{eqnarray} \\frac{\\partial^2 \\LLreg(\\v^*,\\w)}{\\partial \\wklcd \\, \\partial \\wijab } &amp;=&amp; - N_{ijkl} \\: p(x_i \\eq a, x_j \\eq b, x_k \\eq c, x_l \\eq d | \\v,\\w) \\nonumber \\\\ &amp;&amp; + N_{ijkl} \\: p(x_i \\eq a, x_j \\eq b | \\v,\\w) \\, p(x_k \\eq c, x_l \\eq d | \\v,\\w) - \\lambda_w \\delta_{ijab,klcd} \\,, \\end{eqnarray}\\] where \\(N_{ijkl}\\) is the number of sequences that have a residue in \\(i\\), \\(j\\), \\(k\\) and \\(l\\). Looking at three cases separately: case 1: \\((k,l) = (i,j)\\) and \\((c,d) = (a,b)\\) case 2: \\((k,l) = (i,j)\\) and \\((c,d) \\ne (a,b)\\) case 3: \\((k,l) \\ne (i,j)\\) and \\((c,d) \\ne (a,b)\\), the elements of \\(\\H\\), which are the negative second partial derivatives of \\(\\LLreg(\\v^*,\\w)\\) with respect to the components of \\(\\w\\), are \\[\\begin{eqnarray} \\mathrm{case~1:} (\\H)_{ijab, ijab} &amp;=&amp; N_{ij} \\, p(x_i \\eq a, x_j \\eq b| \\v^*,\\w^*) \\, ( 1 - p(x_i \\eq a, x_j \\eq b| \\v^*,\\w^*) \\,) \\\\ &amp;&amp; + \\lambda_w \\\\ \\mathrm{case~2:} (\\H)_{ijcd, ijab} &amp;=&amp; - N_{ij} \\, p(x_i \\eq a, x_j \\eq b |\\v^*,\\w^*) \\, p(x_i \\eq c, x_j \\eq d |\\v^*,\\w^*) \\\\ \\mathrm{case~3:} (\\H)_{klcd, ijab} &amp;=&amp; N_{ijkl} \\, p(x_i \\eq a, x_j \\eq b, x_k \\eq c, x_l \\eq d | \\v^*,\\w^*) \\nonumber \\\\ &amp;&amp; - N_{ijkl} \\, p(x_i \\eq a, x_j \\eq b | \\v^*,\\w^*)\\, p(x_k \\eq c, x_l \\eq d | \\v^*,\\w^*) \\,. \\tag{8.8} \\end{eqnarray}\\] We know from eq. (5.10) that at the mode \\(\\w^*\\) the model probabilities match the empirical frequencies up to a small regularization term, \\[\\begin{equation} p(x_i \\eq a, x_j \\eq b | \\v^*,\\w^*) = q(x_i \\eq a, x_j \\eq b) - \\frac{\\lambda_w}{N_{ij}} \\wijab^* \\,, \\end{equation}\\] and therefore the negative Hessian elements in cases 1 and 2 can be expressed as \\[\\begin{align} (\\H)_{ijab, ijab} =&amp; N_{ij} \\left( q(x_i \\eq a, x_j \\eq b) - \\frac{\\lambda_w}{N_{ij}} \\wijab^* \\right) \\left( 1 - q(x_i \\eq a, x_j \\eq b) +\\frac{\\lambda_w}{N_{ij}} \\wijab^* \\right) \\\\ &amp; + \\lambda_w \\\\ (\\H)_{ijcd, ijab} =&amp; -N_{ij} \\left(\\,q(x_i \\eq a, x_j \\eq b) - \\frac{\\lambda_w}{N_{ij}} \\wijab^* \\right) \\left( q(x_i \\eq c, x_j \\eq d) -\\frac{\\lambda_w}{N_{ij}} \\wijcd^* \\right) . \\tag{8.9} \\end{align}\\] In order to write the previous eq. (8.9) in matrix form, the regularised empirical frequencies \\(\\qij\\) will be defined as \\[\\begin{equation} (\\qij)_{ab} = q&#39;_{ijab} := q(x_i \\eq a, x_j \\eq b) - \\lambda_w \\wijab^* / N_{ij} \\,, \\end{equation}\\] and the \\(400 \\times 400\\) diagonal matrix \\(\\Qij\\) will be defined as \\[\\begin{equation} \\Qij := \\text{diag}(\\qij) \\; . \\end{equation}\\] Now eq. (8.9) can be written in matrix form \\[\\begin{equation} \\H_{ij} = N_{ij} \\left( \\Qij - \\qij \\qij^{\\mathrm{T}} \\right) + \\lambda_w \\I \\; . \\tag{8.10} \\end{equation}\\] 8.5.2 Efficiently Computing the Inverse of Matrix \\(\\Lijk\\) It is possible to efficiently invert the matrix \\(\\Lijk = \\H_{ij} - \\lambda_w \\I + \\Lambda_k\\), that is introduced in 6.2 where \\(\\H_{ij}\\) is the \\(400 \\times 400\\) diagonal block submatrix \\((\\H_{ij})_{ab,cd} := (\\H)_{ijab,ijcd}\\) and \\(\\Lambda_k\\) is an invertible diagonal precision matrix that is introduced in section ??. Equation (8.10) can be used to write \\(\\Lijk\\) in matrix form as \\[\\begin{equation} \\Lijk = \\H_{ij} - \\lambda_w \\I + \\Lk = N_{ij} \\Qij- N_{ij} \\qij \\qij^{\\mathrm{T}} + \\Lk \\,. \\tag{8.11} \\end{equation}\\] Owing to eqs. (5.2) and (5.6), \\(\\sum_{a,b=1}^{20} q&#39;_{ijab} = 1\\). The previous equation (8.11) facilitates the calculation of the inverse of this matrix using the Woodbury identity for matrices \\[\\begin{equation} (\\mathbf{A} + \\mathbf{B} \\mathbf{D}^{-1} \\mathbf{C})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{B} (\\mathbf{D} + \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B}) ^{-1} \\mathbf{C} \\mathbf{A}^{-1} \\;. \\end{equation}\\] by setting \\[\\begin{align} \\mathbf{A} &amp;= N_{ij} \\Qij + \\Lk \\\\ \\mathbf{B} &amp;= \\qij \\\\ \\mathbf{C} &amp;= \\qij^\\mathrm{T} \\\\ \\mathbf{D} &amp;=- N_{ij}^{-1} \\\\ \\end{align}\\] \\[\\begin{align} \\left( \\H_{ij} - \\lambda_w \\I + \\Lk \\right)^{-1} &amp; = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\qij \\left( -N_{ij}^{-1} + \\qij^\\mathrm{T} \\mathbf{A}^{-1} \\qij \\right)^{-1} \\qij^\\mathrm{T} \\mathbf{A}^{-1} \\\\ &amp; = \\mathbf{A}^{-1} + \\frac{ (\\mathbf{A}^{-1} \\qij) (\\mathbf{A}^{-1} \\qij)^{\\mathrm{T}} }{ N_{ij}^{-1} - \\qij^\\mathrm{T} \\mathbf{A}^{-1} \\qij} \\,. \\tag{8.12} \\end{align}\\] Note that \\(\\mathbf{A}\\) is diagonal as \\(\\Qij\\) and \\(\\Lk\\) are diagonal matrices: \\(\\mathbf{A} = \\text{diag}(N_{ij} q&#39;_{ijab} + (\\Lk)_{ab,ab})\\). Moreover, \\(\\mathbf{A}\\) has only positive diagonal elements, because \\(\\Lk\\) is invertible and has only positive diagonal elements and because \\(q&#39;_{ijab} = p(x_i \\eq a, x_j \\eq b | \\v^*,\\w^*) \\ge 0\\). Therefore \\(\\mathbf{A}\\) is invertible: \\(\\mathbf{A}^{-1} = \\text{diag}(N_{ij} q&#39;_{ijab} + (\\Lk)_{ab,ab} )^{-1}\\). Because \\(\\sum_{a,b=1}^{20} q&#39;_{ijab} = 1\\), the denominator of the second term is \\[\\begin{equation} N_{ij}^{-1} - \\sum_{a,b=1}^{20} \\frac{{q&#39;}_{ijab}^2}{N_{ij} q&#39;_{ijab} + {(\\Lk)}_{ab,ab} } &gt; N_{ij}^{-1} - \\sum_{a,b=1}^{20} \\frac{{q&#39;}^2_{ijab}}{N_{ij} q&#39;_{ijab}} = 0 \\end{equation}\\] and therefore the inverse of \\(\\Lijk\\) in eq. (8.12) is well defined. The log determinant of \\(\\Lijk\\) is necessary to compute the ratio of Gaussians (see equation (??)) and can be computed using the matrix determinant lemma: \\[\\begin{equation} \\det(\\mathbf{A} + \\mathbf{uv}^\\mathrm{T}) = (1+\\mathbf{v}^\\mathrm{T} \\mathbf{A}^{-1} \\mathbf{u}) \\det(\\mathbf{A}) \\end{equation}\\] Setting \\(\\mathbf{A} = N_{ij} \\Qij + \\Lk\\) and \\(\\v = \\qij\\) and \\(\\mathbf{u} = - N_{ij} \\qij\\) yields \\[\\begin{equation} \\det(\\Lijk ) = \\det(\\H_{ij} - \\lambda_w \\I + \\Lk) = (1 - N_{ij}\\qij^\\mathrm{T} \\mathbf{A}^{-1}\\qij) \\det(\\mathbf{A}) \\,. \\end{equation}\\] \\(\\mathbf{A}\\) is diagonal and has only positive diagonal elements so that \\(\\log(\\det(\\mathbf{A})) = \\sum \\log \\left( \\text{diag}(\\mathbf{A}) \\right)\\). 8.5.3 Training the Hyperparameters \\(\\muk\\), \\(\\Lk\\) and \\(\\gamma_k\\) The model parameters \\(\\mathbf{\\mu} = (\\mathbf{\\mu}_{1},\\ldots,\\mathbf{\\mu}_K)\\), \\(\\mathbf{\\Lambda} = (\\mathbf{\\Lambda}_1,\\ldots,\\mathbf{\\Lambda}_K)\\) and \\(\\mathbf{\\gamma} = (\\mathbf{\\gamma}_1,\\ldots,\\mathbf{\\gamma}_K)\\) will be trained by maximizing the logarithm of the full likelihood over a set of training MSAs \\(\\X^1,\\ldots,\\X^N\\) and associated structures with distance vectors \\(\\r^1,\\ldots,\\r^N\\) plus a regularizer \\(R(\\mathbf{\\mu}, \\mathbf{\\Lambda})\\): \\[\\begin{equation} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}, \\mathbf{\\gamma}) + R(\\mathbf{\\mu}, \\mathbf{\\Lambda}) = \\sum_{n=1}^N \\log p(\\X^n | \\r^n, \\mathbf{\\mu}, \\mathbf{\\Lambda}, \\mathbf{\\gamma} ) + R(\\mathbf{\\mu}, \\mathbf{\\Lambda}) \\rightarrow \\max \\, . \\end{equation}\\] The regulariser penalizes values of \\(\\muk\\) and \\(\\Lk\\) that deviate too far from zero: \\[\\begin{align} R(\\mathbf{\\mu}, \\mathbf{\\Lambda}) = -\\frac{1}{2 \\sigma_{\\mu}^2} \\sum_{k=1}^K \\sum_{ab=1}^{400} \\mu_{k,ab}^2 -\\frac{1}{2 \\sigma_\\text{diag}^2} \\sum_{k=1}^K \\sum_{ab=1}^{400} \\Lambda_{k,ab,ab}^2 \\tag{8.13} \\end{align}\\] Reasonable values are \\(\\sigma_{\\mu}=0.1\\), \\(\\sigma_\\text{diag} = 100\\). The log likelihood can be optimized using LBFG-S-B[???], which requires the computation of the gradient of the log likelihood. For simplicity of notation, the following calculations consider the contribution of the log likelihood for just one protein, which allows to drop the index \\(n\\) in \\(\\rij^n\\), \\((\\wij^n)^*\\) and \\(\\Hij^n\\). From eq. (6.12) the log likelihood for a single protein is \\[\\begin{equation} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}, \\gamma_k) = \\sum_{1 \\le i &lt; j \\le L} \\log \\sum_{k=0}^K g_{k}(\\rij) \\frac{\\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss(\\mathbf{0} | \\muijk, \\Lijk^{-1})} + R(\\mathbf{\\mu}, \\mathbf{\\Lambda}) + \\text{const.}\\,. \\tag{8.14} \\end{equation}\\] 8.5.4 The gradient of the log likelihood with respect to \\(\\mathbf{\\mu}\\) By applying the formula \\(d f(x) / dx = f(x) \\, d \\log f(x) / dx\\) to compute the gradient of eq. (8.14) (neglecting the regularization term) with respect to \\(\\mu_{k,ab}\\), one obtains \\[\\begin{equation} \\frac{\\partial}{\\partial \\mu_{k,ab}} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}, \\gamma_k) = \\sum_{1\\le i&lt;j\\le L} \\frac{ g_{k}(\\rij) \\frac{ \\Gauss ( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\frac{\\partial}{\\partial \\mu_{k,ab}} \\log \\left( \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\right) } { \\sum_{k&#39;=0}^K g_{k&#39;}(\\rij) \\, \\frac{ \\Gauss(\\mathbf{0} | \\muk&#39;, \\Lk&#39;^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} } . \\tag{8.15} \\end{equation}\\] To simplify this expression, we define the responsibility of component \\(k\\) for the posterior distribution of \\(\\wij\\), the probability that \\(\\wij\\) has been generated by component \\(k\\): \\[\\begin{align} p(k|ij) = \\frac{ g_{k}(\\rij) \\frac{ \\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss(\\mathbf{0} | \\muijk, \\Lijk^{-1})} } {\\sum_{k&#39;=0}^K g_{k&#39;}(\\rij) \\frac{ \\Gauss(\\mathbf{0} | \\muk&#39;, \\Lk&#39;^{-1})}{\\Gauss( \\mathbf{0} | \\muijk&#39;, \\Lijk&#39;^{-1})} } \\,. \\tag{8.16} \\end{align}\\] By substituting the definition for responsibility, (8.15) simplifies \\[\\begin{equation} \\frac{\\partial}{\\partial \\mu_{k,ab}} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}, \\gamma_k) = \\sum_{1\\le i&lt;j\\le L} p(k | ij) \\frac{\\partial}{\\partial \\mu_{k,ab}} \\log \\left( \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\right) , \\tag{8.17} \\end{equation}\\] and analogously for partial derivatives with respect to \\(\\Lambda_{k,ab,cd}\\). The partial derivative inside the sum can be written \\[\\begin{equation} \\frac{\\partial}{\\partial \\mu_{k,ab}} \\log \\left( \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\right) = \\frac{1}{2} \\frac{\\partial}{\\partial \\mu_{k,ab}} \\left( \\log | \\Lk | - \\muk^\\mathrm{T} \\Lk \\muk - \\log | \\Lijk | + \\muijk^\\mathrm{T} \\Lijk \\muijk \\right)\\,. \\end{equation}\\] Using the following formula for a matrix \\(\\mathbf{A}\\), a real variable \\(x\\) and a vector \\(\\mathbf{y}\\) that depends on \\(x\\), \\[\\begin{equation} \\frac{\\partial}{\\partial x} \\left( \\mathbf{y}^\\mathrm{T} \\mathbf{A} \\mathbf{y} \\right) = \\frac{\\partial \\mathbf{y}^\\mathrm{T}}{\\partial x} \\mathbf{A} \\mathbf{y} + \\mathbf{y}^\\mathrm{T} \\mathbf{A} \\frac{\\partial \\mathbf{y}}{\\partial x} = \\mathbf{y}^\\mathrm{T} (\\mathbf{A} + \\mathbf{A}^\\mathrm{T}) \\frac{\\partial \\mathbf{y}}{\\partial x} \\tag{8.18} \\end{equation}\\] the partial derivative therefore becomes \\[\\begin{align} \\frac{\\partial}{\\partial \\mu_{k,ab}} \\log \\left( \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\right) =&amp; \\left( -\\muk^\\mathrm{T} \\Lk \\mathbf{e}_{ab} \\, + \\muijk^\\mathrm{T} \\Lijk \\Lijk^{-1} \\Lk \\mathbf{e}_{ab} \\right) \\\\ =&amp; \\mathbf{e}^\\mathrm{T}_{ab} \\Lk ( \\muijk - \\muk ) \\; . \\end{align}\\] Finally, the gradient of the log likelihood with respect to \\(\\mathbf{\\mu}\\) becomes \\[\\begin{align} \\nabla_{\\muk} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}, \\gamma_k) = \\sum_{1\\le i&lt;j\\le L} p(k|ij) \\, \\Lk \\left( \\muijk - \\muk \\right) \\; . \\tag{8.19} \\end{align}\\] 8.5.5 The gradient of the log likelihood with respect to \\(\\Lk\\) Analogously to eq. (8.17) one first needs to solve \\[\\begin{align} &amp; \\frac{\\partial}{\\partial \\Lambda_{k,ab,cd}} \\log \\frac{\\Gauss( \\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} = \\\\ &amp;\\frac{1}{2} \\frac{\\partial}{\\partial \\Lambda_{k,ab,cd}} \\left( \\log |\\Lk| - \\muk^\\mathrm{T} \\Lk \\muk - \\log |\\Lijk| + \\muijk^\\mathrm{T} \\Lijk \\muijk \\right) \\,, \\tag{8.20} \\end{align}\\] by applying eq. (8.18) as before as well as the formulas \\[\\begin{align} \\frac{\\partial}{\\partial x} \\log |\\mathbf{A} | &amp;= \\text{Tr}\\left( \\mathbf{A}^{-1} \\frac{\\partial \\mathbf{A}}{\\partial x} \\right) , \\\\ \\frac{\\partial \\mathbf{A}^{-1}}{\\partial x} &amp;= - \\mathbf{A}^{-1} \\frac{\\partial \\mathbf{A}}{\\partial x} \\mathbf{A}^{-1} \\,. \\end{align}\\] This yields \\[\\begin{align} \\frac{\\partial}{\\partial \\Lambda_{k,ab,cd}} \\log |\\Lk| &amp;= \\text{Tr} \\left( \\Lk^{-1} \\frac{\\partial \\Lk}{\\partial \\Lambda_{k,ab,cd}} \\right) = \\text{Tr} \\left( \\Lk^{-1} \\mathbf{e}_{ab} \\mathbf{e}_{cd}^\\mathrm{T} \\right) = \\Lambda^{-1}_{k,cd,ab} \\\\ \\frac{\\partial}{\\partial \\Lambda_{k,ab,cd}} \\log |\\Lijk| &amp;= \\text{Tr} \\left( \\Lijk^{-1} \\frac{\\partial (\\H_{ij} - \\lambda_w \\I + \\Lk)}{\\partial \\Lambda_{k,ab,cd}} \\right) = \\Lambda^{-1}_{ij,k,cd,ab} \\\\ \\frac{\\partial (\\muk^\\mathrm{T} \\Lk \\muk)}{\\partial \\Lambda_{k,ab,cd}} &amp;= \\muk^\\mathrm{T} \\mathbf{e}_{ab} \\mathbf{e}_{cd}^\\mathrm{T} \\muk = \\mathbf{e}_{ab}^\\mathrm{T} \\muk \\muk^\\mathrm{T} \\mathbf{e}_{cd} = (\\muk \\muk^\\mathrm{T})_{ab,cd} \\\\ \\frac{\\partial ( \\muijk^\\mathrm{T} \\Lijk \\muijk) }{\\partial \\Lambda_{k,ab,cd}} &amp;= \\muijk^\\mathrm{T} \\frac{\\partial \\Lijk}{\\partial \\Lambda_{k,ab,cd}} \\muijk + 2 \\muijk^\\mathrm{T} \\Lijk \\frac{\\partial \\Lijk^{-1}}{\\partial \\Lambda_{k,ab,cd}} (\\Hij \\wij^* + \\Lk \\muk) + 2 \\muijk^\\mathrm{T} \\frac{\\partial \\Lk}{\\partial \\Lambda_{k,ab,cd}} \\muk \\nonumber \\\\ &amp;= (\\muijk \\muijk^\\mathrm{T} + 2 \\muijk \\muk^\\mathrm{T})_{ab,cd} - 2 \\muijk^\\mathrm{T} \\Lijk \\Lijk^{-1} \\frac{\\partial \\Lijk}{\\partial \\Lambda_{k,ab,cd}} \\Lijk^{-1} (\\Hij\\wij^* + \\Lk \\muk) \\\\ &amp;= (\\muijk \\muijk^\\mathrm{T} + 2 \\muijk \\muk^\\mathrm{T})_{ab,cd} - 2 \\muijk^\\mathrm{T} \\frac{\\partial \\Lijk}{\\partial \\Lambda_{k,ab,cd}} \\muijk\\\\ &amp;= (- \\muijk \\muijk^\\mathrm{T} + 2 \\muijk \\muk^\\mathrm{T})_{ab,cd} \\,. \\end{align}\\] Inserting these results into eq. (8.20) yields \\[\\begin{align} \\frac{\\partial}{\\partial \\Lambda_{k,ab,cd}} \\log \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} = \\frac{1}{2} \\left( \\Lk^{-1} - \\Lijk^{-1} - (\\muijk - \\muk) (\\muijk - \\muk)^\\mathrm{T} \\right)_{ab,cd}\\,. \\end{align}\\] Substituting this expression into the equation (8.17) analogous to the derivation of gradient for \\(\\mu_{k,ab}\\) yields the equation \\[\\begin{align} \\nabla_{\\Lk} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}, \\gamma_k) = \\frac{1}{2} \\sum_{1\\le i&lt;j\\le L} p(k|ij) \\, \\left( \\Lk^{-1} - \\Lijk^{-1} - (\\muijk - \\muk) (\\muijk - \\muk)^\\mathrm{T} \\right). \\tag{8.21} \\end{align}\\] 8.5.6 The gradient of the log likelihood with respect to \\(\\gamma_k\\) With \\(\\rij \\in \\{0,1\\}\\) defining a residue pair in physical contact or not in contact, the mixing weights can be modelled as a softmax function according to eq. (6.5). The derivative of the mixing weights \\(g_k(\\rij)\\) is: \\[\\begin{eqnarray} \\frac{\\partial g_{k&#39;}(\\rij)} {\\partial \\gamma_k} = \\left\\{ \\begin{array}{lr} g_k(\\rij) (1 - g_k(\\rij)) &amp; : k&#39; = k\\\\ g_{k&#39;}(\\rij) - g_k(\\rij) &amp; : k&#39; \\neq k \\end{array} \\right. \\end{eqnarray}\\] The partial derivative of the likelihood function with respect to \\(\\gamma_k\\) is: \\[\\begin{align} \\frac{\\partial} {\\partial \\gamma_k} L\\!L(\\mathbf{\\mu}, \\mathbf{\\Lambda}, \\gamma_k) =&amp; \\sum_{1\\le i&lt;j\\le L} \\frac{\\sum_{k&#39;=0}^K \\frac{\\partial}{\\partial \\gamma_k} g_{k&#39;}(\\rij) \\frac{\\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( 0 | \\muijk, \\Lijk^{-1})}} {\\sum_{k&#39;=0}^K g_{k&#39;}(\\rij) \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})}} \\\\ =&amp; \\sum_{1\\le i&lt;j\\le L} \\frac{\\sum_{k&#39;=0}^K g_{k&#39;}(\\rij) \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})} \\cdot \\begin{cases} 1-g_k(\\rij) &amp; \\text{if } k&#39; = k \\\\ -g_k(\\rij) &amp; \\text{if } k&#39; \\neq k \\end{cases}} {\\sum_{k&#39;=0}^K g_{k&#39;}(\\rij) \\frac{ \\Gauss(\\mathbf{0} | \\muk, \\Lk^{-1})}{\\Gauss( \\mathbf{0} | \\muijk, \\Lijk^{-1})}} \\\\ =&amp; \\sum_{1\\le i&lt;j\\le L} \\sum_{k&#39;=0}^K p(k&#39;|ij) \\begin{cases} 1-g_k(\\rij) &amp; \\text{if } k&#39; = k \\\\ -g_k(\\rij) &amp; \\text{if } k&#39; \\neq k \\end{cases} \\\\ =&amp; \\sum_{1 \\leq i&lt;j\\leq L} p(k|ij) - g_k(\\rij) \\sum_{k&#39;=0}^K p(k&#39;|ij) \\nonumber\\\\ =&amp; \\sum_{1 \\leq i&lt;j\\leq L} p(k|ij) - g_k(\\rij) \\end{align}\\] "],
["bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances.html", "8.6 Bayesian Statistical Model for Prediction of Protein Residue-Residue Distances", " 8.6 Bayesian Statistical Model for Prediction of Protein Residue-Residue Distances 8.6.1 Modelling the dependence of \\(\\wij\\) on distance It is straightforward to extend the model presented in 6.2 for distances. The mixture weights \\(g_k(\\rij)\\) in eq. (6.4) are modelled as softmax over linear functions \\(\\gamma_k(\\rij)\\) (Figure (fig:softmax-linear-fct): \\[\\begin{align} g_k(\\rij) &amp;= \\frac{\\exp \\gamma_k(\\rij)}{\\sum_{k&#39;=0}^K \\exp \\gamma_{k&#39;}(\\rij)} \\, , \\\\ \\gamma_k(\\rij) &amp;= - \\sum_{k&#39;=0}^{k} \\alpha_{k&#39;} ( \\rij - \\rho_{k&#39;}) . \\tag{8.22} \\end{align}\\] Figure 8.10: The Gaussian mixture coefficients \\(g_k(\\rij)\\) of \\(p(\\wij|\\rij)\\) are modelled as softmax over linear functions \\(\\gamma_k(\\rij)\\). \\(\\rho_k\\) sets the transition point between neighbouring components \\(g_{k-1}(\\rij)\\) and \\(g_k(\\rij)\\), while \\(\\alpha_k\\) quantifies the abruptness of the transition between \\(g_{k-1}(\\rij)\\) and \\(g_k(\\rij)\\). The functions \\(g_k(\\rij)\\) remain invariant when adding an offset to all \\(\\gamma_k(\\rij)\\). This degeneracy can be removed by setting \\(\\gamma_0(\\rij) = 0\\) (i.e., \\(\\alpha_0 = 0\\) and \\(\\rho_0=0\\)). Further, the components are ordered, \\(\\rho_1&gt; \\ldots &gt; \\rho_K\\) and it is demanded that \\(\\alpha_k &gt; 0\\) for all \\(k\\). This ensures that for \\(\\rij \\rightarrow \\infty\\) we will obtain \\(g_0(\\rij) \\rightarrow 1\\) and hence \\(p(\\w | \\X) \\rightarrow \\Gauss(0, \\sigma_0^2 \\I )\\). The parameters \\(\\rho_k\\) mark the transition points between the two Gaussian mixture components \\(k-1\\) and \\(k\\), i.e., the points at which the two components obtain equal weights. This follows from \\(\\gamma_k(\\rij) - \\gamma_{k-1}(r) = \\alpha_{t} ( \\rij - \\rho_{t})\\) and hence \\(\\gamma_{k-1}(\\rho_k) = \\gamma_k(\\rho_k)\\). A change in \\(\\rho_k\\) or \\(\\alpha_k\\) only changes the behaviour of \\(g_{k-1}(\\rij)\\) and \\(g_k(\\rij)\\) in the transition region around \\(\\rho_k\\). Therefore, this particular definition of \\(\\gamma_k(\\rij)\\) makes the parameters \\(\\alpha_k\\) and \\(\\rho_k\\) as independent of each other as possible, rendering the optimisation of these parameters more efficient. 8.6.2 Training the Hyperparameters \\(\\rho_k\\) and \\(\\alpha_k\\) for distance-dependent prior "],
["training-random-forest-contat-prior.html", "8.7 Training Random Forest Contat Prior", " 8.7 Training Random Forest Contat Prior 8.7.1 Sequence Derived Features Given a multiple sequence alignment of a protein family, various sequence features can be derived that have been found to be informative of a residue-residue contact. In total there are 250 features that can be divided into global, single position and pairwise features and are described in the following sections. If not stated otherwise, weighted features have been computed using amino acid counts or amino acid frequencies based on weighted sequences as described in section 8.2.3. 8.7.1.1 Global Features These features describe alignment characteristics. Every pair of residues \\((i,j)\\) from the same protein will be attributed the same feature. Features characterizing the total alignment Feature Description No. Features per residue pair \\((i, j)\\) L log of protein length 1 N number of sequences 1 Neff number of effective sequences Neff computed as the sum over sequence weights (see section 8.2.3) 1 gaps average percentage of gaps over all positions 1 diversity \\(\\frac{\\sqrt{N}}{L}\\), N=number of sequences, L=protein length 1 amino acid composition weighted amino acid frequencies in alignment 20 secondary structure prediction average three state propensities PSIPRED (v4.0)[151] 3 secondary structure prediction average three state propensities Netsurfp (v1.0)[152] 3 contact prior protein length simple contact predictor based on expected number of contacts per protein with respect to protein length (see next subsection 8.7.1.4) 1 There are in total 32 global alignment features. 8.7.1.2 Single Position Features These features describe characteristics of a single alignment column. Every residue pair \\((i,j)\\) will be described by two features, once for each position. Single Position Sequence Features Feature Description No. Features per residue pair \\((i, j)\\) shannon entropy (20 states) \\(- \\sum_{a=1}^{20} p_a \\log p_a\\) 2 shannon entropy (21 states) \\(- \\sum_{a=1}^{21} p_a \\log p_a\\) 2 kullback leibler divergence between weighted observed and background amino acid frequencies [153] 2 jennson shannon divergence between weighted observed and background amino acid frequencies [153] 2 PSSM log odds ratio of weighted observed and background amino acid frequencies [153] 40 secondary structure prediction three state propensities PSIPRED (v4.0) [151] 6 secondary structure prediction three state propensities Netsurfp (v1.0) [152] 6 solvent accessibility prediction RSA and RSA Z-score Netsurfp (v1.0) [152] 4 relative position in sequence \\(\\frac{i}{L}\\) for a protien of length \\(L\\) 2 number of ungapped sequences \\(\\sum_n w_n I(x_{ni} \\neq 20)\\) for sequences \\(x_n\\) and sequence weights \\(w_n\\) 2 percentage of gaps \\(\\frac{\\sum_n w_n I(x_{ni} = 20)}{N_{\\text{eff}}}\\) for sequences \\(x_n\\) and sequence weights \\(w_n\\) 2 Average physico-chemical properties Atchley Factors 1-5 [154] 10 Average physico-chemical properties Polarity according to Grantham, 1974. Data taken from AAindex Database [155]. 2 Average physico-chemical properties Polarity according to Zimmermann et al., 1986. Data taken from AAindex Database [155]. 2 Average physico-chemical properties Isoelectric point according to Zimmermann et al., 1968. Data taken from AAindex Database [155]. 2 Average physico-chemical properties Hydrophobicity scale according to Wimley &amp; White, 1996. Data taken from UCSF Chimera [156]. 2 Average physico-chemical properties Hydrophobicity index according to Kyte &amp; Doolittle, 1982. Data taken from AAindex Database [155]. 2 Average physico-chemical properties Hydrophobicity according to Cornette [157]. 2 Average physico-chemical properties Bulkiness according to Zimmerman et al., 1968. Data taken from AAindex Database [155]. 2 Average physico-chemical properties Average volumes of residues according to Pontius et al., 1996. Data taken from AAindex Database [155]. 2 There are in total 96 single sequence features. Additionally, all single features will be computed within a window of size 5. The window feature for center residue \\(i\\) will be computed as the mean feature over residues \\([i-2, \\ldots, i, \\ldots, i+2]\\). Whenever the window extends the range of the sequence (for \\(i\\!&lt;\\!2\\) and \\(i\\!&gt;\\!(L-2)\\)), the window feature will be computed only for valid sequence positions. This results in additional 96 window features. 8.7.1.3 Pairwise Features These features are computed for every pair of columns \\((i, j)\\) in the alignment with \\(i&lt;j\\). Pairwise Sequence Features Feature Description No. Features per residue pair \\((i, j)\\) sequence separation \\(j-i\\) 1 gaps pairwise percentage of gaps using weighted sequences 1 number of ungapped sequences \\(\\sum_n w_n I(x_{ni} \\! \\neq \\! 20, x_{nj} \\! \\neq \\! 20)\\) for sequences \\(x_n\\) and sequence weights \\(w_n\\) 1 correlation physico-chemical features pairwise correlation of all physico-chemical properties listed in 8.7.1.2 13 pairwise potential Average quasi-chemical energy of interactions in an average buried environment. Data taken from AAindex Database [155]. 1 pairwise potential Average quasi-chemical energy of transfer of amino acids from water to the protein environment. Data taken from AAindex Database [155]. 1 pairwise potential Average general contact potential by Li&amp;Fang [54] 1 pairwise potential Average statistical potential from residue pairs in beta-sheets by Zhu&amp;Braun [158] 1 joint_shannon_entropy (20 state) \\(- \\sum_{a=1}^{20}\\sum_{b=1}^{20} p(a,b) \\log p(a,b)\\) 1 joint_shannon_entropy (21 state) \\(- \\sum_{a=1}^{21}\\sum_{b=1}^{21} p(a,b) \\log p(a,b)\\) 1 mutual information (MI) mutual information of amino acid counts at two positions; several variants: MI with pseudo-counts, MI with pseudo-counts + APC, normalized MI 3 OMES according to Fodor&amp;Aldrich [142] with and without APC 2 There are in total 26 pairwise sequence features. 8.7.1.4 Protein length dependent Contact Prior The average number of contats per residue, computed as the observed number of contacts divided by protein length L, has a non-linear relationship with protein length L as can be seen in Figure 8.11. Figure 8.11: Observed number of contacts per residue has a non-linear relationship with protein length. Distribution is shown for several thresholds of sequence separation. In log space, the average number of contats per residue can be fitted with a linear regression (see Figure 8.12) and yields the following functions: \\(f(L) = 1.556 + 0.596 \\log (L)\\) for sequence separation of 0 positions \\(f(L) = -1.273 + 0.59 \\log (L)\\) for sequence separation of 8 positions \\(f(L) = -1.567 + 0.615 \\log (L)\\) for sequence separation of 12 positions \\(f(L) = -2.0 + 0.624 \\log (L)\\) for sequence separation of 24 positions A simple contact predictor can be formulated as the ratio of the expected number of contacts per residue, given by \\(f(L)\\), and the possible number of contacts per residue which is \\(L-1\\), \\[ p(r_{ij} = 1 | L) = \\frac{f(L)}{L-1} \\; , \\] with \\(r_{ij}=1\\) representing a contact between residue \\(i\\) and \\(j\\). Figure 8.12: Linear regression fits for average number of contats per residue on logarithm of protein length. Distribution and linear regression fits are shown for different sequence separation thresholds. 8.7.2 Hyperparameter Optimization for Random Forest Prior There are several hyperparameters in a random forest model that need to be tuned to achieve best balance between predictive power and runtime. While more trees in the random forest generally improve performance of the model, they will slow down training and prediction. A crucial hyperparamter is the number of features that is randomly selected for a split at each node in a tree [159]. Stochasticity introduced by the random selection of features is a key characteristic of random forests as it reduces correlation between the trees and thus the variance of the predictor. Selecting many features typically increases performance as more options can be considered for each split, but at the same time increases risk of overfitting and decreases speed of the algorithm. In general, random forests are robust to overfitting, as long as there are enough trees in the ensemble and the selection of features for splitting a node introduces sufficient stochasticity. Overfitting can furthermore be prevented by restricting the depth of the trees, which is known as pruning or by enforcing a minimal node size with respect to the number of features per node. A positive side-effect of taking these measures is a speedup of the algorithm. [160] In the following, I use 5-fold cross-validation to identify the optimal architecture of the random forest. I used the module RandomForestClassifier in the Python package sklearn (v. 0.19) [161] and trained the models on sequence features extracted from MSAs as described in section 8.7.1. Single position features are computed with a window of size five as described in section 8.7.1.2. Proteins constitute highly imbalanced datasets with respect to the number of residue pairs that form and form not physical contacts. As can be seen in Figure 8.13, depending on the enforced sequence separation threshold the percentage of contacts varies between approximately 1% and 5%. Figure 8.13: Fraction of contacts among all possible contacts (\\(\\frac{L(L-1)}{2}\\)) in a protein against protein length L. The distribution has a non-linear relationship. At a sequence separation threshold &gt;8 positions the fraction of contacts for intermediate size proteins with length &gt;100 is approximately 2%. Most studies applying machine learning algorithms to the problem of predicting residue-residue contacts, chose the standard approach of rebalancing the data set by undersampling of the majority class. Study Proportion of Contacts Proportion of Non-contacts Wu et al. (2008) [53] 1 4 Li et al. (2011) [54] 1 1, 2 Wang et al. (2011) [55] 1 4 DiLena et al. (2012) [63] 1 ~4 Wang et al. (2013) [56] 1 ~4 I followed the same strategy and undersampled residue pairs that are not physical contacts with a proportion of contacts to non-contacts of 1:5. The training set is comprised of 50.000 residue pairs \\(&lt; 8 \\AA\\) (“contacts”“) and 250.000 residue pairs \\(&gt; 8 \\AA\\) (”non-contacts“) so that each of the five cross-validation models will be trained on 40.000 contacts and 200.000 non-contacts. As the training set has been undersampled for non-contacts, it is not representative of real world proteins and the models should be validated on a more realistic validation set. Each of the five models is therefore cross-validated on an own independent dataset of residue pairs extracted from 40 proteins by means of the standard contact prediction benchmark (mean precision against top ranked contacts). First I assessed performance of models for combinations of the parameter n_estimators, defining the number of trees in the forest and the parameter max_depth defining the maximum depth of the trees: n_estimators \\(\\in \\{100,500,1000\\}\\) max_depth \\(\\in \\{10, 100, 1000, None\\}\\) Figure 8.14 shows that the top five parameter combinations perform nearly identical. Random forests with 1000 trees perform slightly better than models constituting 500 trees, irrespective of the depth of the trees. In order to keep model complexity small, n_estimators=1000 and max_depth=100 for further analysis. Figure 8.14: Mean precision over 200 proteins against highest scoring contact predictions from random forest models for different settings of n_estimators and max_depth. Dashed lines show the performance of models that have been learned on the five different subsets of training data. Solid lines give the mean precision over the five models. Only those models are shown that yielded the five highest mean precision values (given in parantheses in the legend). Random forest models with 1000 trees and maximum depth of trees of either 100, 1000 or unrestricted tree depth perform nearly identical (lines overlap). Random forest models with 500 trees and max_depth=10 or max_depth=100 perform slightly worse. Next, I optimized the parameters min_samples_leaf, defining the minimum number of samples required to be at a leaf node and max_features, defining the number of randomly selected features considered for each split using the following settings: min_samples_leaf \\(\\in \\{1, 10, 100\\}\\) max_features \\(\\in \\{\\text{sqrt}, \\text{log2}, 0.15, 0.3\\}\\) Randomly selecting 39% of features (=75 features) and requiring at least 10 samples per leaf gives highest mean precision as can be seen in Figure 8.15. I chose max_features = 0.30 and min_samples_leaf = 10 for further analysis. Tuning the hyperparameters in a different order or on a larger dataset gives similar results. Figure 8.15: Mean precision over 200 proteins against highest scoring contact predictions from random forest models with different settings of min_samples_leaf and max_features. Dashed lines show the performance of models that have been learned on the five different subsets of training data. Solid lines give the mean precision over the five models. Only those models are shown that yielded the five best mean precision values (given in parantheses in the legend). In a next step I assessed dataset specific settings, such as the window size over which single positions features will be computed, the distance threshold to define non-contacts and the optimal proportions of contacts and non-contacts in the training set. I used the previously identified settings of random forest hyperparameters (n_estimators=1000, min_samples_leaf=10, max_depth=100, max_features=0.30). ratio of non-contacts/contacts \\(\\in \\{2, 5, 10, 20 \\}\\) within a fixed total dataset size of 300 000 residue pairs window size: \\(\\in \\{5, 7, 9, 11\\}\\) non-contact threshold \\(\\in \\{8, 15, 20\\}\\) As can be seen in appendix E.1 and E.2, the default choice of using a window size of five positions and the non-contact threshold of \\(8 \\angstrom\\) proves to be the optimal setting. Furthermore, using five-times as many non-contacts as contacts in the training set results in highest mean precision as can be seen in appendix E.3. These estimates might be biased in a way since the random forest hyperparameters have been optimized on a dataset using exactly these optimal settings. 8.7.3 Feature Selection Many features obtain low Gini importance scores and can most likely be removed from the data set which will also reduce model complexity. It has been found, that prediction performance might even increase after removing the most irrelevant feaures [140]. For example, during the development of EPSILON-CP, a deep neural network method for contact prediction, the authors performed feature selection using boosted trees. By removing 75% of the most non-informative features (mostly attributed to amino acid composition), the performance of their predictor increased slightly [71]. Other studies have also emphasized the importance of feature selection to improve performance and reduce model complexity [52,54]. I therefore developed a feature selection pipeline that retrains the random forest model on subsets of features. The subsets are composed of those features having Gini importance larger than the \\(\\{10, 30, 50, 70, 90\\}\\)-percentile of the distribution obtained by training a model on all features. Performance is then evaluated by means of the standard contact prediction benchmark (mean precision against top ranked contacts) for the models trained on these subsets of features. 8.7.4 Using Pseudo-likelihood Coevolution Score as Additional Feature In addition to the 250 sequence derived features, the pseudo-likelihood contact score (L2norm + APC) is used as a feature. The random forest was trained on 100.000 residue pairs in contact (\\(\\Delta \\Cb &lt; 8 \\AA \\; \\;\\)) and 500.000 residue pairs not in contact (\\(\\Delta \\Cb &gt; 8 \\AA \\; \\;\\)) using the cross-validated hyperparameters as described in the last section. The pseudo-likelihood contact score comprises by far the most important feature as can be seen in the following Figure 8.16. Figure 8.16: Most important features in the random forest model. Features are ranked according to Gini importance which is the mean decrease in Gini impurity over all splits and all trees in the forest. Training the model only on the 26 most important features improves precision of the model compared to using the full feature set as is illustrated in the following figure 8.16. Figure 8.17: Mean precision over proteins in testset for the top ranked contacts for variaous random forest models trained on subsets of features. Subsets of features have been selected as described in section 8.7.3. Learning a random forest model on the 26 most important features yields the best model with respect to precision. References "],
["abbrev.html", "A Abbreviations", " A Abbreviations APC Avarage Product Correction CASP Critical Assessment of protein Structure Prediction CD Contrastive Divergence DCA Direct Coupling Analysis DI Direct Information EM electron microscopy IDP intrinsically disordered proteins MAP Maximum a posteriori MCMC Markov Chain Monte Carlo MI mutual information ML Maximum-Likelihood MLE Maximum-Likelihood Estimate MRF Markov-Random Field MSA Multiple Sequence Alignment Neff Number of effective sequences PCD Persistent Contrastive Divergence PDB protein data bank SGD stochastic gradient descent "],
["amino-acids.html", "A.1 Amino Acid Alphabet", " A.1 Amino Acid Alphabet One-letter Abbreviation Three-letter Abbreviation Amino Acid One-letter Abbreviation Three-letter Abbreviation Amino Acid A Ala Alanine M Met Methionine C Cys Cysteine N Asn AsparagiNe D Asp Aspartic AciD P Pro Proline E Glu Glutamic Acid Q Gln Glutamine F Phe Phenylalanine R Arg ARginine G Gly Glycine S Ser Serine H His Histidine T Thr Threonine I Ile Isoleucine V Val Valine K Lys Lysine T Trp Tryptophan L Leu Leucine Y Tyr TYrosine -->"],
["dataset-properties.html", "B Dataset Properties", " B Dataset Properties The following figures display various statistics about the dataset used throughout this thesis. See section 8.1 for information on how this dataset has been generated. "],
["alignment-diversity.html", "B.1 Alignment Diversity", " B.1 Alignment Diversity Figure B.1: Distribution of alignment diversity (\\(=\\sqrt(\\frac{N}{L})\\)) in the dataset an its ten subsets. "],
["proportion-of-gaps-in-alignment.html", "B.2 Proportion of Gaps in Alignment", " B.2 Proportion of Gaps in Alignment Figure B.2: Distribution of gap percentage of alignments in the dataset an its ten subsets. "],
["alignment-size-number-of-sequences.html", "B.3 Alignment Size (number of sequences)", " B.3 Alignment Size (number of sequences) Figure B.3: Distribution of alignment size (number of sequences N) in the dataset an its ten subsets. "],
["protein-length.html", "B.4 Protein Length", " B.4 Protein Length Figure B.4: Distribution of protein length L in the dataset an its ten subsets. -->"],
["amino-acid-interaction-preferences-reflected-in-coupling-matrices.html", "C Amino Acid Interaction Preferences Reflected in Coupling Matrices ", " C Amino Acid Interaction Preferences Reflected in Coupling Matrices "],
["pi-cation.html", "C.1 Pi-Cation interactions", " C.1 Pi-Cation interactions Figure C.1 shows a Tyrosine and a Lysine residue forming a cation-\\(\\pi\\) interaction in protein 2ayd. The corresponding coupling matrix in figure C.2 reflects the strong interaction preference. Figure C.1: Tyrosing (residue 37) and Lysine (residue 48) forming a cation-\\(\\pi\\) interaction in protein 2ayd. Figure C.2: Coupling Matrix for residue pair i=37 and j=48 of PDB 2ayd chain A domain 1. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis represent single potentials for residue i=37 and bars at the y-axis represent single potentials for residue j=48. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values. "],
["disulfide.html", "C.2 Disulfide Bonds", " C.2 Disulfide Bonds Figure C.3 shows two cysteine residues forming a covalent disulfide bond in protein 1alu. The corresponding coupling matrix in figure C.4 reflects the strong interaction preference of cysteines. Figure C.3: Two cystein residues (residues 54 and 64) forming a covalent disulfide bond in protein 1alu. Figure C.4: Coupling Matrix for residue pair i=54 and j=64 of PDB 1alu chain A. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis represent single potentials for residue i=54 and bars at the y-axis represent single potentials for residue j=64. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values. "],
["aromatic-proline.html", "C.3 Aromatic-Proline Interactions", " C.3 Aromatic-Proline Interactions Figure @ref(fig:coupling-matrix-aromatic-proline-pymol )shows a proline and a tryptophan residue forming such a CH/\\(\\pi\\) interaction in protein 1aol. The corresponding coupling matrix in figure C.6 reflects this interaction with strong positive coupling between proline and tryptophan. Figure C.5: Proline and tryptophan (residues 17 and 34) stacked on top of each otherengaging in a CH/\\(\\pi\\) interaction in protein 1alu. Figure C.6: Coupling Matrix for residue pair i=17 and j=34 of PDB 1aol chain A. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis represent single potentials for residue i=17 and bars at the y-axis represent single potentials for residue j=34. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values. "],
["aromatic-network.html", "C.4 Network-like structure of aromatic residues", " C.4 Network-like structure of aromatic residues Figure C.7: Network-like structure of aromatic residues in the protein core. 80% of aromatic residues are involved in such networks that are important for protein stability [14]. --> References "],
["optimizing-full-likelihood-with-gradient-descent.html", "D Optimizing Full Likelihood with Gradient Descent ", " D Optimizing Full Likelihood with Gradient Descent "],
["number-of-iterations-for-different-learning-rates.html", "D.1 Number of iterations for different learning rates", " D.1 Number of iterations for different learning rates Figure D.1: Distribution of the number of iterations until convergence for gradient descent optimizations of the full likelihood using different learning rates. The learning rate is decreased according to \\(\\alpha = \\alpha_0 / (1 + 0.01 \\cdot t)\\) with t being the iteration number and the maximum number of iterations is set to 5000. cd_alpha-1e-4: using an initial learning rate of 1e-4. cd_alpha-5e-4: using an initial learning rate of 5e-4. cd_alpha-1e-3: using an initial learning rate of 1e-3. cd_alpha-5e-3: using an initial learning rate of 5e-3. "],
["learning-rate-schedules-distribution-iterations.html", "D.2 Number of iterations for different learning rate schedules and fixed initial learning rate \\(\\alpha_0 =\\) 1e-4", " D.2 Number of iterations for different learning rate schedules and fixed initial learning rate \\(\\alpha_0 =\\) 1e-4 (ref:caption-full-likelihood-opt-numit_lin_learning-rate-schedule) Distribution of the number of iterations until convergence for gradient descent optimizations of the full likelihood using different decay rates with a default learning rate schedule. Initial learning rate \\(\\alpha_0\\) is fixed to 1e-4 and maximum number of iterations is set to 5000. The learning rate is decreased according to \\(\\alpha = \\alpha_0 / (1 + \\gamma \\cdot t)\\) with t being the iteration number and \\(\\gamma\\) the decay rate and its value is given after the underscore in the legend names. Figure D.2: (ref:caption-full-likelihood-opt-numit-lin-learning-rate-schedule) (ref:caption-full-likelihood-opt-numit_sig_learning-rate-schedule) Distribution of the number of iterations until convergence for gradient descent optimizations of the full likelihood using different decay rates with a sigmoidal learning rate schedule. Initial learning rate \\(\\alpha_0\\) is fixed to 1e-4 and maximum number of iterations is set to 5000. The learning rate is decreased according to \\(\\alpha_{t+1} = \\alpha_{t} / (1 + \\gamma \\cdot t)\\) with t being the iteration number and \\(\\gamma\\) the decay rate and its value is given after the underscore in the legend names. Figure D.3: (ref:caption-full-likelihood-opt-numit-sig-learning-rate-schedule) (ref:caption-full-likelihood-opt-numit_sqrt_learning-rate-schedule) Distribution of the number of iterations until convergence for gradient descent optimizations of the full likelihood using different decay rates with a square root learning rate schedule. Initial learning rate \\(\\alpha_0\\) is fixed to 1e-4 and maximum number of iterations is set to 5000. The learning rate is decreased according to \\(\\alpha_{t+1} = \\alpha_{t} / (1 + \\gamma \\cdot t)\\) with t being the iteration number and \\(\\gamma\\) the decay rate and its value is given after the underscore in the legend names. Figure D.4: (ref:caption-full-likelihood-opt-numit-sqrt-learning-rate-schedule) -->"],
["training-of-the-random-forest-contact-prior.html", "E Training of the Random Forest Contact Prior ", " E Training of the Random Forest Contact Prior "],
["rf-window-size.html", "E.1 Evaluating window size with 5-fold Cross-validation", " E.1 Evaluating window size with 5-fold Cross-validation Figure E.1: Mean precision over validation set of 200 proteins for top ranked contact predictions for different choices of window size for single position features. Dashed lines represent the models trained on four subsets of the training data according to the 5-fold cross-validation scheme. Solid lines represent the mean over the five cross-validation models. "],
["rf-noncontact-threshold.html", "E.2 Evaluating non-contact threshold with 5-fold Cross-validation", " E.2 Evaluating non-contact threshold with 5-fold Cross-validation Figure E.2: Mean precision over validation set of 200 proteins for top ranked contact predictions for different choices of the non-contact threshold to define non-contacts. Dashed lines represent the models trained on four subsets of the training data according to the 5-fold cross-validation scheme. Solid lines represent the mean over the five cross-validation models. "],
["rf-ratio-noncontacts.html", "E.3 Evaluating ratio of non-contacts and contacts in the training set with 5-fold Cross-validation", " E.3 Evaluating ratio of non-contacts and contacts in the training set with 5-fold Cross-validation Figure E.3: Mean precision over validation set of 200 proteins for top ranked contact predictions for different choices of dataset composition with respect to the ratio of contacts and non-contacts. Dashed lines represent the models trained on four subsets of the training data according to the 5-fold cross-validation scheme. Solid lines represent the mean over the five cross-validation models. -->"],
["references.html", "References", " References "]
]
