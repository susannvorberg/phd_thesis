<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>PhD thesis: residue-residue contact prediction</title>
  <meta name="description" content="This is my PhD thesis on residue-residue contact prediction.">
  <meta name="generator" content="bookdown 0.3.9 and GitBook 2.6.7">

  <meta property="og:title" content="PhD thesis: residue-residue contact prediction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my PhD thesis on residue-residue contact prediction." />
  <meta name="github-repo" content="susannvorberg/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PhD thesis: residue-residue contact prediction" />
  
  <meta name="twitter:description" content="This is my PhD thesis on residue-residue contact prediction." />
  

<meta name="author" content="Susann Vorberg">


<meta name="date" content="2017-10-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="training-on-couplings-from-pseudo-likelihood-maximization.html">
<link rel="next" href="practical-methods.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
  tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
  TeX: { 
    extensions: ["mediawiki-texvc.js", "sinuitx.js"],
    Macros: {
      Cb: "C_\\beta",
      eq: "\\!=\\!",
      Gauss: "\\mathcal{N}",
      H: "\\mathbf{H}",
      Hij : "\\H_{ij}",
      I: "\\mathbf{I}",
      Lijk: "\\mathbf{\\Lambda}_{ij,k}",
      Lk: "\\mathbf{\\Lambda}_k",
      LL: "L\\!L(\\mathbf{v}, \\mathbf{w})",
      LLreg: "L\\!L_\\mathrm{reg}",
      muijk: "\\mathbf{\\mu}_{ij,k}",
      muk: "\\mathbf{\\mu}_k",
      neff: "N_\\mathrm{eff}",
      r: "\\mathbf{r}",
      rij: "r_{ij}",
      c: "\\mathbf{c}",
      cij: "c_{ij}",
      seq: "\\mathbf{x}",
      Qij: "\\mathbf{Q}_{ij}",
      q: "\\mathbf{q}",
      qij: "\\mathbf{q'}_{ij}",
      Sn: "\\mathcal{S}_n",
      v: "\\mathbf{v}",
      vi: "\\mathcal{v}_{i}",
      vj: "\\mathcal{v}_{j}",
      via: "\\mathcal{v}_{ia}",
      vja: "\\mathcal{v}_{ja}",
      w: "\\mathbf{w}",
      wij: "\\mathbf{w}_{ij}",
      wijab: "\\mathcal{w}_{ijab}",
      wijcd: "\\mathcal{w}_{ijcd}",
      wklcd: "\\mathcal{w}_{klcd}",
      X: "\\mathbf{X}",
      angstrom: "\\AA \\; \\;"
      }
  }
});
</script>


 
<!--
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js", "[siunitx]/siunitx.js"],
  TeX: { TagSide: "left" }
});
MathJax.Ajax.config.path['siunitx']  = '../latex/MathJax-siunitx-master/';
</script>
//-->



<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD Thesis Susann Vorberg</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="general-intro.html"><a href="general-intro.html"><i class="fa fa-check"></i><b>1</b> Biological Background</a></li>
<li class="chapter" data-level="2" data-path="introduction-to-contact-prediction.html"><a href="introduction-to-contact-prediction.html"><i class="fa fa-check"></i><b>2</b> Introduction to Contact Prediction</a><ul>
<li class="chapter" data-level="2.1" data-path="local-methods.html"><a href="local-methods.html"><i class="fa fa-check"></i><b>2.1</b> Local Statistical Models</a></li>
<li class="chapter" data-level="2.2" data-path="global-methods.html"><a href="global-methods.html"><i class="fa fa-check"></i><b>2.2</b> Global Statistical Models</a></li>
<li class="chapter" data-level="2.3" data-path="meta-predictors.html"><a href="meta-predictors.html"><i class="fa fa-check"></i><b>2.3</b> Machine Learning Methods and Meta-Predictors</a></li>
<li class="chapter" data-level="2.4" data-path="maxent.html"><a href="maxent.html"><i class="fa fa-check"></i><b>2.4</b> Modelling Protein Families with Potts Model</a><ul>
<li class="chapter" data-level="2.4.1" data-path="maxent.html"><a href="maxent.html#potts-model-properties"><i class="fa fa-check"></i><b>2.4.1</b> Model Properties</a></li>
<li class="chapter" data-level="2.4.2" data-path="maxent.html"><a href="maxent.html#potts-mle"><i class="fa fa-check"></i><b>2.4.2</b> Inferring Parameters for the Potts Model</a></li>
<li class="chapter" data-level="2.4.3" data-path="maxent.html"><a href="maxent.html#potts-model-solutions"><i class="fa fa-check"></i><b>2.4.3</b> Solving the Inverse Potts Problem</a></li>
<li class="chapter" data-level="2.4.4" data-path="maxent.html"><a href="maxent.html#post-processing-heuristics"><i class="fa fa-check"></i><b>2.4.4</b> Computing Contact Maps</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="application-contact-prediction.html"><a href="application-contact-prediction.html"><i class="fa fa-check"></i><b>2.5</b> Applications</a></li>
<li class="chapter" data-level="2.6" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html"><i class="fa fa-check"></i><b>2.6</b> Evaluating Contact Prediction Methods</a><ul>
<li class="chapter" data-level="2.6.1" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#seq-sep"><i class="fa fa-check"></i><b>2.6.1</b> Sequence Separation</a></li>
<li class="chapter" data-level="2.6.2" data-path="intro-cp-evaluation.html"><a href="intro-cp-evaluation.html#interpretation-of-evaluation-results"><i class="fa fa-check"></i><b>2.6.2</b> Interpretation of Evaluation Results</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>2.7</b> Challenges for Coevolutionary Inference</a><ul>
<li class="chapter" data-level="2.7.1" data-path="challenges.html"><a href="challenges.html#phylogenetic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>2.7.1</b> Phylogenetic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="2.7.2" data-path="challenges.html"><a href="challenges.html#entropic-effects-as-a-source-of-noise"><i class="fa fa-check"></i><b>2.7.2</b> Entropic Effects as a Source of Noise</a></li>
<li class="chapter" data-level="2.7.3" data-path="challenges.html"><a href="challenges.html#finite-sampling-effects"><i class="fa fa-check"></i><b>2.7.3</b> Finite Sampling Effects</a></li>
<li class="chapter" data-level="2.7.4" data-path="challenges.html"><a href="challenges.html#multiple-sequence-alignments"><i class="fa fa-check"></i><b>2.7.4</b> Multiple Sequence Alignments</a></li>
<li class="chapter" data-level="2.7.5" data-path="challenges.html"><a href="challenges.html#alternative-sources-of-coevolution"><i class="fa fa-check"></i><b>2.7.5</b> Alternative Sources of Coevolution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpreting-coupling-matrices.html"><a href="interpreting-coupling-matrices.html"><i class="fa fa-check"></i><b>3</b> Interpretation of Coupling Matrices</a><ul>
<li class="chapter" data-level="3.1" data-path="correlation-between-couplings-and-class.html"><a href="correlation-between-couplings-and-class.html"><i class="fa fa-check"></i><b>3.1</b> Single Coupling Values Carry Evidence of Contacts</a></li>
<li class="chapter" data-level="3.2" data-path="physico-chemical-fingerprints-in-coupling-matrices.html"><a href="physico-chemical-fingerprints-in-coupling-matrices.html"><i class="fa fa-check"></i><b>3.2</b> Physico-Chemical Fingerprints in Coupling Matrices</a></li>
<li class="chapter" data-level="3.3" data-path="coupling-profiles.html"><a href="coupling-profiles.html"><i class="fa fa-check"></i><b>3.3</b> Coupling Profiles Vary with Distance</a></li>
<li class="chapter" data-level="3.4" data-path="higher-order-coupling-profiles.html"><a href="higher-order-coupling-profiles.html"><i class="fa fa-check"></i><b>3.4</b> Higher Order Dependencies Between Couplings</a></li>
<li class="chapter" data-level="3.5" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>3.5</b> Methods</a><ul>
<li class="chapter" data-level="3.5.1" data-path="methods.html"><a href="methods.html#method-coupling-correlation"><i class="fa fa-check"></i><b>3.5.1</b> Correlation of Couplings with Contact Class</a></li>
<li class="chapter" data-level="3.5.2" data-path="methods.html"><a href="methods.html#method-coupling-profile"><i class="fa fa-check"></i><b>3.5.2</b> Coupling Distribution Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="optimizing-full-likelihood.html"><a href="optimizing-full-likelihood.html"><i class="fa fa-check"></i><b>4</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="4.1" data-path="full-likelihood-gradient.html"><a href="full-likelihood-gradient.html"><i class="fa fa-check"></i><b>4.1</b> Approximating the Gradient of the Full Likelihood with Contrastive Divergence</a></li>
<li class="chapter" data-level="4.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html"><i class="fa fa-check"></i><b>4.2</b> Optimizing the Full Likelihood</a><ul>
<li class="chapter" data-level="4.2.1" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#convergence-criteria-sgd"><i class="fa fa-check"></i><b>4.2.1</b> Convergence Criterion for Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="4.2.2" data-path="full-likelihood-optimization.html"><a href="full-likelihood-optimization.html#sgd-hyperparameter-tuning"><i class="fa fa-check"></i><b>4.2.2</b> Tuning Hyperparameters of Stochastic Gradient Descent Optimizer</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html"><i class="fa fa-check"></i><b>4.3</b> Tuning the Gibbs Sampling Scheme for Contrastive Divergence</a><ul>
<li class="chapter" data-level="4.3.1" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#regularization-for-cd-with-sgd"><i class="fa fa-check"></i><b>4.3.1</b> Tuning Regularization Coefficients for Contrastive Divergence</a></li>
<li class="chapter" data-level="4.3.2" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-sampling-size"><i class="fa fa-check"></i><b>4.3.2</b> Varying the Sample Size</a></li>
<li class="chapter" data-level="4.3.3" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>4.3.3</b> Varying the number of Gibbs Steps</a></li>
<li class="chapter" data-level="4.3.4" data-path="cd-sampling-optimization.html"><a href="cd-sampling-optimization.html#cd-gibbs-steps"><i class="fa fa-check"></i><b>4.3.4</b> Persistent Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="adam-results.html"><a href="adam-results.html"><i class="fa fa-check"></i><b>4.4</b> Using ADAM to optimize Contrastive Divergence</a></li>
<li class="chapter" data-level="4.5" data-path="comparing-pll-cd.html"><a href="comparing-pll-cd.html"><i class="fa fa-check"></i><b>4.5</b> Comparing CD couplings to pLL couplings</a><ul>
<li class="chapter" data-level="4.5.1" data-path="comparing-pll-cd.html"><a href="comparing-pll-cd.html#protein-1c75a00"><i class="fa fa-check"></i><b>4.5.1</b> Protein 1c75A00</a></li>
<li class="chapter" data-level="4.5.2" data-path="comparing-pll-cd.html"><a href="comparing-pll-cd.html#protein-1ss3a00-and-1c55a00"><i class="fa fa-check"></i><b>4.5.2</b> Protein 1ss3A00 and 1c55A00</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="theoretical-methods.html"><a href="theoretical-methods.html"><i class="fa fa-check"></i><b>4.6</b> Theoretical Methods</a><ul>
<li class="chapter" data-level="4.6.1" data-path="theoretical-methods.html"><a href="theoretical-methods.html#potts-full-likelihood"><i class="fa fa-check"></i><b>4.6.1</b> The Potts Model</a></li>
<li class="chapter" data-level="4.6.2" data-path="theoretical-methods.html"><a href="theoretical-methods.html#gap-treatment"><i class="fa fa-check"></i><b>4.6.2</b> Treating Gaps as Missing Information</a></li>
<li class="chapter" data-level="4.6.3" data-path="theoretical-methods.html"><a href="theoretical-methods.html#the-regularized-full-log-likelihood-and-its-gradient-with-gap-treatment"><i class="fa fa-check"></i><b>4.6.3</b> The Regularized Full Log Likelihood and its Gradient With Gap Treatment</a></li>
<li class="chapter" data-level="4.6.4" data-path="theoretical-methods.html"><a href="theoretical-methods.html#prior-v"><i class="fa fa-check"></i><b>4.6.4</b> The prior on <span class="math inline">\(\v\)</span></a></li>
<li class="chapter" data-level="4.6.5" data-path="theoretical-methods.html"><a href="theoretical-methods.html#methods-sgd"><i class="fa fa-check"></i><b>4.6.5</b> Practical Methods</a></li>
<li class="chapter" data-level="4.6.6" data-path="theoretical-methods.html"><a href="theoretical-methods.html#methods-full-likelihood-adam"><i class="fa fa-check"></i><b>4.6.6</b> Tuning Hyperparameters of <em>ADAM</em> Optimizer</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="methods-cd-sampling.html"><a href="methods-cd-sampling.html"><i class="fa fa-check"></i><b>4.7</b> Computing the Gradient with Contrastive Divergence</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="contact-prior.html"><a href="contact-prior.html"><i class="fa fa-check"></i><b>5</b> Random Forest Contact Prior</a><ul>
<li class="chapter" data-level="5.1" data-path="random-forest-classifiers.html"><a href="random-forest-classifiers.html"><i class="fa fa-check"></i><b>5.1</b> Random Forest Classifiers</a></li>
<li class="chapter" data-level="5.2" data-path="hyperparameter-optimization-for-random-forest.html"><a href="hyperparameter-optimization-for-random-forest.html"><i class="fa fa-check"></i><b>5.2</b> Hyperparameter Optimization for Random Forest</a></li>
<li class="chapter" data-level="5.3" data-path="evaluating-random-forest-model-as-contact-predictor.html"><a href="evaluating-random-forest-model-as-contact-predictor.html"><i class="fa fa-check"></i><b>5.3</b> Evaluating Random Forest Model as Contact Predictor</a></li>
<li class="chapter" data-level="5.4" data-path="using-contact-scores-as-additional-features.html"><a href="using-contact-scores-as-additional-features.html"><i class="fa fa-check"></i><b>5.4</b> Using Contact Scores as Additional Features</a></li>
<li class="chapter" data-level="5.5" data-path="methods-1.html"><a href="methods-1.html"><i class="fa fa-check"></i><b>5.5</b> Methods</a><ul>
<li class="chapter" data-level="5.5.1" data-path="methods-1.html"><a href="methods-1.html#seq-features"><i class="fa fa-check"></i><b>5.5.1</b> Features used to train Random Forest Model</a></li>
<li class="chapter" data-level="5.5.2" data-path="methods-1.html"><a href="methods-1.html#seq-features-global"><i class="fa fa-check"></i><b>5.5.2</b> Global Features</a></li>
<li class="chapter" data-level="5.5.3" data-path="methods-1.html"><a href="methods-1.html#seq-features-single"><i class="fa fa-check"></i><b>5.5.3</b> Single Position Features</a></li>
<li class="chapter" data-level="5.5.4" data-path="methods-1.html"><a href="methods-1.html#seq-features-pairwise"><i class="fa fa-check"></i><b>5.5.4</b> Pairwise Features</a></li>
<li class="chapter" data-level="5.5.5" data-path="methods-1.html"><a href="methods-1.html#rf-training"><i class="fa fa-check"></i><b>5.5.5</b> Training Random Forest Contact Prior</a></li>
<li class="chapter" data-level="5.5.6" data-path="methods-1.html"><a href="methods-1.html#rf-feature-selection"><i class="fa fa-check"></i><b>5.5.6</b> Feature Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayesian-approach.html"><a href="bayesian-approach.html"><i class="fa fa-check"></i><b>6</b> A Bayesian Statistical Model for Residue-Residue Contact Prediction</a><ul>
<li class="chapter" data-level="6.1" data-path="overview-posterior-distances.html"><a href="overview-posterior-distances.html"><i class="fa fa-check"></i><b>6.1</b> Computing the Posterior Probabiilty of a Contact</a></li>
<li class="chapter" data-level="6.2" data-path="coupling-prior.html"><a href="coupling-prior.html"><i class="fa fa-check"></i><b>6.2</b> Modelling the Prior Over Couplings Depending on Contact States</a></li>
<li class="chapter" data-level="6.3" data-path="bayesian-model-training-hyperparameters.html"><a href="bayesian-model-training-hyperparameters.html"><i class="fa fa-check"></i><b>6.3</b> Training the Hyperparameters in the Likelihood Function of Contact States</a></li>
<li class="chapter" data-level="6.4" data-path="training-on-couplings-from-pseudo-likelihood-maximization.html"><a href="training-on-couplings-from-pseudo-likelihood-maximization.html"><i class="fa fa-check"></i><b>6.4</b> Training on couplings from pseudo-likelihood maximization</a></li>
<li class="chapter" data-level="6.5" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html"><i class="fa fa-check"></i><b>6.5</b> Theoretical Methods</a><ul>
<li class="chapter" data-level="6.5.1" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#methods-coupling-prior"><i class="fa fa-check"></i><b>6.5.1</b> Modelling the Prior Over Couplings Depending on Contact States</a></li>
<li class="chapter" data-level="6.5.2" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#laplace-approx"><i class="fa fa-check"></i><b>6.5.2</b> Gaussian Approximation to the Posterior of Couplings</a></li>
<li class="chapter" data-level="6.5.3" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#likelihood-fct-distances"><i class="fa fa-check"></i><b>6.5.3</b> Integrating out the Hidden Variables to Obtain the Likelihood Function of the Contact States</a></li>
<li class="chapter" data-level="6.5.4" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#posterior-of-rij"><i class="fa fa-check"></i><b>6.5.4</b> Computing The Posterior Probability of Contacts</a></li>
<li class="chapter" data-level="6.5.5" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#Hessian-offdiagonal"><i class="fa fa-check"></i><b>6.5.5</b> The Hessian off-diagonal Elements Carry a Negligible Signal</a></li>
<li class="chapter" data-level="6.5.6" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#neg-Hessian-computation"><i class="fa fa-check"></i><b>6.5.6</b> Efficiently Computing the negative Hessian of the regularized log-likelihood</a></li>
<li class="chapter" data-level="6.5.7" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#inv-lambda-ij-k"><i class="fa fa-check"></i><b>6.5.7</b> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></a></li>
<li class="chapter" data-level="6.5.8" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#gradient-muk"><i class="fa fa-check"></i><b>6.5.8</b> The gradient of the log likelihood with respect to <span class="math inline">\(\muk\)</span></a></li>
<li class="chapter" data-level="6.5.9" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#gradient-lambdak"><i class="fa fa-check"></i><b>6.5.9</b> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></a></li>
<li class="chapter" data-level="6.5.10" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#the-gradient-of-the-log-likelihood-with-respect-to-gamma_k"><i class="fa fa-check"></i><b>6.5.10</b> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></a></li>
<li class="chapter" data-level="6.5.11" data-path="theoretical-methods-1.html"><a href="theoretical-methods-1.html#extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances"><i class="fa fa-check"></i><b>6.5.11</b> Extending the Bayesian Statistical Model for the Prediction of Protein Residue-Residue Distances</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="practical-methods.html"><a href="practical-methods.html"><i class="fa fa-check"></i><b>6.6</b> Practical Methods</a><ul>
<li class="chapter" data-level="6.6.1" data-path="practical-methods.html"><a href="practical-methods.html#training-hyperparameters"><i class="fa fa-check"></i><b>6.6.1</b> Training the Hyperparameters in the Likelihood Function of Contact States</a></li>
<li class="chapter" data-level="6.6.2" data-path="practical-methods.html"><a href="practical-methods.html#dataset-training-hyperparmeters"><i class="fa fa-check"></i><b>6.6.2</b> Dataset Specifications</a></li>
<li class="chapter" data-level="6.6.3" data-path="practical-methods.html"><a href="practical-methods.html#model-specifications-training-hyperparmeters"><i class="fa fa-check"></i><b>6.6.3</b> Model Specifications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>7</b> Conclusion</a></li>
<li class="chapter" data-level="8" data-path="general-methods.html"><a href="general-methods.html"><i class="fa fa-check"></i><b>8</b> General Methods</a><ul>
<li class="chapter" data-level="8.1" data-path="dataset.html"><a href="dataset.html"><i class="fa fa-check"></i><b>8.1</b> Dataset</a></li>
<li class="chapter" data-level="8.2" data-path="computing-pseudo-likelihood-couplings.html"><a href="computing-pseudo-likelihood-couplings.html"><i class="fa fa-check"></i><b>8.2</b> Computing Pseudo-Likelihood Couplings</a><ul>
<li class="chapter" data-level="8.2.1" data-path="computing-pseudo-likelihood-couplings.html"><a href="computing-pseudo-likelihood-couplings.html#diff-ccmpred-ccmpredpy"><i class="fa fa-check"></i><b>8.2.1</b> Differences between CCMpred and CCMpredpy</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="seq-reweighting.html"><a href="seq-reweighting.html"><i class="fa fa-check"></i><b>8.3</b> Sequence Reweighting</a></li>
<li class="chapter" data-level="8.4" data-path="amino-acid-frequencies.html"><a href="amino-acid-frequencies.html"><i class="fa fa-check"></i><b>8.4</b> Computing Amino Acid Frequencies</a></li>
<li class="chapter" data-level="8.5" data-path="methods-regularization.html"><a href="methods-regularization.html"><i class="fa fa-check"></i><b>8.5</b> Regularization</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="abbrev.html"><a href="abbrev.html"><i class="fa fa-check"></i><b>A</b> Abbreviations</a></li>
<li class="chapter" data-level="B" data-path="amino-acids.html"><a href="amino-acids.html"><i class="fa fa-check"></i><b>B</b> Amino Acid Alphabet</a></li>
<li class="chapter" data-level="C" data-path="dataset-properties.html"><a href="dataset-properties.html"><i class="fa fa-check"></i><b>C</b> Dataset Properties</a></li>
<li class="chapter" data-level="D" data-path="interpretation-of-coupling-matrices.html"><a href="interpretation-of-coupling-matrices.html"><i class="fa fa-check"></i><b>D</b> Interpretation of Coupling Matrices</a></li>
<li class="chapter" data-level="E" data-path="optimizing-full-likelihood-with-gradient-descent.html"><a href="optimizing-full-likelihood-with-gradient-descent.html"><i class="fa fa-check"></i><b>E</b> Optimizing Full Likelihood with Gradient Descent</a></li>
<li class="chapter" data-level="F" data-path="training-of-the-random-forest-contact-prior.html"><a href="training-of-the-random-forest-contact-prior.html"><i class="fa fa-check"></i><b>F</b> Training of the Random Forest Contact Prior</a></li>
<li class="chapter" data-level="G" data-path="bayesian-statistical-model-for-contact-prediction.html"><a href="bayesian-statistical-model-for-contact-prediction.html"><i class="fa fa-check"></i><b>G</b> Bayesian statistical model for contact prediction</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PhD thesis: residue-residue contact prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="theoretical-methods-1" class="section level2">
<h2><span class="header-section-number">6.5</span> Theoretical Methods</h2>
<div id="methods-coupling-prior" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Modelling the Prior Over Couplings Depending on Contact States</h3>
<p>The mixture weights <span class="math inline">\(g_k(\cij)\)</span> in eq. <a href="coupling-prior.html#eq:definition-mixture-coupling-prior">(6.4)</a> are modelled as softmax:</p>
<span class="math display" id="eq:def-g-k-binary">\[\begin{equation}
    g_k(\cij) = \frac{\exp \gamma_k(\cij)}{\sum_{k&#39;=0}^K \exp \gamma_{k&#39;}(\cij)} 
\tag{6.5}
\end{equation}\]</span>
<p>The functions <span class="math inline">\(g_k(\cij)\)</span> remain invariant when adding an offset to all <span class="math inline">\(\gamma_k(\cij)\)</span>. This degeneracy can be removed by setting <span class="math inline">\(\gamma_0(\cij)=1\)</span>.</p>
</div>
<div id="laplace-approx" class="section level3">
<h3><span class="header-section-number">6.5.2</span> Gaussian Approximation to the Posterior of Couplings</h3>
<p>From sampling experiments done by Markus Gruber we know that the regularized pseudo-log-likelihood for realistic examples of protein MSAs obeys the equipartition theorem. The equipartition theorem states that in a harmonic potential (where third and higher order derivatives around the energy minimum vanish) the mean potential energy per degree of freedom (i.e. per eigendirection of the Hessian of the potential) is equal to <span class="math inline">\(k_B T/2\)</span>, which is of course equal to the mean kinetic energy per degree of freedom. Hence we have a strong indication that in realistic examples the pseudo log likelihood is well approximated by a harmonic potential. We assume here that this will also be true for the regularized log likelihood.</p>
<p>The posterior distribution of couplings <span class="math inline">\(\w\)</span> is given by</p>
<span class="math display">\[\begin{equation}
p(\w | \X , \v^*) = p(\X | \v^*, \w) \Gauss (\w | \mathbf{0}, \lambda_w^{-1} \I)
\end{equation}\]</span>
<p>where the single potentials <span class="math inline">\(\v\)</span> are set to the target vector <span class="math inline">\(\v^*\)</span> as discussed in section <a href="overview-posterior-distances.html#overview-posterior-distances">6.1</a>.</p>
<p>The posterior distribution can be approximated with a so called “Laplace Approximation”<span class="citation">[<a href="#ref-Murphy2012">94</a>]</span>: by performing a second order Taylor expansion around the mode <span class="math inline">\(\w^*\)</span> of the log posterior it can be written as</p>
<span class="math display">\[\begin{align}
    \log p(\w | \X , \v^*) \overset{!}{\approx} &amp;  \;  \log p(\w^* | \X , \v^*) \nonumber\\
                &amp; + \nabla_\w \log p(\w | \X , \v^*)|_{\w^*}(\w-\w^*) \nonumber\\ 
                &amp; - \frac{1}{2} (\w-\w^*)^{\mathrm{T}} \H (\w-\w^*)  \; .
\end{align}\]</span>
<p>where <span class="math inline">\(\H\)</span> signifies the <em>negative</em> Hessian matrix with respect to the components of <span class="math inline">\(\w\)</span>,</p>
<span class="math display">\[\begin{equation}
    (\H)_{klcd, ijab} = - \left. \frac{\partial^2  \log p(\w | \X , \v^{*})}{\partial \w_{klcd} \, \partial \wijab  } \right|_{(\w^{*})} \; .
\end{equation}\]</span>
<p>The mode <span class="math inline">\(\w^*\)</span> will be determined with the <a href="abbrev.html#abbrev">CD</a> approach described in detail in section <a href="optimizing-full-likelihood.html#optimizing-full-likelihood">4</a>. Since the gradient vanishes at the mode maximum, <span class="math inline">\(\nabla_\w \log p(\w | \X , \v^*)|_{\w^*} = 0\)</span>, the second order approximation can be written as</p>
<span class="math display">\[\begin{equation}
    \log p(\w | \X , \v^*) {\approx}  \log p(\w^* | \X , \v^*)  - \frac{1}{2} (\w-\w^*)^{\mathrm{T}} \, \H \, (\w-\w^*)  \;.
\end{equation}\]</span>
<p>Hence, the posterior of couplings can be approximated with a Gaussian</p>
<span class="math display" id="eq:reg-lik-gauss-approx">\[\begin{align}
   p(\w | \X , \v^*) &amp;\approx p(\w^* | \X , \v^*) \exp \left( - \frac{1}{2} (\w-\w^*)^{\mathrm{T}} \H  (\w -\w^*) \right) \nonumber \\
              &amp;= p(\w^* | \X , \v^*) \frac{(2 \pi)^\frac{D}{2}} { |\H|^\frac{D}{2}} \times \Gauss (\w | \w^*, \H^{-1} ) \nonumber \\
              &amp;\propto  \Gauss (\w | \w^*, \H^{-1}) \,,
\tag{6.6}
\end{align}\]</span>
<p>with proportionality constant that depends only on the data and with a precision matrix equal to the negative Hessian matrix. The surprisingly easy computation of the Hessian can be found in Methods section <a href="theoretical-methods-1.html#neg-Hessian-computation">6.5.6</a>.</p>
<!--
### Iterative improvement of Laplace approximation {#laplace-approx-improvement}

The quality of the Gaussian approximation to the posterior distribution of couplings $p(\w | \X , \v^*)$ depends on two points,

1. how well is the posterior distribution of couplings approximated by a Gaussian 
2. how closely does the mode of the  posterior distribution of couplings lie near the mode of the integrand in equation \@ref(eq:). 

The second point can be addressed quite effectively in the following way. 

(see Murphy page 658 eq. 18.137 and eq 18.138)


Supppose the optimal prior parameters $(\tilde{\muk}, \tilde{\Lk})$ have been trained as described in Methods section \@ref(training-hyperparameters), using the standard isotropic regularisation prior $\Gauss(\w_{ij} | \mathbf{0}, \lambda_w^{-1} \I)$. 
An improved regularisation prior $\Gauss( \wij | \mu(r_{ij}), \mathbf{\Sigma}(r_{ij}))$ can then be selected using the knowledge of the true, optimised prior, by matching the mean and variance of the improved regularisation with those of the true prior from the first optimisation:

\begin{align} 
    \mathbf{\mu}(r_{ij}) &= \operatorname{E}_{p( \wij | \rij, \tilde{\mathbf{\mu}}, \tilde{\Lambda})} \left[  \wij \right]  \\
    &= \int \wij \, p( \wij | \rij, \tilde{\mathbf{\mu}}, \tilde{\Lambda}) d \w  \\
    &= \int \wij \sum_{k=0}^K g_k(\rij) \, \Gauss(\wij | \tilde{\muk}, \tilde{\Lambda}_k^{-1})  d \w \\
    &= \sum_{k=0}^K g_k(\rij) \int \wij  \, \Gauss(\wij | \tilde{\muk}, \tilde{\Lambda}_k^{-1})  d \w  \\
    \mathbf{\mu}(r_{ij}) &= \sum_{k=0}^K g_k(\rij) \, \tilde{\muk}
\end{align}

and similarly,  

\begin{align}
    \mathbf{\Sigma}(r_{ij}) &= \operatorname{var}_{ p(\wij | \rij, \tilde{\mathbf{\mu}}, \tilde{\Lambda} )}  \left[ \wij \right] \\
    &= \int (\wij - \mathbf{\mu}(r_{ij})) (\wij - \mathbf{\mu}(r_{ij}))^\mathrm{T} \, p( \wij | \rij, \tilde{\mathbf{\mu}}, \tilde{\Lambda}) d \w \\
    &= \sum_{k=0}^K g_k(\rij) \int  (\wij - \mathbf{\mu}(r_{ij})) (\wij - \mathbf{\mu}(r_{ij}))^\mathrm{T} \, \Gauss(\wij | \tilde{\muk}, \tilde{\Lk}^{-1}) d \w \\
    &= \sum_{k=0}^K g_k(\rij) \int  (\wij - \mathbf{\mu}(r_{ij}) + \tilde{\muk}) (\wij - \mathbf{\mu}(r_{ij}) + \tilde{\muk})^\mathrm{T} \, \Gauss(\wij | \mathbf{0} , \tilde{\Lk}^{-1}) d \w \\
    \mathbf{\Sigma}(r_{ij}) &= \sum_{k=0}^K g_k(\rij) \left( \tilde{\Lk}^{-1} + (\mathbf{\mu}(r_{ij}) - \tilde{\muk}) (\mathbf{\mu}(r_{ij}) - \tilde{\muk})^\mathrm{T}\right) \,. 
\end{align}

We can now run a second optimisation with better regularisation prior, in which the $\tilde{\mathbf{\mu}}$ and $\tilde{\Lambda}$ are fixed and will not be optimised. Instead we optimise the marginal likelihood as a function of $\muk$ and $\Lk$. Since the new regularisation prior will be very close to the mode of the integrand in the marginal likelihood, our approximation for the second iteration has improved in comparison to the first iteration. In principle, a third iteration can be done in which our regularisation prior derived from the prior that was found by optimisation in the second iteration. However this is unlikely to further improve the predictions. 

-->
</div>
<div id="likelihood-fct-distances" class="section level3">
<h3><span class="header-section-number">6.5.3</span> Integrating out the Hidden Variables to Obtain the Likelihood Function of the Contact States</h3>
<p>In order to compute the likelihood function of the contact states, one needs to solve the integral over <span class="math inline">\((\v, \w)\)</span>,</p>
<span class="math display" id="eq:likelihood-distances">\[\begin{equation}
    p(\X | \c) = \int \int p(\X | \v,\w) \, p(\v, \w | \c) \,d\v\,d\w \; .
\tag{6.7}
\end{equation}\]</span>
<p>Inserting the prior over parameters <span class="math inline">\(p(\v, \w | \c)\)</span> from eq. <a href="overview-posterior-distances.html#eq:definition-parameter-prior">(6.3)</a> into the previous equation and performing the integral over <span class="math inline">\(\v\)</span>, as discussed earlier in section <a href="overview-posterior-distances.html#overview-posterior-distances">6.1</a>, yields</p>
<span class="math display">\[\begin{eqnarray}
    p(\X | \c) &amp;=&amp; \int \left( \int  p(\X | \v,\w) \, \Gauss(\v|\v^*,\lambda_v^{-1} \I) \,d\v \right) \, \prod_{1\le i&lt;j\le L} p(\wij|\cij) \, d\w  \\
    p(\X | \c) &amp;=&amp; \int  p(\X | \v^*,\w) \, \prod_{1\le i&lt;j\le L} p(\wij|\cij) \, d\w  
\label{eq:in_over_w_1}
\end{eqnarray}\]</span>
<p>Next, the likelihood of sequences, <span class="math inline">\(p(\X | \v^*,\w)\)</span>, will be multiplied with the regularisation prior <span class="math inline">\(\Gauss(\w|\mathbf{0}, \lambda_w^{-1} \I)\)</span> and at the same time the coupling prior, which depends on the contact states, will be divided by the regularisation prior again:</p>
<span class="math display">\[\begin{eqnarray}
      p(\X | \c) &amp;=&amp; \int p(\X | \v^*,\w) \, \Gauss(\w|\mathbf{0}, \lambda_w^{-1} \I) \, \prod_{1\le i&lt;j\le L} \frac{p(\wij|\cij)}{\Gauss(\wij|\mathbf{0}, \lambda_w^{-1} \I)} \,d\w \, .
\end{eqnarray}\]</span>
<p>Now the crucial advantage of the likelihood regularisation is borne out: the strength of the regularisation prior, <span class="math inline">\(\lambda_w\)</span>, can be chosen such that the mode <span class="math inline">\(\w^*\)</span> of the regularised likelihood is near to the mode of the integrand in the last integral. The regularisation prior <span class="math inline">\(\Gauss(\wij|\mathbf{0}, \lambda_w^{-1} \I)\)</span> is then a simpler, approximate version of the real coupling prior <span class="math inline">\(\prod_{1\le i&lt;j\le L} p(\wij|\cij)\)</span> that depends on the contact state. This allows to approximate the regularised likelihood with a Gaussian distribution (eq. <a href="theoretical-methods-1.html#eq:reg-lik-gauss-approx">(6.6)</a>), because this approximation will be fairly accurate in the region around its mode, which is near the region around the mode of the integrand and this again is in the region that contributes most to the integral:</p>
<span class="math display" id="eq:int-over-w">\[\begin{eqnarray}
      p(\X | \c) &amp;\propto&amp; \int \Gauss (\w | \w^*, \H^{-1} ) \, \prod_{1 \le i&lt;j \le L} \frac{p(\wij | \cij)}{\Gauss(\wij|\mathbf{0}, \lambda_w^{-1} \I)} d\w \,.
\tag{6.8}
\end{eqnarray}\]</span>
<p>The matrix <span class="math inline">\(\H\)</span> has dimensions <span class="math inline">\((L^2 \times 20^2) \times (L^2 \times 20^2)\)</span>. Computing it is obviously infeasible, even if there was a way to compute <span class="math inline">\(p(x_i \eq a, x_j \eq b| \v^*,\w^*)\)</span> efficiently. In Methods section <a href="theoretical-methods-1.html#Hessian-offdiagonal">6.5.5</a> is shown that in practice, the off-diagonal block matrices with <span class="math inline">\((i,j) \ne (k,l)\)</span> are negligible in comparison to the diagonal block matrices. For the purpose of computing the integral in eq. <a href="theoretical-methods-1.html#eq:int-over-w">(6.8)</a>, it is therefore a good approximation to simply set the off-diagonal block matrices (case 3 in eq. <a href="theoretical-methods-1.html#eq:Hw-offdiag">(6.14)</a>) to zero! The first term in the integrand of eq. <a href="theoretical-methods-1.html#eq:int-over-w">(6.8)</a> now factorizes over <span class="math inline">\((i,j)\)</span>,</p>
<span class="math display">\[\begin{equation}
  \Gauss (\w | \w^{*}, \H^{-1}) \approx \prod_{1 \le i &lt; j \le L} \Gauss (\wij | \wij^{*}, \H_{ij}^{-1}) ,
\end{equation}\]</span>
<p>with the diagonal block matrices <span class="math inline">\((\H_{ij})_{ab,cd} := (\H)_{ijab,ijcd}\)</span>. Now the product over all residue indices can be moved in front of the integral and each integral can be performed over <span class="math inline">\(\wij\)</span> separately,</p>
<span class="math display" id="eq:int-over-w-2">\[\begin{eqnarray}
  p(\X | \c) &amp;\propto&amp; \int \prod_{1 \le i &lt; j \le L} \Gauss (\wij | \wij^{*}, \H_{ij}^{-1}) \prod_{1 \le i&lt;j \le L} \frac{p(\wij | \cij)}{\Gauss(\wij|\mathbf{0}, \lambda_w^{-1} \I)} d\w  \\
  p(\X | \c) &amp;\propto&amp; \int \prod_{1\le i&lt;j\le L} \left(  \Gauss (\wij | \wij^*, \H_{ij}^{-1}) \, \frac{p(\wij | \cij)}{\Gauss(\wij | \mathbf{0}, \lambda_w^{-1} \I)} \right) d\w \\
  p(\X | \c) &amp;\propto&amp; \prod_{1\le i&lt;j\le L}  \int \Gauss (\wij | \wij^*, \H_{ij}^{-1}) \frac{p(\wij | \cij)}{\Gauss (\wij | \mathbf{0}, \lambda_w^{-1} \I)} d \wij 
\tag{6.9}
\end{eqnarray}\]</span>
<p>Inserting the coupling prior defined in eq. <a href="coupling-prior.html#eq:definition-mixture-coupling-prior">(6.4)</a> yields</p>
<span class="math display" id="eq:int-over-w-3">\[\begin{eqnarray}
   p(\X | \c) &amp;\propto&amp; \prod_{1\le i&lt;j\le L} \int \Gauss (\wij | \wij^*, \H_{ij}^{-1}) \frac{\sum_{k=0}^K g_{k}(\cij) \Gauss(\wij | \muk, \Lk^{-1})}{\Gauss (\wij | \mathbf{0}, \lambda_w^{-1} \I)} d \wij \\
   p(\X | \c) &amp;\propto&amp; \prod_{1\le i&lt;j\le L} \sum_{k=0}^K g_{k}(\cij) \int \frac{\Gauss (\wij | \wij^*, \H_{ij}^{-1})}{\Gauss (\wij | \mathbf{0}, \lambda_w^{-1} \I)} \Gauss(\wij | \muk, \Lk^{-1}) d\wij \; .
\tag{6.10}
\end{eqnarray}\]</span>
The integral can be carried out using the following formula:
<span class="math display">\[\begin{equation}
    \int d\seq \, \frac{ \Gauss( \seq | \mathbf{\mu}_1, \mathbf{\Lambda}_1^{-1}) }{\Gauss(\seq|\mathbf{0},\mathbf{\Lambda}3^{-1})} \, \Gauss(\seq|\mathbf{\mu}_2,\mathbf{\Lambda}_2^{-1}) = \\
    \frac{\Gauss(\mathbf{0}| \mathbf{\mu}_1, \mathbf{\Lambda}_{1}^{-1}) \Gauss(\mathbf{0}| \mathbf{\mu}_2, \mathbf{\Lambda}_{2}^{-1})}{\Gauss(\mathbf{0}|\mathbf{0}, \mathbf{\Lambda}_{3}^{-1}) \Gauss(\mathbf{0}| \mathbf{\mu}_{12}, \mathbf{\Lambda}_{123}^{-1})} 
\end{equation}\]</span>
with
<span class="math display">\[\begin{eqnarray}
    \mathbf{\Lambda}_{123} &amp;:=&amp; \mathbf{\Lambda}_1 - \mathbf{\Lambda}_3 + \mathbf{\Lambda}_2 \\
    \mathbf{\mu}_{12}  &amp;:=&amp; \mathbf{\Lambda}_{123}^{-1}(\mathbf{\Lambda}_1 \mathbf{\mu}_1 + \mathbf{\Lambda}_2 \mathbf{\mu}_2).
\end{eqnarray}\]</span>
We define
<span class="math display" id="eq:def-Jkij">\[\begin{align}
    \Lijk   &amp;:= \H_{ij} - \lambda_w \I + \Lk \\ 
    \muijk  &amp;:= \Lijk^{-1}(\H_{ij} \wij^* + \Lk \muk) \,.
\tag{6.11}
\end{align}\]</span>
<p>and obtain</p>
<span class="math display" id="eq:pXr-final">\[\begin{align}
p(\X | \c) \propto \prod_{1 \le i &lt; j \le L}  \sum_{k=0}^K g_{k}(\cij) \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})}  \,.
\tag{6.12}
\end{align}\]</span>
<p><span class="math inline">\(\Gauss( \mathbf{0} | \mathbf{0}, \lambda_w^{-1} \I)\)</span> and <span class="math inline">\(\Gauss( \mathbf{0} | \wij^*, \H_{ij}^{-1})\)</span> are constants that depend only on <span class="math inline">\(\X\)</span> and <span class="math inline">\(\lambda_w\)</span> and can be omitted.</p>
</div>
<div id="posterior-of-rij" class="section level3">
<h3><span class="header-section-number">6.5.4</span> Computing The Posterior Probability of Contacts</h3>
<p>The posterior distribution for <span class="math inline">\(c_{ij}\)</span> can be computed by marginalizing over all other contact states, which are summarized in the vector <span class="math inline">\(\c_{\backslash ij}\)</span>:</p>
<span class="math display">\[\begin{eqnarray}
    p(\cij | \X, \phi) &amp;=&amp; \int d \c_{\backslash ij} \, p(\c |\X, \mathbf{\phi}) \nonumber\\
                &amp;\propto &amp; \int d \c_{\backslash ij} \, p(\X|\c) \, p(\c | \phi) \nonumber\\
                &amp;\propto &amp; \int d \c_{\backslash ij} \prod_{i&#39;&lt;j&#39;} \sum_{k=0}^K g_{k}(c_{i&#39;j&#39;}) \, \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})}
 \, \prod_{i&#39;&lt;j&#39;} p(c_{i&#39;j&#39;} |\phi_{i&#39;j&#39;})  \,,
 \end{eqnarray}\]</span>
<p>and, by pulling out of the integral over <span class="math inline">\(\c_{\backslash ij}\)</span> the term depending only on <span class="math inline">\(\cij\)</span>,</p>
<span class="math display">\[\begin{eqnarray}
    p(\cij | \X, \phi) &amp; \propto &amp; 
            p(\cij |\phi_{ij}) \, \sum_{k=0}^K g_{k}(\cij) \, \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})} \nonumber\\
            &amp; \times  &amp; \prod_{i&#39;&lt;j&#39;, (i&#39;,j&#39;) \ne (i,j)} \int d c_{i&#39;j&#39;} \, p(c_{i&#39;j&#39;} |\phi_{i&#39;j&#39;}) \, \sum_{k=0}^K g_{k}(c_{i&#39;j&#39;}) \, \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})}
\end{eqnarray}\]</span>
<p>Since the second factor involving the integrals over <span class="math inline">\(c_{i&#39;j&#39;}\)</span> is a constant with respect to <span class="math inline">\(\cij\)</span>, we find</p>
<span class="math display" id="eq:posterior-marginal-rij">\[\begin{equation}
    p(\cij | \X, \phi) \propto  p(\cij |\phi_{ij}) \,  \sum_{k=0}^K g_{k}(\cij) \, \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})}  \, .
\tag{6.13}
\end{equation}\]</span>
</div>
<div id="Hessian-offdiagonal" class="section level3">
<h3><span class="header-section-number">6.5.5</span> The Hessian off-diagonal Elements Carry a Negligible Signal</h3>
<p>Assume that <span class="math inline">\(\lambda_w=0\)</span>, i.e., no regularisation is applied. Suppose in columns <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> a set of sequences in the MSA contain amino acids <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and the same sequences contain <span class="math inline">\(c\)</span> and <span class="math inline">\(d\)</span> in columns <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span>. Furthermore, assume that <span class="math inline">\((a,b)\)</span> occur nowhere else in columns <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> and the same holds for <span class="math inline">\((c,d)\)</span> in columns <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span>. This means that the coupling between <span class="math inline">\(a\)</span> at position <span class="math inline">\(i\)</span> and <span class="math inline">\(b\)</span> at position <span class="math inline">\(j\)</span> can be perfectly compensated by the coupling between <span class="math inline">\(c\)</span> at position <span class="math inline">\(k\)</span> and <span class="math inline">\(d\)</span> at position <span class="math inline">\(l\)</span>. Adding <span class="math inline">\(10^6\)</span> to <span class="math inline">\(w_{ijab}\)</span> and substracting <span class="math inline">\(10^6\)</span> from <span class="math inline">\(w_{klcd}\)</span> leaves <span class="math inline">\(p(\X|\v,\w)\)</span> unchanged. This means that <span class="math inline">\(w_{ijab}\)</span> and <span class="math inline">\(w_{klcd}\)</span> are almost perfectly negatively correlated in <span class="math inline">\(\Gauss(\w|\w^*,(\H)^{-1})\)</span>. Another way to see this is to evaluate <span class="math inline">\((\H)_{ijab,klcd}\)</span> with eq. <a href="theoretical-methods-1.html#eq:Hw-offdiag">(6.14)</a>, which gives <span class="math inline">\((\H)_{klcd, ijab}=N_{ij}\,p(x_i \eq a, x_j \eq b| \v^*,\w^*) \, ( 1 - p(x_i \eq a, x_j \eq b| \v^*,\w^*)\)</span> for this case. Under the assumption <span class="math inline">\(\lambda_w=0\)</span>, this precision matrix element is the same as the diagonal elements <span class="math inline">\((\H)_{ijab, ijab}\)</span> and <span class="math inline">\((\H)_{klcd, klcd}\)</span> (see case 2 in eq. <a href="theoretical-methods-1.html#eq:Hw-offdiag">(6.14)</a>).</p>
<p>But when a realistic regularisation constant is assumed, e.g. <span class="math inline">\(\lambda_w \eq 0.2 L \approx 20\)</span>, <span class="math inline">\(w_{ijab}\)</span> and <span class="math inline">\(w_{klcd}\)</span> will be pushed to near zero, because the matrix element that couples <span class="math inline">\(w_{ijab}\)</span> with <span class="math inline">\(w_{klcd}\)</span>, <span class="math inline">\(N_{ij}\,p(x_i \eq a, x_j \eq b| \v^*,\w^*) \, ( 1 - p(x_i \eq a, x_j \eq b| \v^*,\w^*)\)</span> is the number of sequences that share amino acids <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> at position <span class="math inline">\((i,j)\)</span> and <span class="math inline">\(c\)</span> and <span class="math inline">\(d\)</span> at position <span class="math inline">\((k,l)\)</span>, and this number is usually much smaller than <span class="math inline">\(\lambda_w\)</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:hessian-off-diagonal"></span>
<img src="img/theory/hessian-off-diagonal.png" alt="Setting the off-diagonal block matrices to zero in \(\H\) corresponds to replacing the violett Gaussian distrubution by the pink one. The ratios between the overlaps of \(\Gauss\!\left(\w \left| \w^*, \H^{-1} \right. \right)\) with the distributions \(\Gauss(\wij | \muk, \Lk^{-1})\) for various choices of \(k\) is only weakly affected by this replacement." width="80%" />
<p class="caption">
Figure 6.4: Setting the off-diagonal block matrices to zero in <span class="math inline">\(\H\)</span> corresponds to replacing the violett Gaussian distrubution by the pink one. The ratios between the overlaps of <span class="math inline">\(\Gauss\!\left(\w \left| \w^*, \H^{-1} \right. \right)\)</span> with the distributions <span class="math inline">\(\Gauss(\wij | \muk, \Lk^{-1})\)</span> for various choices of <span class="math inline">\(k\)</span> is only weakly affected by this replacement.
</p>
</div>
<p>It is therefore a good approximation to set the off-diagonal block matrices <span class="math inline">\((\H)_{klcd, ijab}\)</span> (case 3 in eq. <a href="theoretical-methods-1.html#eq:Hw-offdiag">(6.14)</a>) to zero. This corresponds to replacing the violett distribution in Figure <a href="theoretical-methods-1.html#fig:hessian-off-diagonal">6.4</a> by the pink one. To see why, first note that the functions <span class="math inline">\(g_k(\cij)\)</span> and the component distributions <span class="math inline">\(\Gauss(\wij | \mu_k, \Lk^{-1})\)</span> will be learned in such a way as to maximize the likelihood for predicting the correct contact state <span class="math inline">\(\c^m\)</span> from the respective alignments <span class="math inline">\(\X^m\)</span> for many <a href="abbrev.html#abbrev">MSAs</a> of protein families <span class="math inline">\(m\)</span>. Therefore, these model parameters will adjust to the fact that the off-diagonal blocks in <span class="math inline">\(\H\)</span> are neglected. Second, note that the integral over the product of <span class="math inline">\(\Gauss(\w | \w^*, \H^{-1})\)</span> and <span class="math inline">\(\prod_{i&lt;j} p(\wij | \cij) / \Gauss(\wij| 0, \lambda_w^{-1} \I)\)</span> in eq. <a href="theoretical-methods-1.html#eq:int-over-w">(6.8)</a> evaluates the overlap of these two Gaussians. Third, the components of <span class="math inline">\(p(\wij|\cij)\)</span> will be very much concentrated within a radius of less than <span class="math inline">\(1\)</span> from the origin, because even residues with short <span class="math inline">\(C_\beta\)</span>-<span class="math inline">\(C_\beta\)</span> distance will rarely have coupling coefficients above <span class="math inline">\(1\)</span>. Fourth, the Gaussian components have no couplings between elements of <span class="math inline">\(\wij\)</span> and <span class="math inline">\(\w_{kl}\)</span>, which is why they are axis-aligned (green in Figure <a href="theoretical-methods-1.html#fig:hessian-off-diagonal">6.4</a>). For these reasons, the relative strengths of the overlaps with different mixture components labeled by <span class="math inline">\(k\)</span> in eq. <a href="coupling-prior.html#eq:definition-mixture-coupling-prior">(6.4)</a> should be little affected by setting the off-diagonal block matrix couplings to zero.</p>
</div>
<div id="neg-Hessian-computation" class="section level3">
<h3><span class="header-section-number">6.5.6</span> Efficiently Computing the negative Hessian of the regularized log-likelihood</h3>
<p>Surprisingly, the elements of the Hessian at the mode <span class="math inline">\(\w^*\)</span> are easy to compute. Let <span class="math inline">\(i,j,k,l \in \{1,\ldots,L\}\)</span> be columns in the <a href="abbrev.html#abbrev">MSA</a> and let <span class="math inline">\(a, b, c, d \in \{1,\ldots,20\}\)</span> represent amino acids. The partial derivative <span class="math inline">\(\partial / \partial \w_{klcd}\)</span> of the second term in the gradient of the couplings in eq. <a href="theoretical-methods.html#eq:gradient-LLreg-pair">(4.10)</a> is</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab } 
    &amp;=&amp;  - \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} \frac{\partial \left( \frac{\exp \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}^L w_{ij}(y_i,y_j) \right) }{Z_n(\v,\w)} \right)}{\partial \wklcd}   I(y_i \eq a, y_j \eq b) \nonumber \\
    &amp;&amp;- \lambda_w \delta_{ijab,klcd} \,,
\end{eqnarray}\]</span>
<p>where <span class="math inline">\(\delta_{ijab,klcd} = I(ijab=klcd)\)</span> is the Kronecker delta. Applying the product rule, it is found</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab  } 
    &amp;=&amp;  - \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} \frac{\exp \left(\sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}^L w_{ij}(y_i,y_j)  \right)}{Z_n(\v,\w)}  I(y_i \eq a, y_j \eq b) \nonumber \\
    &amp;&amp; \times  \left[ \frac{\partial}{\partial \wklcd} \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}  w_{ij}(y_i,y_j)  \right) 
                  - \frac{1}{Z_n(\v,\w)} \frac{\partial  Z_n(\v,\w) }{\partial\wklcd} \right] \nonumber\\
    &amp;&amp;- \lambda_w \delta_{ijab,klcd} \\
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab  } 
    &amp;=&amp;  - \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} \frac{\exp \left(\sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}^L w_{ij}(y_i,y_j)  \right)}{Z_n(\v,\w)}  I(y_i \eq a, y_j \eq b) \nonumber\\
    &amp;&amp; \times \left[ I(y_k \eq c, y_l \eq d) - \frac{\partial}{\partial \wklcd} \log Z_n(\v,\w) \right] \nonumber\\
    &amp;&amp;- \lambda_w \delta_{ijab,klcd} \,.
\end{eqnarray}\]</span>
<p>This expression can be simpified using</p>
<span class="math display">\[\begin{equation}
    p(\mathbf{y} | \v,\w) = \frac{\exp \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L} w_{ij}(y_i,y_j) \right)}{Z_n(\v,\w)}  ,
\end{equation}\]</span>
<p>yielding</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab} 
    &amp;=&amp; -  \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} p(\mathbf{y} | \v,\w) \, I(y_i \eq a, y_j \eq b, y_k \eq c, y_l \eq d) \nonumber \\
    &amp;&amp; + \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \mathcal{S}_n} p(\mathbf{y} | \v,\w) \, I(y_i \eq a, y_j \eq b ) \sum_{\mathbf{y} \in \Sn} p(\mathbf{y} | \v,\w)  I(y_k \eq c, y_l \eq d ) \nonumber \\
    &amp;&amp; - \lambda_w \delta_{ijab,klcd} \,.
\end{eqnarray}\]</span>
<p>If <span class="math inline">\(\X\)</span> does not contain too many gaps, this expression can be approximated by</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab  } 
    &amp;=&amp; - N_{ijkl} \: p(x_i \eq a, x_j \eq b, x_k \eq c, x_l \eq d | \v,\w)  \nonumber \\
    &amp;&amp; +  N_{ijkl} \: p(x_i \eq a, x_j \eq b | \v,\w) \, p(x_k \eq c, x_l \eq d | \v,\w) - \lambda_w \delta_{ijab,klcd} \,,
\end{eqnarray}\]</span>
<p>where <span class="math inline">\(N_{ijkl}\)</span> is the number of sequences that have a residue in <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span>, <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span>.</p>
<p>Looking at three cases separately:</p>
<ul>
<li>case 1: <span class="math inline">\((k,l) = (i,j)\)</span> and <span class="math inline">\((c,d) = (a,b)\)</span></li>
<li>case 2: <span class="math inline">\((k,l) = (i,j)\)</span> and <span class="math inline">\((c,d) \ne (a,b)\)</span></li>
<li>case 3: <span class="math inline">\((k,l) \ne (i,j)\)</span> and <span class="math inline">\((c,d) \ne (a,b)\)</span>,</li>
</ul>
<p>the elements of <span class="math inline">\(\H\)</span>, which are the negative second partial derivatives of <span class="math inline">\(\LLreg(\v^*,\w)\)</span> with respect to the components of <span class="math inline">\(\w\)</span>, are</p>
<span class="math display" id="eq:Hw-offdiag">\[\begin{eqnarray}
  \mathrm{case~1:} (\H)_{ijab, ijab} &amp;=&amp; N_{ij} \, p(x_i \eq a, x_j \eq b| \v^*,\w^*) \, ( 1 - p(x_i \eq a, x_j \eq b| \v^*,\w^*) \,) \nonumber \\
                                      &amp;&amp; +\lambda_w \\
  \mathrm{case~2:} (\H)_{ijcd, ijab} &amp;=&amp; -N_{ij} \, p(x_i \eq a, x_j \eq b |\v^*,\w^*) \, p(x_i \eq c, x_j \eq d |\v^*,\w^*) \\
  \mathrm{case~3:} (\H)_{klcd, ijab} &amp;=&amp; N_{ijkl} \, p(x_i \eq a, x_j \eq b, x_k \eq c, x_l \eq d  | \v^*,\w^*) \nonumber \\
                                      &amp;&amp; -N_{ijkl} \, p(x_i \eq a, x_j \eq b | \v^*,\w^*)\, p(x_k \eq c, x_l \eq d | \v^*,\w^*) \,.
\tag{6.14}
\end{eqnarray}\]</span>
<p>We know from eq. <a href="theoretical-methods.html#eq:gradient-LLreg-pair-approx">(4.12)</a> that at the mode <span class="math inline">\(\w^*\)</span> the model probabilities match the empirical frequencies up to a small regularization term,</p>
<span class="math display">\[\begin{equation}
    p(x_i \eq a, x_j \eq b | \v^*,\w^*) = q(x_i \eq a, x_j \eq b) - \frac{\lambda_w}{N_{ij}}  \wijab^* \,,
\end{equation}\]</span>
<p>and therefore the negative Hessian elements in cases 1 and 2 can be expressed as</p>
<!-- The first term ($N_{ij} \left(\,q(x_i\!=\!a, x_j\!=\!b)  - \frac{\lambda_w}{N_{ij}} \wijab^* \right)$) actually is accurate and only the second $(\dots)$ iss approximated according to the gap approximation in eq. \@ref(eq:gradient-LLreg-approx) 
-->
<span class="math display" id="eq:Hw-diag">\[\begin{eqnarray}
   (\H)_{ijab, ijab} &amp;=&amp; N_{ij} \left( q(x_i \eq a, x_j \eq b)  - \frac{\lambda_w}{N_{ij}} \wijab^* \right) \left( 1 - q(x_i \eq a, x_j \eq b) +\frac{\lambda_w}{N_{ij}} \wijab^* \right) \nonumber\\
                      &amp;&amp; +\lambda_w \\
   (\H)_{ijcd, ijab} &amp;=&amp; -N_{ij} \left(\,q(x_i \eq a, x_j \eq b)  - \frac{\lambda_w}{N_{ij}} \wijab^* \right) \left( q(x_i \eq c, x_j \eq d) -\frac{\lambda_w}{N_{ij}} \wijcd^* \right) .
\tag{6.15}
\end{eqnarray}\]</span>
<p>In order to write the previous eq. <a href="theoretical-methods-1.html#eq:Hw-diag">(6.15)</a> in matrix form, the <em>regularised</em> empirical frequencies <span class="math inline">\(\qij\)</span> will be defined as</p>
<span class="math display">\[\begin{equation}
    (\qij)_{ab} = q&#39;_{ijab} := q(x_i \eq a, x_j \eq b) - \lambda_w  \wijab^* / N_{ij} \,,
\end{equation}\]</span>
<p>and the <span class="math inline">\(400 \times 400\)</span> diagonal matrix <span class="math inline">\(\Qij\)</span> will be defined as</p>
<span class="math display">\[\begin{equation}
    \Qij := \text{diag}(\qij) \; .
\end{equation}\]</span>
<p>Now eq. <a href="theoretical-methods-1.html#eq:Hw-diag">(6.15)</a> can be written in matrix form</p>
<span class="math display" id="eq:mat-Hij">\[\begin{equation}
     \H_{ij} = N_{ij} \left( \Qij -  \qij \qij^{\mathrm{T}} \right)  + \lambda_w \I \; .
\tag{6.16}
\end{equation}\]</span>
</div>
<div id="inv-lambda-ij-k" class="section level3">
<h3><span class="header-section-number">6.5.7</span> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></h3>
<p>It is possible to efficiently invert the matrix <span class="math inline">\(\Lijk = \H_{ij} - \lambda_w \I + \Lambda_k\)</span>, that is introduced in section <a href="theoretical-methods-1.html#likelihood-fct-distances">6.5.3</a> where <span class="math inline">\(\H_{ij}\)</span> is the <span class="math inline">\(400 \times 400\)</span> diagonal block submatrix <span class="math inline">\((\H_{ij})_{ab,cd} := (\H)_{ijab,ijcd}\)</span> and <span class="math inline">\(\Lambda_k\)</span> is an invertible diagonal precision matrix. Equation <a href="theoretical-methods-1.html#eq:mat-Hij">(6.16)</a> can be used to write <span class="math inline">\(\Lijk\)</span> in matrix form as</p>
<span class="math display" id="eq:mat-Lijk">\[\begin{equation}
     \Lijk = \H_{ij} - \lambda_w \I + \Lk = N_{ij} \Qij- N_{ij} \qij \qij^{\mathrm{T}} + \Lk \,.
\tag{6.17}
\end{equation}\]</span>
<p>Owing to eqs. <a href="theoretical-methods.html#eq:normalized-emp-freq">(4.8)</a> and <a href="theoretical-methods.html#eq:zero-sum-wij">(4.14)</a>, <span class="math inline">\(\sum_{a,b=1}^{20} q&#39;_{ijab} = 1\)</span>. The previous equation <a href="theoretical-methods-1.html#eq:mat-Lijk">(6.17)</a> facilitates the calculation of the inverse of this matrix using the <em>Woodbury identity</em> for matrices</p>
<span class="math display">\[\begin{equation}
    (\mathbf{A} + \mathbf{B} \mathbf{D}^{-1} \mathbf{C})^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{B} (\mathbf{D} + \mathbf{C} \mathbf{A}^{-1} \mathbf{B}) ^{-1} \mathbf{C} \mathbf{A}^{-1} \;. 
\end{equation}\]</span>
<p>by setting</p>
<span class="math display">\[\begin{align}
  \mathbf{A} &amp;= N_{ij} \Qij + \Lk \nonumber\\
  \mathbf{B} &amp;= \qij \nonumber\\
  \mathbf{C} &amp;= \qij^\mathrm{T} \nonumber\\
  \mathbf{D} &amp;=- N_{ij}^{-1} \nonumber
\end{align}\]</span>
<p>Now, the inverse of <span class="math inline">\(\Lijk\)</span> can be computed as</p>
<span class="math display" id="eq:fast-inverse-mat-Lijk">\[\begin{align}
      \left( \H_{ij} - \lambda_w \I + \Lk \right)^{-1} &amp; = \mathbf{A}^{-1} - \mathbf{A}^{-1} \qij  \left( -N_{ij}^{-1}  + \qij^\mathrm{T} \mathbf{A}^{-1} \qij \right)^{-1}  \qij^\mathrm{T} \mathbf{A}^{-1} \nonumber\\
     &amp; = \mathbf{A}^{-1} + \frac{ (\mathbf{A}^{-1} \qij) (\mathbf{A}^{-1} \qij)^{\mathrm{T}} }{ N_{ij}^{-1} - \qij^\mathrm{T} \mathbf{A}^{-1} \qij} \,.
\tag{6.18}
\end{align}\]</span>
<p>Note that <span class="math inline">\(\mathbf{A}\)</span> is diagonal as <span class="math inline">\(\Qij\)</span> and <span class="math inline">\(\Lk\)</span> are diagonal matrices: <span class="math inline">\(\mathbf{A} = \text{diag}(N_{ij} q&#39;_{ijab} + (\Lk)_{ab,ab})\)</span>. Moreover, <span class="math inline">\(\mathbf{A}\)</span> has only positive diagonal elements, because <span class="math inline">\(\Lk\)</span> is invertible and has only positive diagonal elements and because <span class="math inline">\(q&#39;_{ijab} = p(x_i \eq a, x_j \eq b | \v^*,\w^*) \ge 0\)</span>. Therefore <span class="math inline">\(\mathbf{A}\)</span> is invertible: <span class="math inline">\(\mathbf{A}^{-1} = \text{diag}(N_{ij} q&#39;_{ijab} + (\Lk)_{ab,ab} )^{-1}\)</span>. Because <span class="math inline">\(\sum_{a,b=1}^{20} q&#39;_{ijab} = 1\)</span>, the denominator of the second term is</p>
<span class="math display">\[\begin{equation}
    N_{ij}^{-1} - \sum_{a,b=1}^{20}  \frac{{q&#39;}_{ijab}^2}{N_{ij} q&#39;_{ijab} + {(\Lk)}_{ab,ab} } &gt; N_{ij}^{-1} - \sum_{a,b=1}^{20} \frac{{q&#39;}^2_{ijab}}{N_{ij} q&#39;_{ijab}} = 0
\end{equation}\]</span>
<p>and therefore the inverse of <span class="math inline">\(\Lijk\)</span> in eq. <a href="theoretical-methods-1.html#eq:fast-inverse-mat-Lijk">(6.18)</a> is well defined. The log determinant of <span class="math inline">\(\Lijk\)</span> is necessary to compute the ratio of Gaussians (see equation <a href="theoretical-methods-1.html#eq:pXr-final">(6.12)</a>) and can be computed using the matrix determinant lemma:</p>
<span class="math display">\[\begin{equation}
  \det(\mathbf{A} + \mathbf{uv}^\mathrm{T}) = (1+\mathbf{v}^\mathrm{T} \mathbf{A}^{-1} \mathbf{u}) \det(\mathbf{A})
\end{equation}\]</span>
<p>Setting <span class="math inline">\(\mathbf{A} = N_{ij} \Qij + \Lk\)</span> and <span class="math inline">\(\v = \qij\)</span> and <span class="math inline">\(\mathbf{u} = - N_{ij} \qij\)</span> yields</p>
<span class="math display">\[\begin{equation}
  \det(\Lijk ) = \det(\H_{ij} - \lambda_w \I + \Lk) = (1 - N_{ij}\qij^\mathrm{T} \mathbf{A}^{-1}\qij) \det(\mathbf{A}) \,.
\end{equation}\]</span>
<p><span class="math inline">\(\mathbf{A}\)</span> is diagonal and has only positive diagonal elements so that <span class="math inline">\(\log(\det(\mathbf{A})) = \sum \log \left( \text{diag}(\mathbf{A}) \right)\)</span>.</p>
</div>
<div id="gradient-muk" class="section level3">
<h3><span class="header-section-number">6.5.8</span> The gradient of the log likelihood with respect to <span class="math inline">\(\muk\)</span></h3>
<p>By applying the formula <span class="math inline">\(d f(x) / dx = f(x) \, d \log f(x) / dx\)</span> to compute the gradient of eq. <a href="practical-methods.html#eq:ll-coupling-prior">(6.32)</a> (neglecting the regularization term) with respect to <span class="math inline">\(\mu_{k,ab}\)</span>, one obtains</p>
<span class="math display" id="eq:gradient-mukab">\[\begin{equation}
 \frac{\partial}{\partial \mu_{k,ab}} L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    = \sum_{1\le i&lt;j\le L}  
    \frac{ 
        g_{k}(\cij) \frac{  \Gauss ( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} 
             \frac{\partial}{\partial \mu_{k,ab}}  \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right)  
     } { \sum_{k&#39;=0}^K g_{k&#39;}(\cij) \, \frac{ \Gauss(\mathbf{0} | \muk&#39;, \Lk&#39;^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})}  } .
\tag{6.19}
\end{equation}\]</span>
<p>To simplify this expression, we define the responsibility of component <span class="math inline">\(k\)</span> for the posterior distribution of <span class="math inline">\(\wij\)</span>, the probability that <span class="math inline">\(\wij\)</span> has been generated by component <span class="math inline">\(k\)</span>:</p>
<span class="math display" id="eq:responsibilities">\[\begin{align}
      p(k|ij)  = 
      \frac{ g_{k}(\cij) \frac{ \Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})} } 
    {\sum_{k&#39;=0}^K g_{k&#39;}(\cij) \frac{ \Gauss(\mathbf{0} | \muk&#39;, \Lk&#39;^{-1})}{\Gauss( \mathbf{0} | \muijk&#39;, \Lijk&#39;^{-1})} }  \,.
\tag{6.20}
\end{align}\]</span>
<p>By substituting the definition for responsibility, <a href="theoretical-methods-1.html#eq:gradient-mukab">(6.19)</a> simplifies</p>
<span class="math display" id="eq:gradient-LL-mukab">\[\begin{equation}
  \frac{\partial}{\partial \mu_{k,ab}}  L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    = \sum_{1\le i&lt;j\le L}  p(k | ij)  \frac{\partial}{\partial \mu_{k,ab}} \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right) ,
\tag{6.21}
\end{equation}\]</span>
<p>and analogously for partial derivatives with respect to <span class="math inline">\(\Lambda_{k,ab,cd}\)</span>. The partial derivative inside the sum can be written</p>
<span class="math display">\[\begin{equation}
     \frac{\partial}{\partial \mu_{k,ab}} \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right)
    = \frac{1}{2}  \frac{\partial}{\partial \mu_{k,ab}}   \left( \log | \Lk | - \muk^\mathrm{T} \Lk \muk - \log | \Lijk | + \muijk^\mathrm{T} \Lijk \muijk \right)\,.
\end{equation}\]</span>
<p>Using the following formula for a matrix <span class="math inline">\(\mathbf{A}\)</span>, a real variable <span class="math inline">\(x\)</span> and a vector <span class="math inline">\(\mathbf{y}\)</span> that depends on <span class="math inline">\(x\)</span>,</p>
<span class="math display" id="eq:matrix-gradient">\[\begin{equation}
    \frac{\partial}{\partial x} \left( \mathbf{y}^\mathrm{T} \mathbf{A} \mathbf{y} \right) = \frac{\partial \mathbf{y}^\mathrm{T}}{\partial x}  \mathbf{A} \mathbf{y} + \mathbf{y}^\mathrm{T} \mathbf{A} \frac{\partial \mathbf{y}}{\partial x}  =  \mathbf{y}^\mathrm{T} (\mathbf{A} + \mathbf{A}^\mathrm{T}) \frac{\partial \mathbf{y}}{\partial x} 
\tag{6.22}
\end{equation}\]</span>
<p>the partial derivative therefore becomes</p>
<span class="math display">\[\begin{align}
     \frac{\partial}{\partial \mu_{k,ab}} \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right)
    =&amp; \left( -\muk^\mathrm{T} \Lk \mathbf{e}_{ab} \, +  \muijk^\mathrm{T} \Lijk \Lijk^{-1} \Lk \mathbf{e}_{ab} \right) \nonumber \\
    =&amp; \, \mathbf{e}^\mathrm{T}_{ab} \Lk ( \muijk - \muk ) \; . 
\end{align}\]</span>
<p>Finally, the gradient of the log likelihood with respect to <span class="math inline">\(\mathbf{\mu}\)</span> becomes</p>
<span class="math display" id="eq:gradient-muk-final">\[\begin{align}
    \nabla_{\muk} L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k) =  \sum_{1\le i&lt;j\le L}  p(k|ij)  \,  \Lk \left(  \muijk  - \muk \right) \; .
\tag{6.23}
\end{align}\]</span>
<p>The correct computation of the gradient <span class="math inline">\(\nabla_{\muk} L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)\)</span> has been verified using finite differences.</p>
</div>
<div id="gradient-lambdak" class="section level3">
<h3><span class="header-section-number">6.5.9</span> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></h3>
<p>Analogously to eq. <a href="theoretical-methods-1.html#eq:gradient-LL-mukab">(6.21)</a> one first needs to solve</p>
<span class="math display" id="eq:grad-log-N-N-lambdakabcd">\[\begin{align}
     &amp; \frac{\partial}{\partial \Lambda_{k,ab,cd}} \log \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \nonumber\\
     &amp;= \frac{1}{2}  \frac{\partial}{\partial \Lambda_{k,ab,cd}}  \left( \log |\Lk| - \muk^\mathrm{T} \Lk \muk - \log |\Lijk| + \muijk^\mathrm{T} \Lijk \muijk \right) \,,
\tag{6.24}
\end{align}\]</span>
<p>by applying eq. <a href="theoretical-methods-1.html#eq:matrix-gradient">(6.22)</a> as before as well as the formulas</p>
<span class="math display">\[\begin{align}
    \frac{\partial}{\partial x} \log |\mathbf{A} | &amp;= \text{Tr}\left( \mathbf{A}^{-1} \frac{\partial \mathbf{A}}{\partial x}  \right) , \nonumber\\
    \frac{\partial \mathbf{A}^{-1}}{\partial x} &amp;= - \mathbf{A}^{-1} \frac{\partial \mathbf{A}}{\partial x} \mathbf{A}^{-1} \,.
\end{align}\]</span>
<p>This yields</p>
<span class="math display">\[\begin{align}
\frac{\partial}{\partial \Lambda_{k,ab,cd}}  \log |\Lk|
     &amp;= \text{Tr} \left( \Lk^{-1} \frac{\partial \Lk}{\partial \Lambda_{k,ab,cd}} \right) 
     = \text{Tr} \left( \Lk^{-1} \mathbf{e}_{ab} \mathbf{e}_{cd}^\mathrm{T} \right) 
     = \Lambda^{-1}_{k,cd,ab} \\
\frac{\partial}{\partial \Lambda_{k,ab,cd}}  \log |\Lijk|
     &amp;= \text{Tr} \left( \Lijk^{-1} \frac{\partial (\H_{ij} - \lambda_w \I + \Lk)}{\partial \Lambda_{k,ab,cd}}   \right) 
     = \Lambda^{-1}_{ij,k,cd,ab} \\
\frac{\partial (\muk^\mathrm{T} \Lk \muk)}{\partial \Lambda_{k,ab,cd}} 
    &amp;= \muk^\mathrm{T} \mathbf{e}_{ab} \mathbf{e}_{cd}^\mathrm{T} \muk 
    = \mathbf{e}_{ab}^\mathrm{T} \muk \muk^\mathrm{T} \mathbf{e}_{cd} = (\muk \muk^\mathrm{T})_{ab,cd} \\
\frac{\partial ( \muijk^\mathrm{T} \Lijk \muijk) }{\partial \Lambda_{k,ab,cd}} 
    =&amp; \muijk^\mathrm{T} \frac{\partial \Lijk}{\partial \Lambda_{k,ab,cd}} \muijk 
    + 2 \muijk^\mathrm{T} \Lijk \frac{\partial \Lijk^{-1}}{\partial \Lambda_{k,ab,cd}}  (\Hij \wij^* + \Lk \muk) \nonumber\\
    &amp;+ 2 \muijk^\mathrm{T} \frac{\partial \Lk}{\partial \Lambda_{k,ab,cd}} \muk \nonumber \\
    =&amp; (\muijk \muijk^\mathrm{T} + 2 \muijk \muk^\mathrm{T})_{ab,cd} \nonumber \\
    &amp; -2 \muijk^\mathrm{T} \Lijk  \Lijk^{-1} \frac{\partial \Lijk}{\partial \Lambda_{k,ab,cd}} \Lijk^{-1} (\Hij\wij^* + \Lk \muk) \nonumber \\
    =&amp; (\muijk \muijk^\mathrm{T} + 2 \muijk \muk^\mathrm{T})_{ab,cd}  
    - 2 \muijk^\mathrm{T}  \frac{\partial \Lijk}{\partial \Lambda_{k,ab,cd}} \muijk \nonumber\\
    =&amp; (- \muijk \muijk^\mathrm{T} + 2 \muijk \muk^\mathrm{T})_{ab,cd} \,.
\end{align}\]</span>
<p>Inserting these results into eq. <a href="theoretical-methods-1.html#eq:grad-log-N-N-lambdakabcd">(6.24)</a> yields</p>
<span class="math display">\[\begin{align}
     \frac{\partial}{\partial \Lambda_{k,ab,cd}} \log \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} 
    = \frac{1}{2} \left( \Lk^{-1} - \Lijk^{-1} - (\muijk - \muk) (\muijk - \muk)^\mathrm{T} \right)_{ab,cd}\,.
\end{align}\]</span>
<p>Substituting this expression into the equation <a href="theoretical-methods-1.html#eq:gradient-LL-mukab">(6.21)</a> analogous to the derivation of gradient for <span class="math inline">\(\mu_{k,ab}\)</span> yields the equation</p>
<span class="math display" id="eq:gradient-lambdak-final">\[\begin{align}
    \nabla_{\Lk}  L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    =  \frac{1}{2} \sum_{1\le i&lt;j\le L}  p(k|ij)  \, 
        \left( \Lk^{-1} - \Lijk^{-1} - (\muijk - \muk) (\muijk - \muk)^\mathrm{T} \right). 
\tag{6.25}
\end{align}\]</span>
<p>The correct computation of the gradient <span class="math inline">\(\nabla_{\Lk} L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)\)</span> has been verified using finite differences.</p>
</div>
<div id="the-gradient-of-the-log-likelihood-with-respect-to-gamma_k" class="section level3">
<h3><span class="header-section-number">6.5.10</span> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></h3>
<p>With <span class="math inline">\(\cij \in \{0,1\}\)</span> defining a residue pair in physical contact or not in contact, the mixing weights can be modelled as a softmax function according to eq. <a href="theoretical-methods-1.html#eq:def-g-k-binary">(6.5)</a>. The derivative of the mixing weights <span class="math inline">\(g_k(\cij)\)</span> is:</p>
<span class="math display">\[\begin{eqnarray}
\frac{\partial g_{k&#39;}(\cij)} {\partial \gamma_k} = \left\{
  \begin{array}{lr}
    g_k(\cij) (1 - g_k(\cij)) &amp; : k&#39; = k\\
    g_{k&#39;}(\cij) - g_k(\cij)  &amp; : k&#39; \neq k
  \end{array}
  \right.
\end{eqnarray}\]</span>
<p>The partial derivative of the likelihood function with respect to <span class="math inline">\(\gamma_k\)</span> is:</p>
<span class="math display">\[\begin{align}
\frac{\partial} {\partial \gamma_k}     L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k) 
  =&amp;  \sum_{1\le i&lt;j\le L} \frac{\sum_{k&#39;=0}^K  \frac{\partial}{\partial \gamma_k} g_{k&#39;}(\cij)  
  \frac{\Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( 0 | \muijk, \Lijk^{-1})}}
  {\sum_{k&#39;=0}^K g_{k&#39;}(\cij)  \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})}} \nonumber \\
  =&amp;  \sum_{1\le i&lt;j\le L} \frac{\sum_{k&#39;=0}^K  g_{k&#39;}(\cij)  
  \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \cdot 
  \begin{cases} 
   1-g_k(\cij) &amp; \text{if } k&#39; = k \\
   -g_k(\cij)  &amp; \text{if } k&#39; \neq k
  \end{cases}}
  {\sum_{k&#39;=0}^K g_{k&#39;}(\cij)  \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})}} \nonumber\\
  =&amp; \sum_{1\le i&lt;j\le L} \sum_{k&#39;=0}^K p(k&#39;|ij) 
  \begin{cases} 
    1-g_k(\cij) &amp; \text{if } k&#39; = k \\
    -g_k(\cij)  &amp; \text{if } k&#39; \neq k 
  \end{cases} \nonumber\\
  =&amp; \sum_{1 \leq i&lt;j\leq L} p(k|ij) - g_k(\cij) \sum_{k&#39;=0}^K p(k&#39;|ij) \nonumber\\
  =&amp; \sum_{1 \leq i&lt;j\leq L} p(k|ij) - g_k(\cij)
\end{align}\]</span>
</div>
<div id="extending-the-bayesian-statistical-model-for-the-prediction-of-protein-residue-residue-distances" class="section level3">
<h3><span class="header-section-number">6.5.11</span> Extending the Bayesian Statistical Model for the Prediction of Protein Residue-Residue Distances</h3>
<p>It is straightforward to extend the Baysian model for contact prediction presented in section <a href="overview-posterior-distances.html#overview-posterior-distances">6.1</a> for distances. The prior over couplings will modelled using distance dependent mixture weights <span class="math inline">\(g_k(\cij)\)</span>. Therefore eq. <a href="coupling-prior.html#eq:definition-mixture-coupling-prior">(6.4)</a> is modified such that mixture weights <span class="math inline">\(g_k(\cij)\)</span> are modelled as softmax over linear functions <span class="math inline">\(\gamma_k(\cij)\)</span> (see Figure <a href="theoretical-methods-1.html#fig:softmax-linear-fct">6.5</a>:</p>
<span class="math display" id="eq:definition-mixture-weights">\[\begin{align}
      g_k(\cij)        &amp;= \frac{\exp \gamma_k(\cij)}{\sum_{k&#39;=0}^K \exp \gamma_{k&#39;}(\cij)} \, , \\
      \gamma_k(\cij)   &amp;= - \sum_{k&#39;=0}^{k} \alpha_{k&#39;} ( \cij - \rho_{k&#39;}) .
\tag{6.26}
\end{align}\]</span>

<div class="figure" style="text-align: center"><span id="fig:softmax-linear-fct"></span>
<img src="img/theory/softmax_linear_fct.png" alt="The Gaussian mixture coefficients \(g_k(\cij)\) of \(p(\wij|\cij)\) are modelled as softmax over linear functions \(\gamma_k(\cij)\). \(\rho_k\) sets the transition point between neighbouring components \(g_{k-1}(\cij)\) and \(g_k(\cij)\), while \(\alpha_k\) quantifies the abruptness of the transition between \(g_{k-1}(\cij)\) and \(g_k(\cij)\)." width="50%" />
<p class="caption">
Figure 6.5: The Gaussian mixture coefficients <span class="math inline">\(g_k(\cij)\)</span> of <span class="math inline">\(p(\wij|\cij)\)</span> are modelled as softmax over linear functions <span class="math inline">\(\gamma_k(\cij)\)</span>. <span class="math inline">\(\rho_k\)</span> sets the transition point between neighbouring components <span class="math inline">\(g_{k-1}(\cij)\)</span> and <span class="math inline">\(g_k(\cij)\)</span>, while <span class="math inline">\(\alpha_k\)</span> quantifies the abruptness of the transition between <span class="math inline">\(g_{k-1}(\cij)\)</span> and <span class="math inline">\(g_k(\cij)\)</span>.
</p>
</div>
<p>The functions <span class="math inline">\(g_k(\cij)\)</span> remain invariant when adding an offset to all <span class="math inline">\(\gamma_k(\cij)\)</span>. This degeneracy can be removed by setting <span class="math inline">\(\gamma_0(\cij) \eq 0\)</span> (i.e., <span class="math inline">\(\alpha_0 \eq 0\)</span> and <span class="math inline">\(\rho_0 \eq 0\)</span>). Further, the components are ordered, <span class="math inline">\(\rho_1&gt; \ldots &gt; \rho_K\)</span> and it is demanded that <span class="math inline">\(\alpha_k &gt; 0\)</span> for all <span class="math inline">\(k\)</span>. This ensures that for <span class="math inline">\(\cij \rightarrow \infty\)</span> we will obtain <span class="math inline">\(g_0(\cij) \rightarrow 1\)</span> and hence <span class="math inline">\(p(\w | \X) \rightarrow \Gauss(0, \sigma_0^2 \I )\)</span>.</p>
<p>The parameters <span class="math inline">\(\rho_k\)</span> mark the transition points between the two Gaussian mixture components <span class="math inline">\(k-1\)</span> and <span class="math inline">\(k\)</span>, i.e., the points at which the two components obtain equal weights. This follows from <span class="math inline">\(\gamma_k(\cij) - \gamma_{k-1}(r) \eq \alpha_{t} ( \cij - \rho_{t})\)</span> and hence <span class="math inline">\(\gamma_{k-1}(\rho_k) \eq= \gamma_k(\rho_k)\)</span>. A change in <span class="math inline">\(\rho_k\)</span> or <span class="math inline">\(\alpha_k\)</span> only changes the behaviour of <span class="math inline">\(g_{k-1}(\cij)\)</span> and <span class="math inline">\(g_k(\cij)\)</span> in the transition region around <span class="math inline">\(\rho_k\)</span>. Therefore, this particular definition of <span class="math inline">\(\gamma_k(\cij)\)</span> makes the parameters <span class="math inline">\(\alpha_k\)</span> and <span class="math inline">\(\rho_k\)</span> as independent of each other as possible, rendering the optimisation of these parameters more efficient.</p>
<div id="the-derivative-of-the-log-likelihood-with-respect-to-rho_k" class="section level4">
<h4><span class="header-section-number">6.5.11.1</span> The derivative of the log likelihood with respect to <span class="math inline">\(\rho_k\)</span></h4>
<p>Analogous to the derivations of <span class="math inline">\(\muk\)</span> in section <a href="theoretical-methods-1.html#gradient-muk">6.5.8</a> and <span class="math inline">\(\Lk\)</span> in section <a href="theoretical-methods-1.html#gradient-lambdak">6.5.9</a>, the partial derivative with respect to <span class="math inline">\(\rho_k\)</span> is</p>
<span class="math display" id="eq:nabla-rhok-LL-pre">\[\begin{equation}
\frac{\partial} {\partial \rho_k} L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \rho, \alpha) =  \sum_{1\le i&lt;j\le L}  \, \sum_{k&#39;=0}^K  p(k&#39;|ij) \,  \frac{\partial} {\partial \rho_k}  \log g_{k&#39;}(\cij)  \,.
    \tag{6.27}
\end{equation}\]</span>
Using the definition of <span class="math inline">\(g_k(\cij)\)</span> in eq. <a href="theoretical-methods-1.html#eq:definition-mixture-weights">(6.26)</a>, we find (rember that <span class="math inline">\(\alpha_0 \eq 0\)</span> as noted in the last section) that
<span class="math display" id="eq:dlog-gk-drho">\[\begin{eqnarray}
    \frac{\partial} {\partial \rho_k}  \log g_{l}(\cij)  
    &amp;=&amp; \frac{\partial} {\partial \rho_k}  \log  \frac{\exp \left(- \sum_{k&#39;&#39;=1}^{k&#39;} \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) \right) }{ \sum_{k&#39;=0}^K  \exp \left(- \sum_{k&#39;&#39;=1}^{k&#39;} \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) \right) } \nonumber \\
    &amp;=&amp; -  \frac{\partial} {\partial \rho_k}  \sum_{k&#39;&#39;=1}^{l} \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} )  
        - \frac{\partial} {\partial \rho_k}  \log  \sum_{k&#39;=0}^K  \exp \left(- \sum_{k&#39;&#39;=1}^{k&#39;} \, \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) \right)  \nonumber \\
    &amp;=&amp; \alpha_k \, I(l \ge k)   
        -  \frac{ \sum_{k&#39;=0}^K \frac{\partial} {\partial \rho_k}  \exp (- \sum_{k&#39;&#39;=1}^{k&#39;} \, \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) ) }{ \sum_{k&#39;=0}^K  \exp (- \sum_{k&#39;&#39;=1}^{k&#39;} \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) ) } \nonumber \\
    &amp;=&amp; \alpha_k \, I(l \ge k)   
        -  \frac{ \sum_{k&#39;=0}^K \exp (- \sum_{k&#39;&#39;=1}^{k&#39;} \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) ) \, \alpha_k \, I(k&#39; \ge k)    }{ \sum_{k&#39;=0}^K  \exp (- \sum_{k&#39;&#39;=1}^{k&#39;} \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) ) } \nonumber \\
    &amp;=&amp; \alpha_k \, I(l \ge k)   
        -  \frac{ \sum_{k&#39;=0}^K \exp (\gamma_{k&#39;}(\cij) ) \, \alpha_k \, I(k&#39; \ge k)    }{ \sum_{k&#39;=0}^K  \exp (\gamma_{k&#39;}(\cij) ) } \nonumber \\
    &amp;=&amp; \alpha_k \, I(l \ge k)   
        -  \sum_{k&#39;=0}^K  g_{k&#39;}(\cij) \, \alpha_k \, I(k&#39; \ge k)  \nonumber \\ 
    &amp;=&amp; \alpha_k \, \left(  I(l \ge k)  -  \sum_{k&#39;=k}^K  g_{k&#39;}(\cij) \right)  \, .
    \tag{6.28}
\end{eqnarray}\]</span>
<p>Inserting this into eq. <a href="theoretical-methods-1.html#eq:nabla-rhok-LL-pre">(6.27)</a> yields</p>
<span class="math display">\[\begin{align}
    \frac{\partial} {\partial \rho_k}  L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \rho, \alpha)  
    &amp;= \sum_{1\le i&lt;j\le L}  \, \sum_{k&#39;=0}^K  p(k&#39;|ij) \, \alpha_k \, \left(  I(k&#39; \ge k)  -  \sum_{k&#39;&#39;=k}^K  g_{k&#39;&#39;}(\cij) \right)  \nonumber \\
    &amp;= \alpha_k \, \sum_{1\le i&lt;j\le L}  \, \left( \sum_{k&#39;=k}^K  p(k&#39;|ij)   -  \sum _{k&#39;=0}^K  p(k&#39;|ij) \, \sum_{k&#39;&#39;=k}^K  g_{k&#39;&#39;}(\cij)  \right) ,
\end{align}\]</span>
<p>and finally</p>
<span class="math display">\[\begin{equation}
    \frac{\partial} {\partial \rho_k}  L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \rho, \alpha)  = \alpha_k \, \sum_{1\le i&lt;j\le L}  \, \sum_{k&#39;=k}^K  ( p(k&#39;|ij) - g_{k&#39;}(\cij) )  \, .
\end{equation}\]</span>
<p>This equation has an intuitive meaning: The gradient is the difference between the summed probability mass predicted to be due to components <span class="math inline">\(k&#39; \ge k\)</span>, <span class="math inline">\(p(k&#39;\ge k | ij)\)</span>, and the sum of the prior probabilities <span class="math inline">\(g_k(\cij)\)</span> for components <span class="math inline">\(k&#39; \ge k\)</span>, where the sum runs over all training points indexed by <span class="math inline">\(i,j\)</span>.</p>
</div>
<div id="the-derivative-of-the-log-likelihood-with-respect-to-alpha_k" class="section level4">
<h4><span class="header-section-number">6.5.11.2</span> The derivative of the log likelihood with respect to <span class="math inline">\(\alpha_k\)</span></h4>
<p>Last and similar to the previous derivation, the partial derivative with respect to <span class="math inline">\(\alpha_k\)</span> is</p>
<span class="math display" id="eq:nabla-alphak-LLpre">\[\begin{equation}
  \frac{\partial} {\partial \alpha_k} L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \rho, \alpha) = \sum_{1\le i&lt;j\le L}  \, \sum_{k&#39;=0}^K  p(k&#39;|ij) \,  \frac{\partial} {\partial \alpha_k}  \log g_{k&#39;}(\cij)  \,.
    \tag{6.29}
\end{equation}\]</span>
<p>Similarly as before,</p>
<span class="math display" id="eq:dlog-gk-dalpha">\[\begin{align}
    \frac{\partial} {\partial \alpha_k}  \log g_{l}(\cij)  
    &amp;= \frac{\partial} {\partial \alpha_k}  \log  \frac{\exp (- \sum_{k&#39;&#39;=1}^{l} \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) }{ \sum_{k&#39;=0}^K  \exp (- \sum_{k&#39;&#39;=1}^{k&#39;} \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) ) } \nonumber \\
    &amp;= -  \frac{\partial} {\partial \alpha_k}  \sum_{k&#39;&#39;=1}^{l} \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} )  
        - \frac{\partial} {\partial \alpha_k}  \log  \sum_{k&#39;=0}^K  \exp \left(- \sum_{k&#39;&#39;=1}^{k&#39;} \, \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) \right)  \nonumber \\
    &amp;= - (\cij - \rho_{k} ) \, I(l \ge k) -  \frac{ \sum_{k&#39;=0}^K \frac{\partial} {\partial \alpha_k}  \exp (- \sum_{k&#39;&#39;=1}^{k&#39;} \, \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) ) }{ \sum_{k&#39;=0}^K  \exp (- \sum_{k&#39;&#39;=1}^{k&#39;} \alpha_{k&#39;&#39;} (\cij - \rho_{k&#39;&#39;} ) ) } \nonumber \\
    &amp;= - (\cij - \rho_{k} )  \, \left(  I(l \ge k)  -  \sum_{k&#39;&#39;=k}^K  g_{k&#39;&#39;}(\cij) )  \right)  \, .
    \tag{6.30}
\end{align}\]</span>
<p>Inserting this into eq. <a href="theoretical-methods-1.html#eq:nabla-alphak-LLpre">(6.29)</a> yields</p>
<span class="math display">\[\begin{align}
    \frac{\partial} {\partial \alpha_k}  L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \rho, \alpha) 
    &amp;= - \sum_{1\le i&lt;j\le L}  \, \sum_{k&#39;=0}^K  p(k&#39;|ij) \, (\cij - \rho_k) \, \left(  I(k&#39; \ge k)  -  \sum_{k&#39;&#39;=k}^K  g_{k&#39;&#39;}(\cij) )  \right)  \nonumber \\
    &amp;= -\sum_{1\le i&lt;j\le L}  \, (\cij - \rho_k) \, \left( \sum_{k&#39;=k}^K  p(k&#39;|ij)   -  \sum _{k&#39;=0}^K  p(k&#39;|ij) \, \sum_{k&#39;&#39;=k}^K  g_{k&#39;&#39;}(\cij) )  \right) ,
\end{align}\]</span>
<p>and finally</p>
<span class="math display">\[\begin{equation}
    \frac{\partial} {\partial \alpha_k}  L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \rho, \alpha)     = \sum_{1\le i&lt;j\le L}  \, (\rho_k - \cij) \, \sum_{k&#39;=k}^K  ( p(k&#39;|ij) -  g_{k&#39;}(\cij) )  \, .
\end{equation}\]</span>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Murphy2012">
<p>94. Murphy, K.P. (2012). Machine Learning: A Probabilistic Perspective (MIT Press).</p>
</div>
</div>
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://contactpredictionthesis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="training-on-couplings-from-pseudo-likelihood-maximization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="practical-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/susannvorberg/phd_thesis/edit/master/07-posterior-distribution.Rmd",
"text": "Edit"
},
"download": ["PhD_thesis_Susann_Vorberg.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
