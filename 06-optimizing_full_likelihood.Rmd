# Optimizing the Full Likelihood {#optimizing-full-likelihood}

Section \@ref(maxent) introduced the *Potts model* for contact prediction that is able to distinguish between directly and indirectly coupled residue pairs by jointly modelling the probabilty of a protein sequence over all residues.
Maximum-likelihood inference of the model parameters is numerically challenging due to the exponential complexity of the partition function that normalizes the probability distribution. 
Several approximate inference techniques for the full likelihood have been developed trying to sidestep the exact computation of the partition function.
At this point in time, pseudo-likelihood is the most successful approximate solution with regard to the specific problem of predicting residue-residue contacts (see section \@ref(pseudo-likelihood)).
It has been shown that the pseudo-likelihood is a consistent estimator to the full likelihood in the limit of large amounts of data.
However, it is unclear whether it represents a good approximation when there is only little data, in other words for small protein families that are the most interesting targets for contact prediction (see Figure \@ref(fig:pfam)).

While the partition function of the full likelihood cannot be efficiently computed, it is possible to approximate the gradient of the full likelihood with an approach called *contrastive divergence* that makes use of [MCMC](#abbrev) sampling techniques [@Hinton2002]. 
This section elaborates on how *contrastive divergence* can be used to optimize the full likelihood with gradient descent techniques.
Furthermore, two aspects of the underlying *Potts model*, namely gap treatment and the choice of regularization, have been refined which is explained in detail in methods section \@ref(potts-full-likelihood). 


## Approximating the Gradient of the Full Likelihood with Contrastive Divergence {#full-likelihood-gradient}

The gradient of the regularized full log likelihood with respect to the couplings $\wijab$ can be written as
\begin{equation}
    \frac{\partial \LLreg}{\partial \wijab} = \; N_{ij} q(x_i \eq a, x_j=b) - N_{ij} \; p(x_i \eq a, x_j \eq b | \v,\w) - \lambda_w \wijab  \; ,
(#eq:gradient-wijab-full-likelihood-approx)
\end{equation}
where $N_{ij} q(x_i \eq a, x_j=b)$ are the empirical pairwise amino acid counts, $p(x_i \eq a, x_j \eq b | \v,\w)$ corresponds to the marginal distribution of the *Potts model* and $\lambda_w \wijab$ is the partial derivative of the L2-regularizer used to constrain the couplings $\w$.
The empirical amino acid counts are constant and need to be computed only once from the alignment.
The model probability term cannot be computed analytically as it involves the partition function that has exponential complexity. 


[MCMC](#abbrev) algorithms are predominantlyused in Bayesian statistics to generate samples from probability distributions that involve the computation of complex integrals and therefore cannot be computed analytically [@Andrieu2003; @Murphy2012]. 
Samples are generated from a probability distribution as the current state of a running Markov chain. 
If the Markov chain is run long enough, the equilibrium statistics of the samples will be identical to the true probability distribution statistics.
In 2002, Lapedes et al. applied [MCMC](#abbrev) sampling to approximate the probability terms in the gradient of the full likelihood [@Lapedes2012a].
They obtained sequence samples from a Markov chain that was run for 4,000,000 steps by keeping every tenth configuration of the chain.
Optimization converged after 10,000 - 15,000 epochs when the gradient had become zero.
The expected amino acid counts according to the model distribution, $N_{ij} \; p(x_i \eq a, x_j \eq b | \v,\w)$, were estimated from the generated samples.
Their approach was successfull but is computationally feasible only for small proteins and points out the limits of applying [MCMC](#abbrev) algorithms.
Typically, they require many sampling steps to obtain unbiased estimates from the stationary distribution which comes at high computational costs.

In 2002, Hinton invented [CD](#abbrev) as an approximation to [MCMC](#abbrev) methods [@Hinton2002].
It was originally developed for training products of experts models but it can generally be applied to maximizing log likelihoods and has become overly popular for training restricted Boltzmann machines [@Fischer2012; @Murphy2012; @Bengio2009]. 
The idea is simple: instead of starting a Markov chain from a random point and running it until it has reached the stationary distribution, it is initialized with a data sample and evolved for only a small number of steps.
Obviously the chain has not yet converged to its stationary distribution and the data sample obtained from the current configuration of the chain presents a biased estimate.
The intuition behind [CD](#abbrev) is that eventhough the gradient estimate is very noisy and biased, it points roughly into a similar direction as the true gradient of the full likelihood.
Therefore the approximate [CD](#abbrev) gradient should become zero approximately where the true gradient of the likelihood becomes zero.
Once the parameters are close to the optimum, starting a Gibbs chain from a data sample should reproduce the empirical distribution and not lead away from it, because the parameters already describe the empirical distribution correctly.

The approximation of the full likelihood gradient with [CD](#abbrev) is visualized in Figure \@ref(fig:gibbs-sampling).
$N$ Markov chains will be initialized with the $N$ sequences from the [MSA](#abbrev) and $N$ new samples will be generated by a single step of Gibbs sampling from each of the $N$ sequences.
One full step of Gibbs sampling will update every sequence position $i \in \{1, \ldots, L\}$ subsequently by randomly selecting an amino acid based on the conditional probabilities for observing an amino acid $a$ at position $i$ given the model parameters and all other sequence positions:
\begin{equation}
  p(\seq_i = a | (x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_L), \v, \w) \propto \exp \left( \vi(a) + \sum_{j=1; i \ne j}^L \wij(a, x_j)  \right)
(#eq:conditional-prob-full-likelihood)
\end{equation}
The generated samples are then used to compute the pairwise amino acid frequencies that correspond to rough estimates of the marginal probabilities from the model.
Finally, an approximate gradient of the full likelihood is obtained by subtracting the empirical and sampled amino acid counts as denoted in eq. \@ref(eq:gradient-wijab-full-likelihood-approx).
 
(ref:caption-gibbs-sampling) Approximating the full likelihood gradient with [CD](#abbrev). Pairwise amino acid counts are computed from the observed sequences of the input alignment shown in red on the left. 
Expected amino acid frequencies according to the model distribution are computed from a sampled alignment shown in blue on the right. The [CD](#abbrev) approximation of the likelihood gradient is obtained by computing the difference in amino acid counts of the observed and sampled alignment. A Markov chain is initialized with an observed sequence and a newly sampled sequence is generated by evolving the Markov chain for one full Gibbs step. The Gibbs step involves updating every position in the sequence (unless it is a gap) according to the conditional probabilities for this position. 

```{r gibbs-sampling, echo = FALSE, out.width = '90%', fig.align='center',  fig.cap = '(ref:caption-gibbs-sampling)'}
knitr::include_graphics("img/full_likelihood/gibbs_sampling.png")
```


It has been shown that taking more than one Gibbs steps gives more precise results.
Taking more Gibbs steps reduces the bias of the gradient estimate as the Markov chain approaches its stationary distribution. 
Whern running the Markov chain infinitely long the [CD](#abbrev) estimate converges to the actual [MLE](#abbrev) estimate.
Surprising results have also been obtained by taking only one step [@Bengio2009].
Several connections have been found to other approximation schemes.
For example, it can be shown that [CD](#abbrev) with Gibbs sampling only one variable is exactly equivalent to optimising the pseudo-likelihood [@Hyvarinen2006; @Hyvarinen2007].

[PCD](#abbrev) is a variation of [CD](#abbrev) such that the Markov chain is not reinitialized at a data sample every time a new gradient is computed.
Instead the Markov chains are evolved between successive gradient computations without resetting them.
The fundamental idea behind [PCD](#abbrev) when using small learning rates is that the model changes only slightly between parameter updates and the chains stay close to the stationary distribution
PCD can explore the distribution further and as the optimum is approached the approximation to the gradient shoul dbecome better. 

- tieleman showed improved convergence properties
- PCD often works better than CD (see e.g., (Tieleman 2008; Marlin et al. 2010; Swersky et al.
2010)), although CD can be faster in the early stages of learning





## Optimizing the Full Likelihood {#full-likelihood-optimization}

Given the likelihood gradient estimates obtained with [CD](#abbrev), the full negative log likelihood can now be minimized using a gradient descent optimization algorithm.
Gradient descent algorithms are used to find the minimum of an objective function with respect to its parametrization by iteratively updating the parameters values in the opposite direction of the gradient of the objective function with respect to these parameters.
[SGD](#abbrev) is a variant thereof that uses an stochastic estimate of the gradient whose average over many updates approaches the true gradient.
The stochasticity is commonly obtained by evaluating a random subsample of the data at each iteration. 
For [CD](#abbrev) stochasticity additionally arises from the Gibbs sampling process in order to obtain a gradient estimate in the first place.

As a consequence of stochasticity, the gradient estimates are noisy, resulting in parameter updates with high variance and strong fluctuations of the objective function.
These fluctuations enable stochastic gradient descent to escape local minima but also complicate finding the exact minimum of the objective function. 
By slowly decreasing the step size of the parameter updates at every iteration, stochastic gradient descent most likely will converge to the global minimum for convex objective functions [@Ruder2017; @Bottou2012; @Bottou2010].
However, choosing an optimal step size for parameter updates as well as finding the optimal annealing schedule offers a challenge and needs manual tuning [@Schaul2013; @Zeiler2012]. 
If the step size is chosen too small, progress will be unnecessarily slow, if it is chosen too large, the optimum will be overshot and can cause the system to diverge (see Figure \@ref(fig:gd-learning-rate-intro)).
Further complications arise from the fact that different parameters often require different optimal step sizes, because the magnitude of gradients might vary considerably for different parameters, e.g. because of sparse data.

(ref:caption-gd-learning-rate-intro) Visualization of gradient descent optimization of an objective function $L(w)$ for different step sizes $\alpha$. The blue dot marks the minimum of the objective function. The direction of the gradient at the initial parameter estimate $w_0$ is given as black arrow. The updated parameter estimate $w_1$ is obtained by taking a step of size $\alpha$ into the opposite direction of the gradient. **Left** If the step size is too small the algorithm will require too many iterations to converge. **Right** If the step size is too large, gradient descent will overshoot the minimum and can cause the system to diverge.

```{r gd-learning-rate-intro, echo = FALSE, out.width = '80%', fig.align='center', fig.cap = '(ref:caption-gd-learning-rate-intro)'}
knitr::include_graphics("img/full_likelihood/intro.png")
```


Unfortunately, it is neither possible to use second order optimization algorithms nor sophisticated first order algorithms like conjugate gradients to optimize the full likelihood.
While the former class of algorithms requires (approximate) computation of the second partial derivatives, the latter requires evaluating the objective function in order to identify the optimal step size via linesearch, both being computationally too demanding.

Method section \@ref(methods-sgd) describes in detail the hyperparameter tuning for stochastic gradient descent. 
It covers the choice of the convergence criterion and finding the optimal learning rate annealing schedule with a learning rate that is defined as a function of the effective number of sequences [Neff](#abbrev).
Furthermore, the regularization coefficient $\lambda_w$ determining the strength of the L2-regularizer constraining the couplings, has been tuned as described in method section \@ref(regularization-for-cd-with-sgd).
The next sections discussing various modifications of the Gibbs sampler, will use stochastic gradient descent with the tuned hyperparameters to optimize the [CD](#abbrev) objective. 

There exist many variants of stochastic gradient descent algorithms that deal with the aforementioned challenges e.g. speeding up convergence rates using momentum or defining adaptive learning rates for each parameter [@Ruder2017]. 
One of these [SGD](#abbrev) variants is Adaptive Moment Estimation (*ADAM*) [@Kingma2014], an algorithm that computes per-parameter learning rates including momentum (see methods section \@ref(methods-full-likelihood-adam) for details). 
A major advantage of *ADAM* over pure [SGD](#abbrev) is that it does not require tuning many hyperparameters as the default values have been found to work quite well. 
*ADAM* will be compared to the manually tuned [SGD](#abbrev) optimizer in section \@ref(adam-results).


### Varying the Sample Size {#cd-sampling-size}

The default Gibbs sampling scheme explained in method section \@(cd-sampling-optimization) involves the random selection of $10L$ sequences from the input alignment, with $L$ being protein length, at every iteration of the optimization procedure.
These sequences are used to initialize the Markov chains for Gibbs sampling new sequences to estimate the gradient with [CD](#abbrev). 
the particular choice of $10L$ sequences was motivated by the fact that there is a relationship between the precision of contacts predicted from pseudo-likelihood and protein length as long as the alignment has less than $10^3$ diverse sequences [@Anishchenko2017].
It has been argued that roughly $5L$ nonredundant sequences are required to obtain confident predictions that can bet used for protein structure prediction [@Kamisetty2013].

I analysed whether varying the number of sequences used for the approximation of the gradient via Gibbs sampling affects performance.
Randomly selecting only a subset of sequences $S$ from the $N$ sequences of the input alignment corresponds to the stochastic gradient descent idea of a minibatch and introduces additional stochasticity over the [CD](#abbrev) Gibbs sampling process. 
Using $S < N$ sequences for Gibbs sampling has the further advantage of decreasing the runtime at each iteration. 
I evaluated different schemes for randomly selecting sequences for Gibbs sampling at every iteration of the optimization:

- sampling $x \cdot L$ sequences with $x \in \{ 1, 5, 10, 50 \}$ without replacement enforcing $S \eq \min(N, xL)$
- sampling $x \cdot N_{\textrm{eff}}$ sequences with $x \in \{ 0.2, 0.3, 0.4 \}$ without replacement

As can be seen in Figure \@ref(fig:cd-performance-samplesize), randomly selecting $L$ sequences for sampling, results in a visible drop in performance. Using $5L$ sequences for sampling results in slighlty decreased performance over using $10L$ or $50L$ sequences. There is no benefit in using more than $10L$ sequences, especially as sampling more sequences increases runtime per iteration.
Specifying the number of sequences for sampling as fractions of [Neff](#abbrev) generally improves precision slightly over selecting $10L$ or $50L$ sequences for sampling.
And by sampling $0.3N_{\textrm{eff}}$ and $0.4N_{\textrm{eff}}$ sequences, [CD](#abbrev) does even slighty improve over pseudo-likelihood.

(ref:caption-cd-performance-samplesize) Mean precision for top ranked contact predictions over 288 proteins. Contact scores are computed as the [APC](#abbrev) corrected Frobenius norm of the couplings $\wij$. **pseudo-likelihood**:  couplings computed with pseudo-likelihood.  **CD sample size = X **: contact scores computed from [CD](#abbrev) with [SGD](#abbrev) and varying number of sample size as specified in the legend. Sample size refers to the number of randomly selected sequences for Gibbs sampling. It is defined either as multiples of protein length $L$ or as fraction of the effective number of sequences [Neff](#abbrev). 

```{r cd-performance-samplesize, echo = FALSE, screenshot.alt="img/full_likelihood/gibbs_sampling/precision_vs_rank_samplesize.png", out.width = '100%', fig.align='center', fig.cap = '(ref:caption-cd-performance-samplesize)'}
knitr::include_url("img/full_likelihood/gibbs_sampling/precision_vs_rank_samplesize.html", height = "500px")
```

When evaluating performance with respect to the number of effective sequences [Neff](#abbrev) (see Figure \@ref(fig:cd-precision-sampling-size-neff)) it can clearly be noted that the optimal samplings size must depend on [Neff](#abbrev). 
Selecting too many sequences, e.g. $50L$ for small alignments (upper left plot in Figure \@ref(fig:cd-precision-sampling-size-neff)), or selecting too few sequences, e.g $1L$ for big alignments (lower right plot in Figure \@ref(fig:cd-precision-sampling-size-neff)), results in a decrease in precision compared to defining sampling size as fractions of [Neff](#abbrev).
Especially small alignments benefit from sampling sizes defined as a fraction of [Neff](#abbrev)  with improvements of about three percentage points in precision over pseudo-likelihood.

(ref:caption-cd-precision-sampling-size-neff) Mean precision for top ranked contact predictions over 300 proteins. Contact scores are computed as the [APC](#abbrev) corrected Frobenius norm of the couplings $\wij$. **pseudo-likelihood**:  contact scores computed from pseudo-likelihood. **CD sample size = X**: contact scores computed from [CD](#abbrev) optimized with [SGD](#abbrev) and varying number of sample size as specified in the legend. Sample size refers to the number of randomly selected sequences for Gibbs sampling. It is defined either as multiples of protein length $L$ or as fraction of the effective number of sequences [Neff](#abbrev). 

```{r cd-precision-sampling-size-neff, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = '(ref:caption-cd-precision-sampling-size-neff)'}
knitr::include_url("img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.html", height = "600px")
```

To understand the effect of different choices of sample size it is necessary to look at single proteins. 
The left plot in Figure \@ref(fig:cd-samplesize-protein1c75a00) shows the development of the L2 norm of the gradient $||\nabla_{\w}||_2$ for protein chain 1c75_A_00 that is of length 71 and has [Neff](#abbrev) = 16808.
The norm of the gradient decreases during optimization and saturates at decreasing levels for increasing choices of sample size.
Increasing the sample size by a factor 100 (from $L$ to $100L$) leads to an approximately 10-fold reduction of the norm of gradients ($\mathrm{1.4e}{+5}$ compared to $\mathrm{1.45e}{+4}$) at convergence, which corresponds to a typical reduction of statistical noise as the square root of the number of samples.
It is not feasible to sample the number of sequences at each iteration that would be necessary to reduce the norm of the gradient to near zero. 
In any case, precision of the top ranked contacts does not improve to the same amount as the norm of the gradient decreases when using larger sample sizes as could be seen in the previous benchmark.
Probably, the improved gradient when using a larger sample size helps to finetune the parameters, which only has a negligible effect on the contact score computed as [APC](#abbrev) corrected Frobenius norm of the couplings $\wij$.
The right plot in Figure \@ref(fig:cd-samplesize-protein1c75a00) shows the development of the L2 norm of coupling parameters $||\w||_2$ over optimization.
The norm of the coupling parameters is almost indistinguishable for the sample size choices $50L$, $100L$ and $0.2 - 0.4$Neff. 
Furthermore, bias originating from the [CD](#abbrev) procedure by using only 1 step of Gibbs sampling might present an even stronger obstacle for improvement in precision than insufficient sample size.

It is not clear, why a higher sample size and consequently an improved gradient results in weaker performance for proteins with small alignments as could be seen in the previous benchmark in Figure \@ref(fig:cd-precision-sampling-size-neff). 
Appendix Figure \@ref(fig:cd-samplesize-protein1ahoa00) shows the development of the norm of the gradient and the norm of coupling parameters for protein chain 1aho_A_00 of length 64 and with 378 sequences ([Neff](#abbrev)=229). 
As before, the gradient improves when more sequences are used in the Gibbs sampling process and therefore should lead to a better approximation of the likelihood.
However, setting $S=10L$ or $S=50L$ which corresponds to using all $N \eq 378$ sequences for approximation of the gradient results in a mean precision over the top $0.1L$ - $L$ contacts of 0.44, whereas using only $0.3N_{\textrm{eff}} \eq 69$ sequences gives a mean precision of 0.62.
One explanation could be that this is some effect of overfitting, eventhough a regularizer is used and the norm of coupling parameters actually is smaller when using higher sampling sizes. 

(ref:caption-cd-samplesize-protein1c75a00) Monitoring parameter norm and gradient norm for protein 1c75_A_00 during [SGD](#abbrev) using different sample sizes. Protein 1c75_A_00 has length L=71 and 28078 sequences in the alignment ([Neff](#abbrev)=16808) **Left** L2-norm of the gradients for coupling parameters $||\w||_2$ (without contrbution of regularizer). The number of sequences, that is used for Gibbs sampling to approximate the gradient, is given in the legend. **Right** L2-norm of the coupling parameters $||\w||_2$. The number of sequences, that is used for Gibbs sampling to approximate the gradient, is given in the legend. 

```{r cd-samplesize-protein1c75a00, echo = FALSE, out.width = '50%', fig.align='center', fig.show='hold', fig.cap = '(ref:caption-cd-samplesize-protein1c75a00)'}
knitr::include_graphics(c("img/full_likelihood/gibbs_sampling/1c75A00_gradient_norm_for_samplesizes.png", "img/full_likelihood/gibbs_sampling/1c75A00_parameter_norm_for_samplesizes.png"))
```

## Using ADAM to optimize Contrastive Divergence {#adam-results}

- violates sum_wijab condition, sum_wijab could be used as convergence criterium but practice shows that it is esp hard to reach for huge Neff, L?
  use change in parameter values as criteria instead

## Comparing CD couplings to pLL couplings




Boxplots for couplings wijab: 
sieht ma nicht viel

```{r comapring-cd-pll-boxplot1, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = 'dideldum'}
knitr::include_url("img/full_likelihood/comparing_couplings/boxplot_for_1c75A00_method1_pseudo-likelihood_method2_contrastivedivergence_score.html", height = "600px")
```

Boxplots for l2norm over couplings ||wij||_2: 
contrastive divergence hat kleineren scores
signifikant?


```{r comapring-cd-pll-boxplot2, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = 'dideldum'}
knitr::include_url("img/full_likelihood/comparing_couplings/boxplot_for_1c75A00_method1_pseudo-likelihood_method2_contrastivedivergence_score_l2norm_apcFalse.html", height = "600px")
```

Boxplots for l2norm-apc over couplings ||wij||_2 - apc: 
auch mit apc: es gibt weniger starke scores mit CD
signifikant?

scipy.stats.wilcoxon(l2norm_apc_pll, l2norm_apc_cd)
#WilcoxonResult(statistic=187035.0, pvalue=0.029710790280912919)

scipy.stats.ranksums(l2norm_apc_pll, l2norm_apc_cd)
#RanksumsResult(statistic=0.38719674947266086, pvalue=0.69861055638055758)

scipy.stats.kendalltau(l2norm_apc_pll, l2norm_apc_cd)
#KendalltauResult(correlation=0.76655297812416368, pvalue=1.3239009316846667e-260)

scipy.stats.spearmanr(l2norm_apc_pll, l2norm_apc_cd)
#SpearmanrResult(correlation=0.92532449605319178, pvalue=0.0)


```{r comapring-cd-pll-boxplot3, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = 'dideldum'}
knitr::include_url("img/full_likelihood/comparing_couplings/boxplot_for_1c75A00_method1_pseudo-likelihood_method2_contrastivedivergence_score_l2norm_apcTrue.html", height = "600px")
```


Scatter Plots for couplings wijab:
nice correlation - pearson correlation?
gibt 3 outlier und das sind auch noch die staerksten couplings
hier sieht man besser als beim boxplot, das CD kleinere couplings hat (absolut)

```{r comapring-cd-pll-scatter1, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = 'dideldum'}
knitr::include_url("img/full_likelihood/comparing_couplings/couplings_for_1c75A00_method1_pseudo-likelihood_method2_contrastivedivergence.html", height = "600px")
```

Scatter for l2norm over couplings ||wij||_2: 
hier sieht man sehr schoen, dass CD systematisch kleinere scores hat als pLL
und man sieht die drei outlier

```{r comparing-cd-pll-scatter2, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = 'dideldum'}
knitr::include_url("img/full_likelihood/comparing_couplings/couplings_for_1c75A00_method1_pseudo-likelihood_method2_contrastivedivergence_l2norm_apcFalse.html", height = "600px")
```

Scatter for l2norm-apc over couplings ||wij||_2 - apc: 
das gleiche mit apc:
systematisch kleinere scores (absolut)

```{r comparing-cd-pll-scatter3, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = 'dideldum'}
knitr::include_url("img/full_likelihood/comparing_couplings/couplings_for_1c75A00_method1_pseudo-likelihood_method2_contrastivedivergence_l2norm_apcTrue.html", height = "600px")
```


Comparing the ranking:
geiler plot
ranking ist sehr aehnlich, besonders fuer top contacts
nur diese drei outlier sind bei CD sehr viel hoeher gerankt als bei pLL

```{r comparing-cd-pll-ranking, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = 'dideldum'}
knitr::include_url("img/full_likelihood/comparing_couplings/comparative_top_ranked_contacts_for_1c75A00_method1_pseudo-likelihood_method2_contrastive divergence.html", height = "600px")
```


Showing the contact maps:
pLL

```{r contact-map-1c75a00-pll, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = 'dideldum'}
knitr::include_url("img/full_likelihood/comparing_couplings/1c75A00.filt.braw_seqsep8_contacthr8_apcTrue_pll.html", height = "600px")
```

CD
```{r contact-map-1c75a00-cd, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = 'dideldum'}
knitr::include_url("img/full_likelihood/comparing_couplings/1c75A00.filt.braw_seqsep8_contacthr8_apcTrue_cd.html", height = "600px")
```


Showing precision:

```{r precision-vs-rank-1c75a00, echo = FALSE, screenshot.alt="img/full_likelihood/appendix/precision_vs_rank_facetted_by_neff_samplesize.png", out.width='100%', fig.align='center', fig.cap = 'dideldum'}
knitr::include_url("img/full_likelihood/comparing_couplings/1c75A00_precision_vs_rank_8seqsep_8contacthr.html", height = "600px")
```



- convergence criterion: mention paper [@Mahsereci2017]


