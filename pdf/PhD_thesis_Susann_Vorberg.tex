\documentclass[12pt,a4paper,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{acronym}
\usepackage[bookmarks=true,colorlinks=true,urlcolor=blue,citecolor=black,linkcolor=black,unicode=true]{hyperref}
\usepackage[top=2.5cm, bottom=2.5cm]{geometry}




\usepackage{natbib}
\bibliographystyle{plainnat}


\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


% Numbering of sections unto depth=5
\setcounter{secnumdepth}{5}

% Table of contents formatting
\renewcommand{\contentsname}{Table of Contents}
\setcounter{tocdepth}{1}

% Headers and page numbering 
\usepackage{fancyhdr}
\pagestyle{plain}

% Chapter styling
\usepackage[grey]{quotchap}
\makeatletter 
\renewcommand*{\chapnumfont}{%
  \usefont{T1}{\@defaultcnfont}{b}{n}\fontsize{80}{100}\selectfont% Default: 100/130
  \color{chaptergrey}%
}
\makeatother


%------------------- Definition of LMU title pages
\newcommand{\LMUCover}[3]{
    \thispagestyle{empty}
    {\parindent0cm \rule{\linewidth}{.7ex}}
    
    \begin{flushright}
      \vspace*{\stretch{1}}
      \sffamily\bfseries\Huge
      #1\\
      \vspace*{\stretch{1}}
      \sffamily\bfseries\large
      #2
      \vspace*{\stretch{1}}
    \end{flushright}
  
    \rule{\linewidth}{.7ex}
    \vspace*{\stretch{5}}
    \vspace*{\stretch{1}}
    
    \begin{center}\sffamily\LARGE{#3}\end{center}
}



\newcommand{\LMUTitlePage}[4]{
    \thispagestyle{empty}
    \vspace*{\stretch{1}}
    
    \begin{center}
      \Large Dissertation zur Erlangung des Doktorgrades der Fakultät für Chemie und Pharmazie der Ludwig-Maximilians-Universität München
    \end{center}
    
    \vspace*{\stretch{1}}
    {\parindent0cm \rule{\linewidth}{.7ex}}
    
    \begin{flushright}
      \vspace*{\stretch{1}}
      \sffamily\bfseries\Huge
      #1\\
      \vspace*{\stretch{1}}
    \end{flushright}
  
    \rule{\linewidth}{.7ex}

    \vspace*{\stretch{3}}
    \begin{center}
      \Large vorgelegt von\\
      \Large #2\\
      \Large geboren in #3\\
      \vspace*{\stretch{2}}
      \Large München, den #4
    \end{center}
}


\newcommand{\LMUErklaerung}[5]{
    \thispagestyle{empty}
    \begin{flushleft}
      \large \textbf{Erklärung} \\[1mm]
      \large Diese Dissertation wurde im Sinne von §7 der Promotionsordnung vom 28. November 2011 von #2 betreut.
      \bigskip
  
      \large \textbf{Eidesstattliche Versicherung}\\[1mm]
      \large Diese Dissertation wurde eigenständig und ohne unerlaubte Hilfe erarbeitet.
      \vspace{5em}
  
      \dots\dots\dots   \dots\dots\dots \hfill \dots\dots\dots\dots\dots\dots\dots\dots\\
      \large Ort, Datum \hfill #1
      \vfill
  
  
      \large Dissertation eingereicht am: \hfill #4
      \bigskip
    
      \large Erstgutachter:  #2 \hfill \dots\dots\dots\dots\dots\dots\dots
      \bigskip
    
      \large Zweitgutachter: #3 \hfill \dots\dots\dots\dots\dots\dots\dots
      \bigskip
    
      \large Tag der mündlichen Prüfung: \hfill #5
    \end{flushleft}
}


\begin{document}

\frontmatter

%%% LMU cover page
\LMUCover
	{Bayesian Model for Prediction of Protein Residue-Residue Contacts}
	{Susann Vorberg}
	{15.10.2017}

\newpage
\thispagestyle{empty}
\cleardoublepage

%%% LMU title page
\LMUTitlePage
	{Bayesian Model for Prediction of Protein Residue-Residue Contacts}
	{Susann Vorberg}
	{Leipzig, Germany}
	{15.10.2017}

\newpage
\thispagestyle{empty}
\cleardoublepage

%%% LMU statement page
\frontmatter\setcounter{page}{1}
\LMUErklaerung
	{Susann Vorberg}
	{Dr. Johannes Soeding}
	{Prof. Dr. Julien Gagneur}
	{15.10.2017}
	{15.12.2017}

\newpage
\thispagestyle{empty}
\cleardoublepage


\chapter{Summary}\label{summary}

Awesome contact prediction project abstract

\chapter{Acknowledgements}\label{acknowledgements}

I thank the world.

\tableofcontents

\listoffigures
\addcontentsline{toc}{chapter}{List of Figures}

\listoftables
\addcontentsline{toc}{chapter}{List of Tables}

\mainmatter \setcounter{page}{1}

\chapter{Introduction}\label{introduction}

In his Nobel Prize speech in 1973 \citep{Anfinsen1973} Anfinsen
postulated one of the basic principles in molecular biology, which is
\textbf{known as} \texttt{Anfinsen\textquotesingle{}s\ dogma}: a
protein's native structure is only determined by its amino acid
sequence. Until now, this dogma has held true at least for globular
proteins.

Determining the sequence of a protein by sequencing its respective mRNA
or the gene itself has become ever easier, faster and cheaper. Early
sequencing projects targeted small bacterial genomes
(\textit{H.influenza} in 1995 \citep{Fleischmann1995}), then faced
larger organisms (\textit{Drosophila melanogaster} in 2000
\citep{Adams2000}) and ultimately aimed at sequencing the human genome,
which was published in 2001 \citep{Venter2001,Lander2001}. What first
seemed to be the end of the road was only just the starting point for
the development of high-throughput methods, called \ac{NGS} methods.
Massively parallel sequencing devices, like Roche's 454, Illumina's
sequencing by synthesis (SBS) technology--based instruments or ABI's
Solid came onto the market and \ac{NGS} proceeded at an unprecedented
pace \citep{Liu2012,Pareek2011}. Already \textit{Next-Next generation}
technologies are edging into the market allowing for sequencing of
single molecules \citep{Ku2013}.

Since protein structure leads to function, structural insights are of
uttermost importance. They are essential for a detailed understanding of
chemical reactions, regulatory processes and transport mechanisms. They
are fundamental for the design of drugs and antibiotics. Moreover
structural abnormalities can lead to misfolding and aggregation
potentially causing diseases so studying them is pathologically
relevant.

\section{Structure Determination}\label{structure-determination}

Development of methods for structure determination has not gone through
such tremendous innovations. X-ray crystallography and \ac{NMR}
spectroscopy have been the gold standard for structure determination for
many years. Both methods suffer from essential drawbacks in contrast to
\ac{NGS} methods: they are time consuming, costly, and yield ambiguous
models. In X-ray crystallography, the first and most problematic step is
the crystallization of the protein target. The crystallization
conditions vary for each protein and their determination requires
significant time and effort. It can take months to produce usable
crystals and certain proteins, for instance some membrane proteins,
might not even crystallize at all. When crystals have been generated,
they are irradiated with X-rays, producing a diffraction pattern from
which an electron density map can be computed. This electron density is
used to construct a protein model which is iteratively refined by
comparing the predicted diffraction pattern of the model to the actual
diffraction pattern (\autoref{fig:xray}). In \ac{NMR} spectroscopy,
soluble proteins are analysed for their nuclear magnetic resonances,
which are recorded with a \ac{NMR} spectrometer. They are compared to a
reference signal and this results in a measure called chemical shift.
Ensembles of structure models that are consistent with the data are
calculated from the chemical shifts (\autoref{fig:nmr}). There are
advantages and disadvantages to both methods. The main disadvantage of
\ac{NMR} concerns the limited size of proteins that this technique can
solve. X-ray crystallography cannot account for conformational
flexibility. The crystal of a protein only represents one maybe
non-native state of a protein. What both methods have in common is the
ambiguity of the produced models which represents the human
interpretation of the primary data.\citep{Petsko2009,Gu2009}

The difference in efficiency for determining sequences and structure
consequently results in what has been called the sequence-structure gap.
The UniProtKB \citep{TheUniProtConsortium2013}, the leading resource for
sequence information on the web, contains more than 40 million sequence
entries (as of 24 July 2013) \citep{StatisticsSP, statisticsT}. In
contrast, the \ac{PDB} \citep{Berman2000} which is the UniProt
counterpart for structural information, grows at a much slower rate and
currently contains about 90,000 experimentally solved protein structures
\autoref{fig:pdb_prot_growth}.

\section{Structure Prediction}\label{structure-prediction}

Despite the knowledge of Anfinsen's postulate, we are not able to
reliably predict the structure of a protein from its sequence alone.
Generally it is assumed that a protein folds into a unique, well-defined
native structure that is near the global free energy minimum
(\autoref{fig:folding_funnel}). Levinthal's paradox
\citep{Levinthal1969} describes the complexity of the folding process
towards this minimum. It stresses the problem that it is not possible
for a protein to exhaustively search the conformational space to get to
its native fold. Due to the ``combinatorial explosion'' of possible
conformations, an exhaustive search would take unreasonably long. Hence,
it is not a feasible approach for structure prediction to scan all
possible conformations. Different approaches have been developed over
time to overcome or elude this problem.

\subsection{Template based methods}\label{template-based-methods}

Homology Modelling is applied in the case where homologous proteins with
solved structures are available and can be identified via sequence
alignments. The underlying assumption for this strategy relates to the
fact that structure is more conserved than sequence. That is, if we find
a related protein that shows sufficient similarity on the sequence
level, we can safely infer that both proteins share a similar structure.
Homology Modelling is assumed to yield reliable results when query and
target protein share more than 30\% sequence similarity, depending on
the sequence length (\textit{safe homology zone}) \citep{Sander1991}.
For proteins sharing less than 30\% sequence similarity, the structure
will not necessarily be conserved. Within this \textit{twilight zone} of
homology modelling the number of false positives regarding structural
similarity explodes \citep{Rost1999} and other structure prediction
methods have to be applied (see
\autoref{fig:schneider_sander_HM_threshold}). When a suitable target
structure has been identified, the backbone of the model is generated by
simply copying the coordinates of the target backbone atoms onto the
model. Non-aligned residues due to gaps in the alignment have to be
modelled \textit{de-novo}, meaning from scratch. This can be done by a
knowledge-based search for suitable fragments in the PDB or by true
energy-based \textit{de-novo} modelling. When the backbone is generated,
the side chains are modelled, usually by searching rotamer libraries for
energetically favoured residue conformations. Finally, the model is
energetically optimized in an iterative procedure. Force fields are
applied to correct the backbone and side chain conformations
\citep{Gu2009}. By now, many automated homology modelling servers are
well-established (Modeller \citep{Eswar2007}, 3D-Jigsaw
\citep{Bates2001}, SwissModel \citep{Arnold2006}) which allow more or
less manual intervention in the modelling process.

Homology modeling is by far the most successful approach to structure
prediction. It is applied in the case where homologous proteins with
solved structures are available and can be identified via sequence
alignments. The underlying assumption for this strategy relates to the
fact that structure is more conserved than sequence. That is, if we find
a related protein that shows sufficient similarity on the sequence
level, we can safely infer that both proteins share a similar structure.
Several automated homology modelling servers are well-established ,
e.g.~Modeller \citep{Eswar2007}, 3D-Jigsaw \citep{Bates2001} or
SwissModel \citep{Arnold2006}.

The limits of homology modeling lie in the identification of suitable
templates. As can be seen in \autoref{fig:pfam}, most protein families
have no known structure that can be used for homology modeling. In these
cases, other techniques, like fold recognition or ab initio predictions
might succeed.

Fold Recognition describes the inverse folding problem
\citep{Bowie1993}: instead of finding the compatible structure for a
given sequence, one tries to find sequences that fit onto a given
structure. Whether the query sequence fits a structure from the database
is not determined by sequence similarities but rather energetic or
environment specific measures. Thus, fold recognition methods are able
to recognize structural similarity even in the absence of sequence
similarity. The rationale basis for this strategy is the assumption that
the fold space is limited. It has been found that seemingly unrelated
proteins often adopt similar folds. This might be due to divergent
evolution (proteins are related, but homology cannot be detected at the
corresponding sequence level) or convergent evolution (functional
requirements lead to similar folds for unrelated proteins)
\citep{Gu2009}. Early approaches include profile based methods. Here,
the structural information of the protein is encoded into profiles,
which subsequently are aligned to the sequences
\citep{Bowie1991,Fischer1996,Ouzounis1993}. Advanced techniques are
known as ``threading'' techniques, describing the process of threading a
sequence through a structure and determining the optimal fit via energy
functions. \citep{Jones1992,Jones1998,Lemer1995}

Use a homologue protein structure as template. Only possible if a
homologue protein structure can be detected (usually via sequence
profile searches).

Threading techniques try to identify structural elements that fit to the
sequence.

\subsection{Template-free structure
prediction}\label{template-free-structure-prediction}

Ab initio or de-novo modeling techniques implement Anfinsen's Dogma most
closely in mimicking the folding process based only on physico-chemical
principles. Energy functions (physical or knowledge-based) are used to
describe the folding landscape and are minimized to arrive at the global
energy minimum corresponding to the native conformation. Since the
native conformation can be found near the global energy minimum of the
folding landscape, energy functions (physical or knowledge-based) have
been developed to describe this landscape. With respect to the idea of a
folding funnel, the energy function is minimized to mimic the folding
process that automatically leads to the global minimum. Again, there
exist numerous webservers that combine energy minimization, threading
techniques and fragment-based approaches, e.g.~Rosetta
\citep{Simons1999}, Tasser \citep{Zhang2004}, Touchstone II
\citep{Zhang2003}.

Drawbacks of these methods are the time requirements due to the
computational complexity of energy functions as well as their
inaccuracy.

Minimize a physical or knowledge-based energy function for the protein.
This has huge complexity due to large conformational space that needs to
be sampled.

\subsection{contact assisted denovo
predictions}\label{contact-assisted-denovo-predictions}

Structure Reconstruction from true contacts maps works well. Even a
small number of contacts is sufficient to reconstruct the fold of the
protein. Distance maps work even better.

What is the optimal distance cutoff to define a contact? Duarte et al
2010: between 8 and 12A Dyrka et al 2016 Konopka et al 2014 Sathyapriya
et al 2009

Many studies that successfuly predict structures denovo with the help of
predicted contact.

\section{Contact Prediction}\label{contact-prediction}

\subsection{Correlated mutations}\label{correlated-mutations}

contact prediction methods aim to identify correlated mutations from an
alignment of homologue protein sequences. main assumption is that two
interacting amino acid residues are coevolving: mutation of one of the
two residues can be compensated by mutation of the other residue

\subsection{Benchamrking methods}\label{benchamrking-methods}

\begin{itemize}
\item
  threshold for defining a contact: usually distance between \(C_\beta\)
  atoms (\(C_\alpha\) for Glycin) \textless{} 8 angstrom.
\item
  PPV: TP/(FP+TP) fraction of correct predictions among all predictions
\end{itemize}

\subsection{Pitfalls}\label{pitfalls}

Coevolution of residues can be mediated by molecules (e.g zinc ions) and
will not always imply spatial proximity in structure. Transitivity can
lead to correlation signals. Phylogenetic bias can also lead to
correlations. Sampling bias needs to be taken into account.

\section{State of the Art CP}\label{state-of-the-art-cp}

\subsection{Local methods}\label{local-methods}

MI and correlation measures suffer from transitivity of correlations

\subsection{Direct coupling analysis}\label{direct-coupling-analysis}

Can disentangle direct and indirect correlations.

Infer parameters of a maximum entropy model, more specifically a Potts
model (statistical physics) aka markov random fiels (computer science).
Likelihood function is convex, but Maximum Likelihood inference of model
is infeasible: Likelihood function needs to be reevaluated at each
iteration during optimization but partition function term sums over
20\^{}L sequences.

Many approximations: - mean field (Marks), Psicov - pseudo-likelihood -
belief-propagation (accurate but slow)

\subsection{Computing contact map from coupling
matrix}\label{computing-contact-map-from-coupling-matrix}

\begin{itemize}
\tightlist
\item
  direct information
\item
  frobenius norm
\end{itemize}

average product correction (also for MI)

(benchmark plot for localmethods + ccmpred)

\subsection{Meta-predictors}\label{meta-predictors}

\begin{itemize}
\tightlist
\item
  combining different approaches
\item
  jones et al: overlap between methods but also many unique predictions
\item
  machine learning methods incorporate sequence-derived features:
\item
  secondary structure predictions
\item
  solvent accessibilty
\item
  contact potentials
\item
  msa properties
\item
  pssms
\item
  physico-chemcial properties of amino acids
\end{itemize}

However, Meta-predictors will improve if basic methods improve.
Ultra-deep learning paper identifies coevolution features as crucial
feature.

\chapter{Methods}\label{methods}

all you need to know

\section{Dataset}\label{dataset}

\bibliography{book.bib}

\end{document}
