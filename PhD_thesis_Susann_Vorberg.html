<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>PhD thesis: residue-residue contact prediction</title>
  <meta name="description" content="This is my PhD thesis on residue-residue contact prediction.">
  <meta name="generator" content="bookdown <!--bookdown:version--> and GitBook 2.6.7">

  <meta property="og:title" content="PhD thesis: residue-residue contact prediction" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my PhD thesis on residue-residue contact prediction." />
  <meta name="github-repo" content="susannvorberg/phd_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PhD thesis: residue-residue contact prediction" />
  
  <meta name="twitter:description" content="This is my PhD thesis on residue-residue contact prediction." />
  

<meta name="author" content="Susann Vorberg">


<meta name="date" content="2017-09-06">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>

<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
  TeX: { 
    extensions: ["mediawiki-texvc.js"],
    Macros: {
      Cb: "C_\\beta",
      eq: "\\!=\\!",
      Gauss: "\\mathcal{N}",
      H: "\\mathbf{H}",
      Hij : "\\H_{ij}",
      I: "\\mathbf{I}",
      Lijk: "\\mathbf{\\Lambda}_{ij,k}",
      Lk: "\\mathbf{\\Lambda}_k",
      LLreg: "L\\!L_\\mathrm{reg}",
      muijk: "\\mathbf{\\mu}_{ij,k}",
      muk: "\\mathbf{\\mu}_k",
      neff: "N_\\mathrm{eff}",
      r: "\\mathbf{r}",
      rij: "r_{ij}",
      seq: "\\mathbf{x}",
      Qij: "\\mathbf{Q}_{ij}",
      q: "\\mathbf{q}",
      qij: "\\mathbf{q'}_{ij}",
      Sn: "\\mathcal{S}_n",
      v: "\\mathbf{v}",
      vi: "\\mathcal{v}_{i}",
      via: "\\mathcal{v}_{ia}",
      w: "\\mathbf{w}",
      wij: "\\mathbf{w}_{ij}",
      wijab: "\\mathcal{w}_{ijab}",
      wijcd: "\\mathcal{w}_{ijcd}",
      wklcd: "\\mathcal{w}_{klcd}",
      X: "\\mathbf{X}"
      }
  }
});
</script>

<!--
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
   extensions: ["[siunitx]/siunitx.js"]
 });
 MathJax.Ajax.config.path['siunitx']  = 'http://rawgit.com/burnpanck/MathJax-siunitx/master/';
 </script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { TagSide: "left" }
});
</script>
//-->



<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">PhD thesis: residue-residue contact prediction</h1>
<h4 class="author"><em>Susann Vorberg</em></h4>
<h4 class="date"><em>2017-09-06</em></h4>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#summary">Summary</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#general-intro"><span class="toc-section-number">1</span> Introduction</a><ul>
<li><a href="#protein-structure"><span class="toc-section-number">1.1</span> Protein Structure</a><ul>
<li><a href="#amino-acid-interactions"><span class="toc-section-number">1.1.1</span> Amino Acid Interactions</a></li>
</ul></li>
<li><a href="#structure-prediction"><span class="toc-section-number">1.2</span> Structure Prediction</a><ul>
<li><a href="#template-based-methods"><span class="toc-section-number">1.2.1</span> Template-based methods</a></li>
<li><a href="#template-free-structure-prediction"><span class="toc-section-number">1.2.2</span> Template-free structure prediction</a></li>
<li><a href="#contact-assisted-str-pred"><span class="toc-section-number">1.2.3</span> contact assisted de-novo predictions</a></li>
</ul></li>
<li><a href="#contact-prediction"><span class="toc-section-number">1.3</span> Contact Prediction</a><ul>
<li><a href="#local-methods"><span class="toc-section-number">1.3.1</span> Local Statistical Models</a></li>
<li><a href="#global-methods"><span class="toc-section-number">1.3.2</span> Global Statistical Models</a></li>
<li><a href="#machine-learnign-methods-and-meta-predictors"><span class="toc-section-number">1.3.3</span> Machine Learnign Methods and Meta-Predictors</a></li>
<li><a href="#intro-cp-evaluation"><span class="toc-section-number">1.3.4</span> Evaluating Contact Prediction Methods</a></li>
<li><a href="#maxent"><span class="toc-section-number">1.3.5</span> Maximum Entropy Modelling of Protein Families</a></li>
<li><a href="#challenges"><span class="toc-section-number">1.3.6</span> Challenges in Coevolutionary Inference</a></li>
</ul></li>
<li><a href="#developing-a-bayesian-model-for-contact-prediction"><span class="toc-section-number">1.4</span> Developing a Bayesian Model for Contact Prediction</a></li>
</ul></li>
<li><a href="#interpreting-coupling-matrices"><span class="toc-section-number">2</span> Interpretation of Coupling Matrices</a><ul>
<li><a href="#single-coupling-values-carry-evidence-of-contacts"><span class="toc-section-number">2.1</span> Single Coupling Values Carry Evidence of Contacts</a></li>
<li><a href="#physico-chemical-fingerprints-in-coupling-matrices"><span class="toc-section-number">2.2</span> Physico-Chemical Fingerprints in Coupling Matrices</a></li>
<li><a href="#coupling-profiles-vary-with-distance"><span class="toc-section-number">2.3</span> Coupling Profiles Vary with Distance</a></li>
<li><a href="#higher-order-dependencies-between-couplings"><span class="toc-section-number">2.4</span> Higher Order Dependencies Between Couplings</a></li>
</ul></li>
<li><a href="#optimizing-full-likelihood"><span class="toc-section-number">3</span> Optimizing the Full-Likelihood</a><ul>
<li><a href="#likelihood-of-the-sequences-as-a-potts-model"><span class="toc-section-number">3.1</span> Likelihood of the sequences as a Potts model</a></li>
<li><a href="#gap-treatment"><span class="toc-section-number">3.2</span> Treating Gaps as Missing Information</a></li>
<li><a href="#gauge-transformation"><span class="toc-section-number">3.3</span> Gauge transformation</a></li>
<li><a href="#the-regularized-log-likelihood-function-llregvw"><span class="toc-section-number">3.4</span> The regularized log likelihood function LLreg(v,w)</a></li>
<li><a href="#the-gradient-of-the-regularized-log-likelihood"><span class="toc-section-number">3.5</span> The gradient of the regularized log likelihood</a></li>
<li><a href="#prior-v"><span class="toc-section-number">3.6</span> The prior on <span class="math inline">\(\v\)</span></a><ul>
<li><a href="#full-likelihood"><span class="toc-section-number">3.6.1</span> Full-likelihood</a></li>
<li><a href="#likelihood-gradient"><span class="toc-section-number">3.6.2</span> Likelihood Gradient</a></li>
<li><a href="#contrastive-divergence"><span class="toc-section-number">3.6.3</span> Contrastive Divergence</a></li>
</ul></li>
</ul></li>
<li><a href="#a-bayesian-statistical-model-for-residue-residue-contact-prediction"><span class="toc-section-number">4</span> A Bayesian Statistical Model for Residue-Residue Contact Prediction</a><ul>
<li><a href="#overview-posterior-distances"><span class="toc-section-number">4.1</span> Computing the Posterior Distribution of Distances <span class="math inline">\(p(\r | \X)\)</span></a></li>
<li><a href="#coupling-prior"><span class="toc-section-number">4.2</span> Modelling the prior over couplings with dependence on <span class="math inline">\(\rij\)</span></a></li>
<li><a href="#laplace-approx"><span class="toc-section-number">4.3</span> Gaussian approximation to the posterior of couplings</a><ul>
<li><a href="#laplace-approx-improvement"><span class="toc-section-number">4.3.1</span> Iterative improvement of Laplace approximation</a></li>
</ul></li>
<li><a href="#likelihood-fct-distances"><span class="toc-section-number">4.4</span> Computing the likelihood function of distances <span class="math inline">\(p(\X | \r)\)</span></a></li>
<li><a href="#posterior-of-rij"><span class="toc-section-number">4.5</span> The posterior probability distribution for <span class="math inline">\(\rij\)</span></a></li>
</ul></li>
<li><a href="#contact-prior"><span class="toc-section-number">5</span> Contact Prior</a></li>
<li><a href="#methods"><span class="toc-section-number">6</span> Methods</a><ul>
<li><a href="#dataset"><span class="toc-section-number">6.1</span> Dataset</a></li>
<li><a href="#optimizing-pseudo-likelihood"><span class="toc-section-number">6.2</span> Optimizing Pseudo-Likelihood</a><ul>
<li><a href="#pseudo-likelihood-objective-function-and-its-gradients"><span class="toc-section-number">6.2.1</span> Pseudo-Likelihood Objective Function and its Gradients</a></li>
<li><a href="#diff-ccmpred-ccmpredpy"><span class="toc-section-number">6.2.2</span> Differences between CCMpred and CCMpredpy</a></li>
<li><a href="#seq-reweighting"><span class="toc-section-number">6.2.3</span> Sequence Reweighting</a></li>
<li><a href="#amino-acid-frequencies"><span class="toc-section-number">6.2.4</span> Computing Amino Acid Frequencies</a></li>
<li><a href="#regularization"><span class="toc-section-number">6.2.5</span> Regularization</a></li>
</ul></li>
<li><a href="#analysis-of-coupling-matrices"><span class="toc-section-number">6.3</span> Analysis of Coupling Matrices</a><ul>
<li><a href="#method-coupling-correlation"><span class="toc-section-number">6.3.1</span> Correlation of Couplings with Contact Class</a></li>
<li><a href="#method-coupling-profile"><span class="toc-section-number">6.3.2</span> Coupling Distribution Plots</a></li>
<li><a href="#bayesian-model-for-residue-resdiue-contact-prediction"><span class="toc-section-number">6.3.3</span> Bayesian Model for Residue-Resdiue Contact Prediction</a></li>
</ul></li>
<li><a href="#Hessian-offdiagonal"><span class="toc-section-number">6.4</span> Off-diagonal elements in <span class="math inline">\(\H\)</span></a></li>
<li><a href="#neg-Hessian-computation"><span class="toc-section-number">6.5</span> Efficiently Computing the negative Hessian of the regularized log-likelihood</a></li>
<li><a href="#inv-lambda-ij-k"><span class="toc-section-number">6.6</span> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></a></li>
<li><a href="#training-hyperparameters"><span class="toc-section-number">6.7</span> Training the Hyperparameters <span class="math inline">\(\muk\)</span>, <span class="math inline">\(\Lk\)</span> and <span class="math inline">\(\gamma_k\)</span></a><ul>
<li><a href="#the-gradient-of-the-log-likelihood-with-respect-to-mathbfmu"><span class="toc-section-number">6.7.1</span> The gradient of the log likelihood with respect to <span class="math inline">\(\mathbf{\mu}\)</span></a></li>
<li><a href="#the-gradient-of-the-log-likelihood-with-respect-to-lk"><span class="toc-section-number">6.7.2</span> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></a></li>
<li><a href="#the-gradient-of-the-log-likelihood-with-respect-to-gamma_k"><span class="toc-section-number">6.7.3</span> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></a></li>
</ul></li>
<li><a href="#bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances"><span class="toc-section-number">6.8</span> Bayesian Statistical Model for Prediction of Protein Residue-Residue Distances</a></li>
<li><a href="#modelling-the-dependence-of-wij-on-distance"><span class="toc-section-number">6.9</span> Modelling the dependence of <span class="math inline">\(\wij\)</span> on distance</a><ul>
<li><a href="#training-the-hyperparameters-rho_k-and-alpha_k-for-distance-dependent-prior"><span class="toc-section-number">6.9.1</span> Training the Hyperparameters <span class="math inline">\(\rho_k\)</span> and <span class="math inline">\(\alpha_k\)</span> for distance-dependent prior</a></li>
</ul></li>
</ul></li>
<li><a href="#appendix-appendix">(APPENDIX) Appendix</a></li>
<li><a href="#abbrev"><span class="toc-section-number">7</span> Abbreviations</a></li>
<li><a href="#dataset-properties"><span class="toc-section-number">8</span> Dataset Properties</a><ul>
<li><a href="#alignment-diversity"><span class="toc-section-number">8.1</span> Alignment Diversity</a></li>
<li><a href="#proportion-of-gaps-in-alignment"><span class="toc-section-number">8.2</span> Proportion of Gaps in Alignment</a></li>
<li><a href="#alignment-size-number-of-sequences"><span class="toc-section-number">8.3</span> Alignment Size (number of sequences)</a></li>
<li><a href="#protein-length"><span class="toc-section-number">8.4</span> Protein Length</a></li>
</ul></li>
<li><a href="#amino-acid-interaction-preferences-reflected-in-coupling-matrices"><span class="toc-section-number">9</span> Amino Acid Interaction Preferences Reflected in Coupling Matrices</a><ul>
<li><a href="#pi-cation"><span class="toc-section-number">9.1</span> Pi-Cation interactions</a></li>
<li><a href="#disulfide"><span class="toc-section-number">9.2</span> Disulfide Bonds</a></li>
<li><a href="#aromatic-proline"><span class="toc-section-number">9.3</span> Aromatic-Proline Interactions</a></li>
<li><a href="#aromatic-network"><span class="toc-section-number">9.4</span> Network-like structure of aromatic residues</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">PhD thesis: residue-residue contact prediction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="summary" class="section level1 unnumbered">
<h1>Summary</h1>
<p>Awesome contact prediction project abstract</p>
</div>
<div id="acknowledgements" class="section level1 unnumbered">
<h1>Acknowledgements</h1>
<p>I thank the world.</p>
<p></p>
<p></p>
<!--chapter:end:index.md-->
</div>
<div id="general-intro" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>In his Nobel Prize speech in 1973 <span class="citation">[<a href="#ref-Anfinsen1973">1</a>]</span> Anfinsen postulated one of the basic principles in molecular biology, which is known as <em>Anfinsen’s dogma</em>: a protein’s native structure is uniquely determined by its amino acid sequence. With certain exceptions (e.g. <a href="#abbrev">IDP</a> <span class="citation">[<a href="#ref-Wright1999">2</a>]</span>), this dogma has proven to hold true for the majority of proteins.</p>
<p>Ever since, it is regarded as the biggest challenge in structural bioinformatics <span class="citation">[<a href="#ref-Samish2015">3</a>]</span>, to realiably predict a protein’s structure given only its amino acid sequence. <em>De-novo</em> protein structure prediction methods use physical or knowledge based energy potentials to find a protein conformation that minimizes the protein’s energy landscape. However, these methods are limited by the complexity of the conformational space and the accuracy of the energy potentials. Considering a protein with 150 amino acids, that has approximately 450 degrees of freedom, Regarding the rotational and translational degrees of freedom of the protein chain, the complexity scales with XXX <span class="citation">[<a href="#ref-Anfinsen1973">1</a>]</span>.</p>
<p>Far more successfull are template-based modelling approaches. Given the observation that structure is more conserved than sequence in a protein family <span class="citation">[<a href="#ref-Lesk1980">4</a>]</span>, the structure of a target protein can be inferred from a homologue protein <span class="citation">[<a href="#ref-Sander1991">5</a>]</span>. The degree of structural conservation is linked to the level of pairwise sequence identity <span class="citation">[<a href="#ref-Chothia1986">6</a>]</span>. Therefore, the accuracy of a model crucially depends on the sequence identity between target and template and determines the applicability of the model <span class="citation">[<a href="#ref-Marti-Renom2000">7</a>]</span>. By definition, homology derived models are unable to capture new folds <span class="citation">[<a href="#ref-Dorn2014">8</a>]</span> and their main limitation lies in the availability of suitable templates.</p>
<div class="figure">
<iframe src="img/pdb_growth.html" width="100%" height="750px">
</iframe>
<p class="caption">
(#fig:seq-str-gap)Yearly growth of number of solved structures in the PDB<span class="citation">[<a href="#ref-Berman2000">9</a>]</span> and protein sequences in the Uniprot<span class="citation">[<a href="#ref-TheUniProtConsortium2013">10</a>]</span>.
</p>
</div>
<p>Unfortunately, the number of solved protein structures increases only slowly, as experimental methods are both time consuming and expensive <span class="citation">[<a href="#ref-Dorn2014">8</a>]</span>. The <a href="#abbrev">PDB</a><span class="citation">[<a href="#ref-Berman2000">9</a>]</span> is the main repository for marcomolecular structures and currently (Jul 2017) holds about 120 000 atomic models of proteins. The primary technique for determining protein structures is X-ray crystallography, accounting for roughly 90% of entries in the <a href="#abbrev">PDB</a>. About 9% of protein structures have been solved using <a href="#abbrev">NMR</a> and less than 1% using <a href="#abbrev">EM</a> (see FIG 1).</p>
<p>All three experimental techniques have advantages and limitations with respect to certain modelling aspects. X-ray crystallography requires the protein to form crystals, which is an arduous and sometimes impossible task. Furthermore, crystal packing forces the protein into a unnatural and rigid environment preventing the observation of conformational flexibility. <span class="math inline">\(\ac{NMR}\)</span> studies the protein in an physiological environment in solution and enables the study of protein dynamics as ensembles of protein structures can be observed. However, <span class="math inline">\(\ac{NMR}\)</span> is limited to look at small proteins. Recently, <span class="math inline">\(\ac{EM}\)</span> has undergone a “resolution revolution” <span class="citation">[<a href="#ref-Egelman2016">11</a>]</span> and macromolecular structures have been solved with resolutions up to 2A[citation]. The limit of <span class="math inline">\(\ac{cryo-EM}\)</span> lies in the size of proteins.</p>
<p>Compared to the tedious task of revealing atomic resolution of a protein tertiary structure, it has become very easy to decipher the primary sequence of proteins. With the latest sequencing technologies [examples], it takes only hours to sequence millions of basepaires at low costs [example numbers] and the number of sequenced genomes has risen tremendously. The UniProtKB <span class="citation">[<a href="#ref-TheUniProtConsortium2013">10</a>]</span>, the leading resource for protein sequences, contains more than 80 million sequence entries (24 July 2017).</p>
<p>Consequently, the gap between the number of protein structures and sequences is still growing and even new developments as single protein structure determination <span class="citation">[<span class="citeproc-not-found" data-reference-id="CITE"><strong>???</strong></span>]</span> are not expected to close this gap near in time. [Figure sequence structure gap]</p>
<p>Protein structure determines protein function. Therefore, structural insights are of uttermost importance. They are essential for a detailed understanding of chemical reactions, regulatory processes and transport mechanisms. They are fundamental for the design of drugs and antibiotics. Moreover structural abnormalities can lead to misfolding and aggregation potentially causing diseases so studying them is pathologically relevant.</p>
<p>The aformentioned trends illustrate the need of computational methods and motivate research to solve <em>Ansinsens Dogma</em> to reliably predict protein structures from sequence alone.</p>
<div id="protein-structure" class="section level2">
<h2><span class="header-section-number">1.1</span> Protein Structure</h2>
<ul>
<li>Primary: Amino Acid Sewuence</li>
<li>Secondary: Helices, sheets, coils, repeats,..</li>
<li>tertiary: interaction of secondary structure elementws</li>
<li>quartary: interaction of domains</li>
</ul>
<div id="amino-acid-interactions" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Amino Acid Interactions</h3>
<p>The Venn diagram in figure @ref(fig:amino-acid-physico-chemical-properties) displays a typical classification of amino acids with respect to their physico-chemical properties.</p>
<p>The aromatic amino acids tryptophan (W), tyrosine (Y), phenylalanine (F), and histidine (H) contain an aromatic ring system. Generally, aromatic ring systems are planar, and electons are shared over the whole ring structure. Interactions between aromatic residues have very constrained geometries regarding the angle between the centroid of their rings. The <span class="math inline">\(\pi\)</span>-electron systems favour T-shaped or offset stacked conformations <span class="citation">[<a href="#ref-Waters2002">12</a>]</span>. Preferred distances between aromatic residues have been observed between 4.5and 7of their ring centroids <span class="citation">[<a href="#ref-Burley1985">13</a>]</span>.</p>
<p>Cysteine (C) residues can form disulphide bonds, which are the only covalent bonds between two amino acid side chains. They comprise the strongest side chain interactions in protein structures and their length varies between 3.5to 4. Disulphide bonds also have a well defined geometry: there are five dihedral angles in a disulphide bond resulting in 20 different possible configurations. Only one configuration is favoured so that the dihedral angle between the carbon and sulfur atoms is close to 90 degrees <span class="citation">[<a href="#ref-Thornton1981">14</a>]</span>. They play a very important role in stabilizing protein structures. The number of disulfide bonds is negatively correlated with protein length: smaller proteins have more disulfide bonds helping to stabilize the structure in absence of strong hydrophobic packing in the core. It has also been found that disulfide bonds are more frequently observed in proteins of hyperthermophilic bacteria, being positively selected for increased stability <span class="citation">[<a href="#ref-Bastolla2005">15</a>]</span>.</p>
<p>Salt bridges are based on electrostatic interactions between positively charged residues (arginine (R) and lysine (K)) and negatively charged residues (aspartic acid (D) and glutamic acid (E)). The strength of electrostatic interactions, as described by Coulomb’s law, decreases with distance between the point charges at the functional groups. It has been found to be maximal at 4with respect to the functional groups of the both residues <span class="citation">[<a href="#ref-Donald2011">16</a>]</span>.</p>
<p>Hydrogen bonds can be formed between a donor residue which possesses an hydrogen atom attached to a strongly electronegative atom and an acceptor residue which possesses an electronegative atom with a lone electron pair. They are electrostatic interactions as well and thus their strength depends on distance as well. Hydrogen bonds are formed at distances of 2.4to 3.5between the non-hydrogen atoms (Berg JM, Tymoczko JL, 2002).</p>
<p>Salt bridges as well as hydrogen bonds have strong geometric preferences (Kumar and Nussinov, 1999). The geometry of a hydrogen bond depends on the angle between the HB donor, the hydrogen atom and the HB acceptor (Torshin et al., 2002).</p>
<p>Cation–<span class="math inline">\(\pi\)</span> interactions are formed between positively charged or partially charged amino acids with amino groups (K,R,Q,E) and aromatic residues (W,Y,F,H). The preferential distance of the amino group to the <span class="math inline">\(\pi\)</span>-electron system has been determined between 3.4and 6<span class="citation">[<a href="#ref-Burley1986">17</a>]</span> <span class="citation">[<a href="#ref-Crowley2005">18</a>]</span> Their role in stabilizing protein structures is still under debate <span class="citation">[<a href="#ref-Slutsky2004">19</a>]</span>.</p>
<p>Proline residues are conformationally restricted, with the alpha-amino group of the backbone directly attached to the side chain. The sterical rigidity of the proline side chain restricts the backbone angle and thus affects secondary structure formation. Proline is known as a helix-breaker. Whereas other aromatic side chains are defined by their negatively charged <span class="math inline">\(\pi\)</span> faces, the face of proline side chains is partially positively charged. Thus, aromatic and proline residues can interact favorably with each other. Once due to the hydrophobic nature of the residues and also due to the interaction between the negatively charged aroamtic <span class="math inline">\(\pi\)</span> face and the polarized C-H bonds in proline, called a CH/<span class="math inline">\(\pi\)</span> interaction.</p>
<p>Petersen et al. (2012) found clear secondary structure elements preferences for each amino acid pair. For example, residue pairs containing Alanine and Leucine are predominantly found in buried <span class="math inline">\(\alpha\)</span>-helices, whereas pairs containing Isoleucine and Valine preferentially are located in <span class="math inline">\(\beta\)</span>-sheet environments. Of course, solvent accessibility represents an important criterion for residue interactions. Hydrophobic residues are rather buried in the structure, whereas polar and charged residues are found more frequently on the protein surface and interact with water molecules.</p>
<div class="figure">
<img src="img/amino_acid_physico_chemical_properties_venn_diagramm.png" alt="Physico-chemical properties of amino acids. The 20 naturally occuring amino acids are grouped with respect to ten physico-chemical properties. Adapted from Figure 1a in [@Livingstone1993]." width="50%" />
<p class="caption">
(#fig:amino-acid-physico-chemical-properties)Physico-chemical properties of amino acids. The 20 naturally occuring amino acids are grouped with respect to ten physico-chemical properties. Adapted from Figure 1a in <span class="citation">[<a href="#ref-Livingstone1993">20</a>]</span>.
</p>
</div>
</div>
</div>
<div id="structure-prediction" class="section level2">
<h2><span class="header-section-number">1.2</span> Structure Prediction</h2>
<p>Despite the knowledge of Anfinsen’s postulate, we are not able to reliably predict the structure of a protein from its sequence alone. Generally it is assumed that a protein folds into a unique, well-defined native structure that is near the global free energy minimum (). Levinthal’s paradox <span class="citation">[<a href="#ref-Levinthal1969">21</a>]</span> describes the complexity of the folding process towards this minimum. It stresses the problem that it is not possible for a protein to exhaustively search the conformational space to get to its native fold. Due to the “combinatorial explosion” of possible conformations, an exhaustive search would take unreasonably long. Hence, it is not a feasible approach for structure prediction to scan all possible conformations. Different approaches have been developed over time to overcome or elude this problem.</p>
<div id="template-based-methods" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Template-based methods</h3>
<p>Homology modeling is by far the most successful approach to structure prediction. The basic concept of this strategy relates to the fact that structure is more conserved than sequence <span class="citation">[<a href="#ref-Lesk1980">4</a>]</span>. After detecting a homologous protein of known structure, that has sufficient sequence similarity, it can be used as a template to model the structure of the target protein.</p>
<p>The degree of structural conservation is linked to the level of pariwise sequence identity <span class="citation">[<a href="#ref-Chothia1986">6</a>]</span>. Homology Modelling is assumed to yield reliably accurate models when query and target protein share more than 30% sequence similarity, depending on the sequence length (<em>safe homology zone</em>) <span class="citation">[<a href="#ref-Sander1991">5</a>]</span>. Below a threshold of ~20-35% pairwise sequence identity (<em>twighlight-zone</em>) the number of false positives regarding structural similarity explodes and structural inference becomes less reliable and more than 95% of structures are dissmilar <span class="citation">[<a href="#ref-Rost1999">22</a>]</span>. Advances in remote homology detection and alignment generation have improved the quality of models, even beyond the once postulated limit of the <em>twighlight-zone</em> <span class="citation">[<a href="#ref-Yan2013">23</a>]</span>. Integration of multiple templates has also proved to increase model quality <span class="citation">[<a href="#ref-Meier2015">24</a>]</span></p>
<p>After the identification of a suitable template, there are different strategies that can be followed to obtain a model for the target protein. The the backbone of the model is generated by simply copying the coordinates of the target backbone atoms onto the model. Non-aligned residues due to gaps in the alignment have to be modelled , meaning from scratch. This can be done by a knowledge-based search for suitable fragments in the PDB or by true energy-based  modelling. When the backbone is generated, the side chains are modelled, usually by searching rotamer libraries for energetically favoured residue conformations. Finally, the model is energetically optimized in an iterative procedure. Force fields are applied to correct the backbone and side chain conformations [<span class="citation">[<a href="#ref-Gu2009">25</a>]</span>}. Several automated pipelines for homology modelling are well-established (Modeller [<span class="citation">[<a href="#ref-Eswar2007">26</a>]</span>}, 3D-Jigsaw [<span class="citation">[<a href="#ref-Bates2001">27</a>]</span>}, SwissModel [<span class="citation">[<a href="#ref-Arnold2006">28</a>]</span>}) which allow more or less manual intervention in the modelling process.</p>
<p>Fold Recognition describes the inverse folding problem : instead of finding the compatible structure for a given sequence, one tries to find sequences that fit onto a given structure. Whether the query sequence fits a structure from the database is not determined by sequence similarities but rather energetic or environment specific measures. Thus, fold recognition methods are able to recognize structural similarity even in the absence of sequence similarity. The rationale basis for this strategy is the assumption that the fold space is limited. It has been found that seemingly unrelated proteins often adopt similar folds. This might be due to divergent evolution (proteins are related, but homology cannot be detected at the corresponding sequence level) or convergent evolution (functional requirements lead to similar folds for unrelated proteins) . Early approaches include profile based methods. Here, the structural information of the protein is encoded into profiles, which subsequently are aligned to the sequences . Advanced techniques are known as “threading” techniques, describing the process of threading a sequence through a structure and determining the optimal fit via energy functions. </p>
</div>
<div id="template-free-structure-prediction" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Template-free structure prediction</h3>
<p>Ab initio or de-novo modeling techniques implement Anfinsen’s Dogma most closely in mimicking the folding process based only on physico-chemical principles. Energy functions (physical or knowledge-based) are used to describe the folding landscape and are minimized to arrive at the global energy minimum corresponding to the native conformation. Since the native conformation can be found near the global energy minimum of the folding landscape, energy functions (physical or knowledge-based) have been developed to describe this landscape. With respect to the idea of a folding funnel, the energy function is minimized to mimic the folding process that automatically leads to the global minimum. Again, there exist numerous webservers that combine energy minimization, threading techniques and fragment-based approaches, e.g. Rosetta , Tasser , Touchstone II .</p>
<p>Drawbacks of these methods are the time requirements due to the computational complexity of energy functions as well as their inaccuracy.</p>
<p>Minimize a physical or knowledge-based energy function for the protein. This has huge complexity due to large conformational space that needs to be sampled.</p>
</div>
<div id="contact-assisted-str-pred" class="section level3">
<h3><span class="header-section-number">1.2.3</span> contact assisted de-novo predictions</h3>
<p>Structure Reconstruction from true contacts maps works well. Even a small number of contacts is sufficient to reconstruct the fold of the protein. Distance maps work even better.</p>
<p>What is the optimal distance cutoff to define a contact? Duarte et al 2010: between 8 and 12A Dyrka et al 2016 Konopka et al 2014 Sathyapriya et al 2009</p>
<p>Many studies that successfuly predict structures denovo with the help of predicted contact.</p>
<p>Vice versa, because contacts at large primary distances are rare, they are most informative for protein structure prediction: Izarzugaza J, Gran ˜a O, Tress M, Valencia A, Clarke N (2007) Assessment of intramolecular contact predictions for CASP7</p>
</div>
</div>
<div id="contact-prediction" class="section level2">
<h2><span class="header-section-number">1.3</span> Contact Prediction</h2>
<p>Contact Prediction refers to the prediction of physical contacts between amino acid side chains in the 3D protein structure, given the protein sequence as input.</p>
<p>Historically, contact prediction was motivated by the idea that compensatory mutations between spatially neighboring residues can be traced down from evolutionary records <span class="citation">[<a href="#ref-Gobel1994">29</a>]</span>. As proteins evolve, they are under selective pressure to maintain their function and correspondingly their structure. Consequently, residues and interactions between residues constraining the fold, protein complex formation or other aspects of function are under selective pressure. Highly constrained residues and interactions will be strongly conserved. Another possibility to maintain structural integrity is the mutual compensation of unbeneficial mutations. For example, the unfavourable mutation of a small amino acid residue into a bulky residue in the densely packed protein core might have been compensated in the course of evolution by a particularly small side chain in a neighboring position. Other physico-chemical quantities such as amino acid charge or hydrogen bonding capacity can also be responsible for compensatory effects. In a <a href="#abbrev">MSA</a>, sequences that descended from a common ancestral sequence are aligned such that the homologous residues line up with each other in columns. According to the hypothesis, compensatory mutations show up as correlations between the amino acid types of pairs of <a href="#abbrev">MSA</a> columns and can be used to infer spatial proximity of residue pairs (see Figure @ref(fig:correlated-mutations)).</p>
<p>(ref:caption-correlated-mutations) Compensatory mutations between spatially neighboring residues subject to particular physico-chemical constraints can leave coevolutionary record in protein sequences. Mining protein family sequence alignments for residue pairs with strong coevolutionary records using statistical models allows inference of spatial proximity for these residue pairs.</p>
<div class="figure" style="text-align: center">
<img src="img/intro/correlated-mutations-transparent.png" alt="(ref:caption-correlated-mutations)" width="80%" />
<p class="caption">
(#fig:correlated-mutations)(ref:caption-correlated-mutations)
</p>
</div>
<p>Early methods from the 1990’s were very inaccurate as the number of available protein sequences was only small and weak statistical models were prone to noise. It took until the end of the last decade that major sources of noise could be eliminated and sophisticated statistical models allowed for the distinction between transitively mediated and causal interactions <span class="citation">[<a href="#ref-Dunn2008">30</a>,<a href="#ref-Weigt2009">31</a>]</span>. With the steady increase in protein sequence data, purely machine learning based methods emerged that are trained on features extracted from <a href="#abbrev">MSAs</a>. Currently, the most accurate methods to predict residue-residue contacts are meta-predictors, combining one or several coevolution methods with sequence derived features and other sources of information.</p>
<p>This chapter will give an overview over important previous methods, will introduce the state-of-the-art statistical model for inferring coevolutionary couplings and present well-known challenges for contact prediction methods.</p>
<div id="local-methods" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Local Statistical Models</h3>
<p>Early contact prediction methods used local pairwise statistics to infer contacts that regard pairs of amino acids in a sequence as statistically independent from another. The drawback of these approaches is that they do not account for transitive effects arising from chains of correlations between multiple residue pairs as described in the section on <a href="#transitive-effects">Transitive Effects</a>.</p>
<p>Several of these methods use correlation coefficient based measures, such as Pearson correlation between amino acid counts, properties associated with amino acids or mutational propensities at the sites of a <a href="#abbrev">MSA</a> <span class="citation">[<a href="#ref-Gobel1994">29</a>,<a href="#ref-Neher1994">32</a>–<a href="#ref-Oliveira2002">34</a>]</span>.</p>
<p>Many methods have been developed that are rooted in information theory and use <a href="#abbrev">MI</a> measures to describe the dependencies between sites in the alignment <span class="citation">[<a href="#ref-Clarke1995">35</a>–<a href="#ref-Martin2005">37</a>]</span>. Phylogenetic and entropic biases have been identified as the major sources of noise that confound the true coevolution signal <span class="citation">[<a href="#ref-Martin2005">37</a>–<a href="#ref-Fodor2004">39</a>]</span>. Different variants of <a href="#abbrev">MI</a> based approaches try to address these effects and improve on the signal-to-noise ratio <span class="citation">[<a href="#ref-Atchley2000">38</a>,<a href="#ref-Tillier2003">40</a>,<a href="#ref-Gouveia_Oliveira2007">41</a>]</span>. The most prominent correction for background noises is <a href="#abbrev">APC</a>, developed by Dunn et al. that drastically removes background noise from entropic effects and is discussed in section @ref(post-processing-heuristics) <span class="citation">[<a href="#ref-Dunn2008">30</a>]</span>.</p>
<p>Another popular method is <em>OMES</em> that essentially computes a chi-squared statistic to detect the differences between observed and expected pairwise amino acid frequencies for a pair of columns <span class="citation">[<a href="#ref-Kass2002">42</a>,<a href="#ref-Noivirt2005">43</a>]</span>.</p>
<p>Eventhough these methods cannot compete with modern predictors, <em>OMES</em> and <a href="#abbrev">MI</a> based scores often serve as a baseline to benchmark the performance of new methods <span class="citation">[<a href="#ref-DeJuan2013">44</a>,<a href="#ref-Jones2012">45</a>]</span>.</p>
</div>
<div id="global-methods" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Global Statistical Models</h3>
<p>Global statistical models make predictions for a single residue pair while considering all other pairs in the protein. By doing so they solve the correlation versus causation phenomenon and distinguish direct from indirect couplings which has been referred to in the literature as <a href="#abbrev">DCA</a> <span class="citation">[<a href="#ref-Weigt2009">31</a>,<a href="#ref-Lapedes1999">46</a>]</span>.</p>
<p>In 1999 Lapedes et al. were the first to propose a global statistical approach for the prediction of residue-residue contacts in order to disentangle transitive effects <span class="citation">[<a href="#ref-Lapedes1999">46</a>]</span>. They consider a Pott’s model that can be derived under a maximum entropy assumption and use the model specific coupling parameters to infer interactions. At that time the wider implications of this great advancement went unnoted but meanwhile the Pott’s Model has become the most prominent statistical model for contact prediction. Section @ref(maxent) deals extensively with the derivation and properties of the Pott’s model, its application to contact prediction and its numerous realizations.</p>
<p>A global statistical model not motivated by the maximum entropy approach was proposed by Burger and Nijmwegen in 2010 <span class="citation">[<a href="#ref-Burger2008">47</a>,<a href="#ref-Burger2010">48</a>]</span>. Their fast Bayesian network model incorporates additional prior information and phylogenetic correction via <a href="#abbrev">APC</a> but cannot compete with the currently most successfull pseudo-likelihood approaches presented in section @ref(pseudo-likelihood).</p>
</div>
<div id="machine-learnign-methods-and-meta-predictors" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Machine Learnign Methods and Meta-Predictors</h3>
<p>These methods combine abundant information on sequence and amino acid properties in order to learn associations between input features and residue-residue contacts. Methods dier mainly in the type of the applied Machine Learning (ML) algorithm, e.g Neural Networks (NNs), Support Vector Machines (SVMs) or Random Forests (RFs) and the chosen input features, e.g. contact predictions, solvent accessibility, physico-chemical properties of amino acids, secondary structure predictions or evolutionary information. (Kukic et al., 2014; Alfonso Marquez-Chamorro, 2013; Li et al., 2011) The problem with these methods is interpretability, as it is dicult to elucidate which feature patterns contribute in which amount to the model.</p>
<ul>
<li>combining different approaches</li>
<li>jones et al: overlap between methods but also many unique predictions</li>
<li>machine learning methods incorporate sequence-derived features:</li>
<li>secondary structure predictions</li>
<li>solvent accessibilty</li>
<li>contact potentials</li>
<li>msa properties</li>
<li>pssms</li>
<li>physico-chemcial properties of amino acids</li>
</ul>
<p>However, Meta-predictors will improve if basic methods improve. Ultra-deep learning paper identifies coevolution features as crucial feature.</p>
</div>
<div id="intro-cp-evaluation" class="section level3">
<h3><span class="header-section-number">1.3.4</span> Evaluating Contact Prediction Methods</h3>
<p>Choosing an appropriate benchmark for contact prediction methods depends on the further utilization of the predictions. Most prominently, predicted contacts are used to assist structure prediction as outlined in section @ref(contact-assisted-str-pred). Therefore, one could in fact assess the quality of structural models computed with the help of predicted contacts. However, predicting structural models adds not only another layer of computational complexity but also raises questions about implementation details of the folding protocol. Generally it has been found that a small number of accurate contacts is sufficient to constrain the overal protein fold as discussed in section @ref(contact-assisted-str-pred).</p>
<p>From these considerations emerged a standard benchmark that evaluates the mean precision over a testset of proteins with known high quality 3D structures with respect to the top scoring predictions from every protein. The number of top scoring predictions per protein is typically normalized with respect to protein length <span class="math inline">\(L\)</span> and precision is defined as the number of true contacts among the top scoring predicted contacts. Usually, a pair of residues is defined to be in contact when the distance between their <span class="math inline">\(\Cb\)</span> atoms (<span class="math inline">\(C\alpha\)</span> in case of glycine) is less than <span class="math inline">\(8 \AA \; \;\)</span> in the reference protein structure <span class="citation">[<a href="#ref-Monastyrskyy2015">49</a>]</span>.</p>
<p><strong>Contact Definition</strong></p>
<p>However, whether two residues truly interact in a protein structure depends only marginally on the distance between their <span class="math inline">\(\Cb\)</span> atoms. More importantly, interactions between side-chains depend on their physico-chemical properties, on their orientation and vary within the vast number of alternative environments within proteins <span class="citation">[<a href="#ref-Bettsa">50</a>]</span> (see section @ref(amino-acid-interactions)). Therefore, a simple <span class="math inline">\(\Cb\)</span> distance threshold cannot capture the true interaction preferences of amino acids and yields an imperfect gold-standard for benchmarking.</p>
<p>Other distance thresholds or definitions for contacts (e.g minimal atomic distances or distance between functional groups) have been studied as well. In fact, Duarte and colleagues found that using a <span class="math inline">\(\Cb\)</span> distance threshold between 9<span class="math inline">\(\AA \; \;\)</span> and 11<span class="math inline">\(\AA \; \;\)</span> yields optimal results when predicting the 3D structure from the respective contacts <span class="citation">[<a href="#ref-Duarte2010">51</a>]</span>.</p>
<p>Anishchenko and colleagues <span class="citation">[<a href="#ref-Anishchenko2017">52</a>]</span> analysed false positive predictions with respect to a minimal atom distance threshold <span class="math inline">\(&lt; 5 \AA \; \;\)</span>, as they found that this cutoff optimally defines direct physical interactions of residue pairs.</p>
<p>With regard to the utilization of contacts for structure prediction, a simple <span class="math inline">\(\Cb\)</span> cutoff is nonetheless a convenient choice, as this threshold can be easily implemented as a restraint in common structure predictions protocols (e.g Modeller).</p>
<p><strong>Sequence Separation</strong></p>
<p>Local residue pairs separated by only some positions in sequence (e.g <span class="math inline">\(|i-j| &lt; 6\)</span>) are usually filtered out for evaluation of contact prediction methods. They are trivial to predict as they typically correspond to contacts within secondary structure elements and reflect the local geometrical constraints. Figure @ref(fig:Cb-distribution) shows the distribution of <span class="math inline">\(\Cb\)</span> distances for various minimal sequence separation thresholds.</p>
<p>(ref:caption-Cb-distribution) Distribution of residue pair <span class="math inline">\(\Cb\)</span> distances over all proteins in the dataset (see Methods @ref(dataset)) at different minimal sequence separation thresholds: blue = <span class="math inline">\(|i-j| &gt; 1\)</span> (all residue pairs), orange = <span class="math inline">\(|i-j| &gt; 6\)</span>, green = <span class="math inline">\(|i-j| &gt; 12\)</span>, red = <span class="math inline">\(|i-j| &gt; 24\)</span>.</p>
<div class="figure">
<iframe src="img/dataset_statistics/Cb_distribution_all_data43579541_log.html" width="100%" height="500px">
</iframe>
<p class="caption">
(#fig:Cb-distribution)(ref:caption-Cb-distribution)
</p>
</div>
<p>Without filtering local residue pairs (sequence separation 1), there are several additional peaks in the distribution around <span class="math inline">\(5.5\AA \; \;\)</span>, <span class="math inline">\(7.4\AA \; \;\)</span> and <span class="math inline">\(10.6\AA \; \;\)</span> that can be attributed to local interactions in e.g. helices (see Figure @ref(fig:peaks-Cb-distribution)).</p>
<p>(ref:caption-peaks-Cb-distribution) <span class="math inline">\(\Cb\)</span> distances between neighboring residues in <span class="math inline">\(\alpha\)</span>-helices. Left: Direct neighbors in <span class="math inline">\(\alpha\)</span>-helices have <span class="math inline">\(\Cb\)</span> distances around <span class="math inline">\(5.4\AA\; \;\)</span> due to the geometrical constraints from <span class="math inline">\(\alpha\)</span>-helical architecture. Right: Residues separated by two positions (<span class="math inline">\(|i-j| = 2\)</span>) are less geometrically restricted to <span class="math inline">\(\Cb\)</span> distances between <span class="math inline">\(7\AA\; \;\)</span> and <span class="math inline">\(7.5\AA\; \;\)</span>.</p>
<div class="figure">
<img src="img/dataset_statistics/cb_distribution_peak_5-6.png" alt="(ref:caption-peaks-Cb-distribution)" width="50%" /><img src="img/dataset_statistics/cb_distribution_peak_7.png" alt="(ref:caption-peaks-Cb-distribution)" width="50%" />
<p class="caption">
(#fig:peaks-Cb-distribution)(ref:caption-peaks-Cb-distribution)
</p>
</div>
<p>Commonly, sequence separation bins are applied to distuinguish short (<span class="math inline">\(6 &lt; |i-j| \le 12\)</span>), medium (<span class="math inline">\(12 &lt; |i-j| \le 24\)</span>) and long range (<span class="math inline">\(|i-j| &gt; 24\)</span>) contacts <span class="citation">[<a href="#ref-Monastyrskyy2015">49</a>]</span>. Especially long range contacts are of importance for structure prediction as they are informative and able to constrain the overal fold of a protein <span class="citation">[<span class="citeproc-not-found" data-reference-id="CITE"><strong>???</strong></span>]</span>.</p>
<p><strong>CASP</strong></p>
<p>CASP, the well-respected and independent competition for the structural bioinformatic’s community that is taking place every two years, introduced the contact prediction category in 1996 and developed a standard procedure for the assessment of predictions. The precision of predicted long range (<span class="math inline">\(|i-j| &gt; 24\)</span>) contacts is assessed based on a <span class="math inline">\(8 \AA \; \; \Cb\)</span> distance threshold for proteins with no (or only hard to detect) structural homologs. During CASP11 further evaluation metrics have been introduced, such as Matthews correlation coefficient and area under the precision-recall curve.</p>
<p>Currently best methods perform in the range XXX</p>
</div>
<div id="maxent" class="section level3">
<h3><span class="header-section-number">1.3.5</span> Maximum Entropy Modelling of Protein Families</h3>
<p>The principle of maximum entropy, proposed by Jaynes in 1957 <span class="citation">[<a href="#ref-Jaynes1957a">53</a>,<a href="#ref-Jaynes1957b">54</a>]</span>, states that the probability distribution which makes minimal assumptions and best represents observed data is the one that is in agreement with measured constraints (prior information) and has the largest entropy. In other words, from all the distributions that are consistent with the given data one chooses the distribution with maximal Shannon entropy.</p>
<p>Applied to the problem of modelling protein families, one seeks a probability distribution <span class="math inline">\(p(\seq)\)</span> for protein sequences <span class="math inline">\(\seq = (x_1, \ldots, x_L)\)</span> of length <span class="math inline">\(L\)</span> from the protein family under study. The categorical variables <span class="math inline">\(x_{i}\)</span> can take one of <span class="math inline">\(q=21\)</span> values representing the 20 naturally occuring amino acids and a gap (‘-’). Given <span class="math inline">\(N\)</span> sequences of the protein family in a <a href="#abbrev">MSA</a> with <span class="math inline">\(\X = \{ \seq_1, \ldots, \seq_N \}\)</span>, the empirically observed single and pairwise amino acid frequencies can be calculated as</p>
<span class="math display">\[\begin{equation}
    \mathcal{f}_i(a) = \mathcal{f}(x_i\eq a) = \frac{1}{N}\sum_{n=1}^N I(x_{ni} \eq a) \\
    \mathcal{f}_{ij}(a,b) = \mathcal{f}(x_i\eq a, x_j\eq b) = \frac{1}{N} \sum_{n=1}^N I(x_{ni} \eq a, x_{nj} \eq b) \; .
 (\#eq:emp-freq)
\end{equation}\]</span>
<p>According to the maximum entropy principle, the distribution <span class="math inline">\(p(\seq)\)</span> should have maximal entropy and reproduce the empirically observed amino acid frequencies, so that</p>
<span class="math display">\[\begin{align}
   \mathcal{f}(x_i\eq a)            &amp;\equiv p(x_i\eq a)  \\
                                    &amp;= \sum_{\seq&#39;_1, \ldots, \seq&#39;_L = 1}^{q} p(x&#39;) I(x&#39;_i \eq a) \\
  \mathcal{f}(x_i\eq a, x_j\eq b)   &amp;\equiv p(x_i\eq a, x_j \eq b) \\
                                    &amp;= \sum_{\seq&#39;_1, \ldots, \seq&#39;_L = 1}^{q}  p(x&#39;) I(x&#39;_i\eq a, x&#39;_j \eq b)  \; .
 (\#eq:maxent-reproducing-emp-freq)
\end{align}\]</span>
<p>Solving for the distribution <span class="math inline">\(p(\seq)\)</span> that maximizes the Shannon entropy <span class="math inline">\(S= -\sum_{\seq&#39;} p(\seq&#39;) \log p(\seq&#39;)\)</span> while satisfying the constraints given in eq. @ref(eq:maxent-reproducing-emp-freq) by introducing the Lagrange multipliers <span class="math inline">\(\wij\)</span> and <span class="math inline">\(\vi\)</span>,</p>
<span class="math display">\[\begin{align}
F \left[ p(\seq) \right] =&amp; -\sum_{\seq&#39;} p(\seq&#39;) \log p(\seq&#39;) \\
        &amp; + \sum_{i=1}^L \sum_{a=1}^{q} \vi(a) \left( p(x_i\eq a) - \mathcal{f}(x_i\eq a) \right) \\
        &amp; + \sum_{1 \leq i &lt; j \leq L}^L \; \sum_{a,b=1}^{q} \wij(a,b) \left( p(x_i\eq a, x_j \eq b) - \mathcal{f}(x_i\eq a, x_j\eq b) \right) \\
        &amp; + \Omega \left( 1-\sum_{\seq&#39;} p(\seq&#39;)  \right)
(\#eq:derivation-max-ent-model)
\end{align}\]</span>
<p>results in the formulation of an exponential model known as <em>Potts model</em> in statistical physics or <a href="#abbrev">MRF</a> in statistics,</p>
<span class="math display">\[\begin{equation}
    p(\seq | \v, \w ) = \frac{1}{Z} \exp \left( \sum_{i=1}^L v_i(x_i) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_i, x_j) \right) \; .
(\#eq:max-ent-model)
\end{equation}\]</span>
<p>The Lagrange multipliers <span class="math inline">\(\wij\)</span> and <span class="math inline">\(\vi\)</span> remain as model parameters to be fitted to data. <span class="math inline">\(Z\)</span> is a normalization constant also known as <em>partition function</em> that ensures the total probabilty adds up to one by summing over all possible assignments to <span class="math inline">\(\seq\)</span>,</p>
<span class="math display">\[\begin{equation}
  Z = \sum_{\seq&#39;_1, \ldots, \seq&#39;_L = 1}^{q} \exp  \left( \sum_{i=1}^L v_i(x_i) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_i, x_j) \right) \; .
  (\#eq:partition-fct-likelihood)
\end{equation}\]</span>
<div id="model-properties" class="section level4">
<h4><span class="header-section-number">1.3.5.1</span> Model Properties</h4>
<p>The Potts model is specified by singlet terms <span class="math inline">\(\via\)</span> which describe the tendency for each amino acid a to appear at position <span class="math inline">\(i\)</span>, and pair terms <span class="math inline">\(\wijab\)</span>, also called couplings, which describe the tendency of amino acid a at position <span class="math inline">\(i\)</span> to co-occur with amino acid b at position <span class="math inline">\(j\)</span>. In contrast to mere correlations, the couplings explain the causative dependence structure between positions by jointly modelling the distribution of all positions in a protein sequence and thus account for transitive effects (see @ref(local-methods)).</p>
<p>Maximum entropy models naturally give rise to exponential family distributions that express useful properties for statistical modelling, such as the convexity of the likelihood function which consequently has a unique, global minimum <span class="citation">[<a href="#ref-Wainwright2007">55</a>,<a href="#ref-Murphy2012">56</a>]</span>.</p>
<p>The Potts model is a discrete instance of what is referred to as a pairwise <a href="#abbrev">Markov random field</a> in the statistics community. <a href="#abbrev">MRFs</a> belong to the class of undirected graphical models, that represent the probability distribution in terms of a graph with nodes and edges characterizing the variables and the dependence structure between variables, respectively.</p>
<div id="gauge-invariance" class="section level5 unnumbered">
<h5>Gauge Invariance</h5>
<p>As <span class="math inline">\(x_{ni}\)</span> can take <span class="math inline">\(q=21\)</span> values, the model has <span class="math inline">\(L \! \times \! q + L(L-1)/2 \! \times \! q^2\)</span> parameters but the parameters are not uniquely determined as multiple parametrizations yield identical probability distributions.</p>
<p>For example, adding a constant <span class="math inline">\(c_i\)</span> to all elements in <span class="math inline">\(v_i\)</span> for any fixed position <span class="math inline">\(i\)</span> or similarly adding a constant <span class="math inline">\(c_{ia}\)</span> to <span class="math inline">\(\via\)</span> for any fixed position <span class="math inline">\(i\)</span> and amino acid <span class="math inline">\(a\)</span> and subtracting the same constant from the <span class="math inline">\(qL\)</span> coefficients <span class="math inline">\(\wijab\)</span> with <span class="math inline">\(b \in \{1, \ldots, q\}\)</span> and <span class="math inline">\(j \in \{1, \ldots, L \}\)</span> leaves the probabilities for all sequences under the model unchanged, since such a change will be compensated by a change of <span class="math inline">\(Z\)</span> in eq. @ref(eq:partition-fct-likelihood).</p>
<p>The overparametrization, referred to as <em>gauge invariance</em> in statistical physics literature, can be eliminated by removing parameters. An appropriate choice of which parameters to remove, referred to as <em>gauge choice</em>, reduces the number of parameters to <span class="math inline">\(L \! \times \! (q-1) + L(L-1)/2 \! \times \! (q-1)^2\)</span>. Popular gauge choices are the <em>zero-sum gauge</em> or <em>Ising-gauge</em> used by <span class="citation">[<a href="#ref-Weigt2009">31</a>]</span> imposed by the restraints,</p>
<span class="math display">\[\begin{equation}
    \sum_{a=1}^{q} v_{ia} = \sum_{a=1}^{q} \wijab = \sum_{a=1}^{q} w_{ijba} = 0
(\#eq:zero-sum-gauge)
\end{equation}\]</span>
<p>for all <span class="math inline">\(i,j,b\)</span> or the <em>lattice-gas gauge</em> used by <span class="citation">[<a href="#ref-Morcos2011">57</a>,<a href="#ref-Marks2011">58</a>]</span> imposed by restraints</p>
<span class="math display">\[\begin{equation}
    \wij(q,a) = \wij(a,q) = \vi(q) = 0
(\#eq:ising-gauge)
\end{equation}\]</span>
<p>for all <span class="math inline">\(i,j,a\)</span> <span class="citation">[<a href="#ref-Cocco2017">59</a>]</span>.</p>
<p>Alternatively, the indeterminacy can be fixed by including a regularization prior (see next section). The regularizer selects for a unique solution among all parametrizations of the optimal distribution and therefore eliminates the need to choose a gauge <span class="citation">[<a href="#ref-Koller2009">60</a>–<a href="#ref-Stein2015a">62</a>]</span>.</p>
</div>
<div id="regularization" class="section level5 unnumbered">
<h5>Regularization</h5>
<p>The number of parameters in a Potts model is typically larger than the number of observations, i.e. the number of sequences in the <a href="#abbrev">MSA</a>. Considering a protein of length <span class="math inline">\(L=100\)</span>, there are approximately <span class="math inline">\(2 \times 10^6\)</span> parameters in the model whereas the largest protein families comprise only around <span class="math inline">\(10^5\)</span> sequences (see Figure @ref(fig:pfam)). An underdetermined problem like this renders the use of regularizer neccessary in order to prevent overfitting.</p>
<p>Typically, an L2-regularization is used that pushes the single and pairwise terms smoothly towards zero and is equivalent to the logarithm of a zero-centered Gaussian prior,</p>
<span class="math display">\[\begin{align}
  R(\v, \w)  &amp;= \log \left[ \mathcal{N}(\v | \mathbf{0}, \lambda_v^{-1} I) \mathcal{N}(\w | \mathbf{0}, \lambda_w^{-1} I) \right] \\
             &amp;= -\frac{\lambda_v}{2} ||\v||_2^2 - \frac{\lambda_w}{2} ||\w||_2^2 + \text{const.} \; ,
(\#eq:l2-reg)
\end{align}\]</span>
<p>where the strength of regularization is tuned via the regularization coefficients <span class="math inline">\(\lambda_v\)</span> and <span class="math inline">\(\lambda_w\)</span> <span class="citation">[<a href="#ref-Seemayer2014">63</a>–<a href="#ref-Kamisetty2013">65</a>]</span>.</p>
</div>
</div>
<div id="partition-function" class="section level4">
<h4><span class="header-section-number">1.3.5.2</span> Intractability of the Partition Function</h4>
<p>Typically, one obtains parameter estimates by maximizing the log-likelihood function of the parameters over observed data. For the Potts model, the log-likelihood function is computed over sequences in the alignment <span class="math inline">\(\mathbf{X}\)</span>:</p>
<span class="math display">\[\begin{align}
    \text{LL}(\v, \w | \mathbf{X}) =&amp; \sum_{n=1}^N \log p(\seq_n) \\
    =&amp; \sum_{n=1}^N \left[ \sum_{i=1}^L v_i(x_{ni}) + \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_{xn}, x_{nj}) - \log Z \right] \\
(\#eq:full-log-likelihood)
\end{align}\]</span>
<p>However, optimizing the log-likelihood requires computing the partition function <span class="math inline">\(Z\)</span> given in eq. @ref(eq:partition-fct-likelihood) that sums <span class="math inline">\(q^L\)</span> terms, with <span class="math inline">\(L\)</span> being in the hundreds for naturally occurig protein domains. Because of this exponential complexity in protein length <span class="math inline">\(L\)</span>, it is computationally intractable to evaluate the log-likelihood function at every iteration of an optimization procedure.</p>
<p>Several approximate solutions have been developed to sidestep the infeasible computation of the partition function for the specific problem of predicting contacts between residues that are briefly explained in the next section.</p>
</div>
<div id="solving-the-inverse-potts-problem" class="section level4">
<h4><span class="header-section-number">1.3.5.3</span> Solving the Inverse Potts Problem</h4>
<p>In 1999 Lapedes et al. were the first to propose maximum entropy models for the prediction of residue-residue contacts in order to disentangle transitive effects <span class="citation">[<a href="#ref-Lapedes1999">46</a>]</span>. They used an iterative Monte Carlo procedure to obtain estimates of the partition function. As the calculations involved were very time-consuming and at that time required supercomputing resources, the wider implications were not noted yet.</p>
<p>In 2009 Weight et al proposed an iterative message-passing algorithm, here referred to as <em>mpDCA</em>, to approximate the partition function <span class="citation">[<a href="#ref-Weigt2009">31</a>]</span>. Eventhough their approach is computationally very expensive and in practive only applicable to small proteins, they obtained remarkable results for the two-component signaling system in bacteria.</p>
<p>Balakrishnan et al <span class="citation">[<a href="#ref-Balakrishnan2011">66</a>]</span> were the first to apply pseudo-likelihood approximations to the full likelihood in 2011. The pseudo-likelihood optimizes a different objective and replaces the global partition function <span class="math inline">\(Z\)</span> with local estimates. Balakrishnan and colleagues applied their method <em>GREMLIN</em> to learn sparse graphical models for 71 protein families. In a follow-up study in 2013 <span class="citation">[<a href="#ref-Kamisetty2013">65</a>]</span>, an improved version of <em>GREMLIN</em> incorporating prior information was evaluated in a comprehensive benchmark tailored towards the contact prediction problem.</p>
<p>Also in 2011, Morcos et al. introduced a naive mean-field inversion approximation to the partition function, named <em>mfDCA</em> <span class="citation">[<a href="#ref-Morcos2011">57</a>]</span>. This method allows for drastically shorter running times as the mean-field approach boils down to inverting the empirical covariance matrix calculated from observed amino acid frequencies for each residue pair <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> of the alignment. This study performed the first high-throughput analysis of intradomain contacts for 131 protein families and facilitated the prediction of protein structures from accurately predicted contacts in <span class="citation">[<a href="#ref-Marks2011">58</a>]</span>.</p>
<p>The initial work by Balakrishnan and collegueas went almost unnoted as it was not primarily targeted to the problem of contact prediction. Ekeberg and collegueas independently developed the pseudo-likelihood method <em>plmDCA</em> and showed its superior precision towards <em>mfDCA</em> <span class="citation">[<a href="#ref-Ekeberg2013">61</a>]</span>.</p>
<p>A related approach to mean-field approximation is sparse inverse covariance estimation, named <em>PSICOV</em>, by Jones et al <span class="citation">[<a href="#ref-Jones2012">45</a>]</span>. They use L1-regularization, known as graphical Lasso, to invert the correlation matrix and learn a sparse graphical model <span class="citation">[<a href="#ref-Friedman2008">67</a>]</span>. Both procedures, <em>mfDCA</em> and <em>PSICOV</em>, assume the model distribution to be a multivariate Gaussian. It has been shown by Banerjee et al. (2008) that this dual optimization solution also applies to binary data (as is the case in this application). In order to represent the <a href="#abbrev">MSA</a> as continuous distributed, each position is encoded as a 20-dimensional binary vector.</p>
<p>Another related approach to <em>mfDCA</em> and <em>PSICOV</em> is <em>gaussianDCA</em>, proposed in 2014 by Baldassi et al. <span class="citation">[<a href="#ref-Baldassi2014">68</a>]</span>. Similar to the other both approaches, they model the data as multivariate Gaussian but within a simple Bayesian formalism by using a suitable prior and estimating parameters over the posterior distribution.</p>
<p>So far, pseudo-likelihood maximization has proven to be the most accurate approach with respect to the standard evaluation procedures for contact prediction presented in the following section. Currently, there exist several implementations of pseudo-likelihood maximization that vary in slight details, perform similarly and thus are equally popular in the community, such as CCMpred <span class="citation">[<a href="#ref-Seemayer2014">63</a>]</span>, plmDCA<span class="citation">[<a href="#ref-Ekeberg2014">64</a>]</span> and GREMLIN <span class="citation">[<a href="#ref-Kamisetty2013">65</a>]</span>.</p>
</div>
<div id="pseudo-likelihood" class="section level4">
<h4><span class="header-section-number">1.3.5.4</span> Pseudo-Likelihood</h4>
<p>Instead of the full likelihood, Besag suggested to optimize a different objective function that he called <em>pseudo-likelihood</em> <span class="citation">[<a href="#ref-Besag1975">69</a>]</span>. The pseudo-likelihood approximates the joint probability with the product over conditionals for each variable, i.e. the conditional probability of observing one variable given all the others:</p>
<span class="math display">\[\begin{equation}
  p(\seq | \v,\w) \approx   \prod_{i=1}^L p(x_i | \seq_{\backslash xi}, \v,\w) =  \prod_{i=1}^L \frac{1}{Z_i} \exp \left(  v_i(x_i) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(x_i, x_j) \right)
\end{equation}\]</span>
<p>Here, the normalization term <span class="math inline">\(Z_i\)</span> sums only over all assignments to one position <span class="math inline">\(i\)</span> in sequence:</p>
<span class="math display">\[\begin{equation}
  Z_i = \sum_{a=1}^{q} \exp \left( v_i(a) \sum_{1 \leq i &lt; j \leq L}^L w_{ij}(a, x_j) \right)
(\#eq:partition-fct-pll)
\end{equation}\]</span>
<p>Replacing the global partition function in the full likelihood with local estimates of lower complexity in the pseudo-likelihood objective resolves the computational intractability of the parameter optimization procedure. Hence, it is feasible to maximize the pseudo-log-likelihood function,</p>
<span class="math display">\[\begin{align}
    \text{pLL}(\v, \w | \mathbf{X}) =&amp; \sum_{n=1}^N \sum_{i=1}^L \log p(x_i | \seq_{\backslash xi}, \v,\w) \\
    =&amp; \sum_{n=1}^N \sum_{i=1}^L  \left[ v_i(x_{ni}) + \sum_{j=i+1}^L  w_{ij}(x_{ni}, x_{nj}) - \log Z_{ni} \right] \;,
\end{align}\]</span>
<p>plus an additional regularization term in order to prevent overfitting and to fix the gauge (see section on <a href="#gauge-invariance">Gauge Invariance</a> and eq. @ref(eq:l2-reg)) to arrive at a <a href="#abbrev">MAP</a> estimate of the parameters,</p>
<span class="math display">\[\begin{equation}
    \hat{\v}, \hat{\w} = \underset{\v, \w}{\operatorname{argmax}} \; \text{pLL}(\v, \w | \mathbf{X}) + R(\v, \w) \; .
\end{equation}\]</span>
<p>Eventhough the pseudo-likelihood optimizes a different objective than the full-likelihood, it has been found to work well in practice for many problems, including contact prediction <span class="citation">[<a href="#ref-Murphy2012">56</a>,<a href="#ref-Koller2009">60</a>–<a href="#ref-Stein2015a">62</a>]</span>. The pseudo-likelihood function retains the concavity of the likelihood and it has been shown to be a consistent estimator in the limit of infinite data for models of the exponential family <span class="citation">[<a href="#ref-Koller2009">60</a>,<a href="#ref-Besag1975">69</a>,<a href="#ref-Gidas1988">70</a>]</span>. That is, as the number of sequences in the alignment increases, pseudo-likelihood estimates converge towards the true full likelihood parameters.</p>
</div>
<div id="post-processing-heuristics" class="section level4">
<h4><span class="header-section-number">1.3.5.5</span> Computing Contact Maps</h4>
<p>Model inference as described in the last section yields <a href="#abbrev">MAP</a> estimates of the couplings <span class="math inline">\(\hat{\w}_{ij}\)</span>. In order to obtain a scalar measure for the coupling strength between two residues <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, current methods heuristically map the <span class="math inline">\(q \! \times \! q\)</span> dimensional coupling matrix <span class="math inline">\(\wij\)</span> to a single scalar quantity.</p>
<p><em>mpDCA</em> <span class="citation">[<a href="#ref-Weigt2009">31</a>]</span> and <em>mfDCA</em> <span class="citation">[<a href="#ref-Morcos2011">57</a>,<a href="#ref-Marks2011">58</a>]</span> employ a score called <a href="#abbrev">DI</a>, that essentially computes the <a href="#abbrev">MI</a> for two positions <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> using the couplings <span class="math inline">\(\wij\)</span> instead of pairwise amino acid frequencies. However, <a href="#abbrev">DI</a> scores have quickly been replaced by the Frobenius norm as it was found to improve prediction performance over <a href="#abbrev">DI</a> <span class="citation">[<a href="#ref-Ekeberg2013">61</a>,<a href="#ref-Baldassi2014">68</a>]</span>.</p>
<p>Currently, all pseudo-likelihood methods (<em>plmDCA</em> <span class="citation">[<a href="#ref-Ekeberg2013">61</a>,<a href="#ref-Ekeberg2014">64</a>]</span>, <em>CCMpred</em> <span class="citation">[<a href="#ref-Seemayer2014">63</a>]</span>, <em>GREMLIN</em> <span class="citation">[<a href="#ref-Kamisetty2013">65</a>]</span>) compute the <em>Frobenius norm</em> of the coupling matrix <span class="math inline">\(\wij\)</span> to obtain a scalar contact score <span class="math inline">\(C_{ij}\)</span>,</p>
<span class="math display">\[\begin{equation}
    C_{ij}  = ||\wij||_2 = \sqrt{\sum_{a,b=1}^q \wijab^2} \; .
(\#eq:frobenius-norm)
\end{equation}\]</span>
<p>It was found that prediction precision improves further when the Frobenius norm is computed only on the <span class="math inline">\(20 \times 20\)</span> submatrix, thus ignoring contributions from gaps <span class="citation">[<a href="#ref-Feinauer2014">71</a>]</span>. <em>PSICOV</em> <span class="citation">[<a href="#ref-Jones2012">45</a>]</span> uses an L1-norm on the <span class="math inline">\(20 \times 20\)</span> submatrix instead of the Frobenius norm.</p>
<p>The Frobenius norm is gauge dependent and is minimized by the <em>zero-sum gauge</em> <span class="citation">[<a href="#ref-Weigt2009">31</a>]</span>. Therefore, in <span class="citation">[<a href="#ref-Ekeberg2013">61</a>,<a href="#ref-Seemayer2014">63</a>,<a href="#ref-Ekeberg2014">64</a>,<a href="#ref-Baldassi2014">68</a>]</span> the coupling matrices are transformed to <em>zero-sum gauge</em> before computing the Frobenius norm:</p>
<span class="math display">\[\begin{equation}
    \w&#39;_{ij}  = \wij - \wij(\cdot, b) - \wij(a, \cdot) + \wij(\cdot, \cdot) \; ,
(\#eq:zero-sum-gauge-transform)
\end{equation}\]</span>
<p>where <span class="math inline">\(\cdot\)</span> denotes average over the respective indices.</p>
<p>Another commonly applied heuristic known as <a href="#abbrev">APC</a> has been found to substantially boost contact prediction performance <span class="citation">[<a href="#ref-Dunn2008">30</a>,<a href="#ref-Kamisetty2013">65</a>]</span>. Dunn et al. introduced <a href="#abbrev">APC</a> in order to remove the influence of background noise arising from correlations between positions with high entropy or phylogenetic couplings <span class="citation">[<a href="#ref-Dunn2008">30</a>]</span>. <a href="#abbrev">APC</a> was first adopted by <em>PSICOV</em> <span class="citation">[<a href="#ref-Jones2012">45</a>]</span> but is now used by most methods to adjust scores. It substracts a term that is computed as the product over average row and column contact scores <span class="math inline">\(\overline{C_i}\)</span> divided by the average contact score over all pairs <span class="math inline">\(\overline{C_{ij}}\)</span>,</p>
<span class="math display">\[\begin{equation}
    C_{ij}^{APC}  = C_{ij} - \frac{\overline{C_i} \; \overline{C_j}}{\overline{C_{ij}}}\; .
(\#eq:apc)
\end{equation}\]</span>
<p>It was long under debate why <a href="#abbrev">APC</a> works so well and how it can be interpreted. Zhang et al. showed that <a href="#abbrev">APC</a> essentially approximates the removal of the first principal component of the contact matrix and therefore removes the highest variability in the matrix that is assumed to arise from background biases <span class="citation">[<a href="#ref-Zhang2016">72</a>]</span>. Furthermore, they studied an advanced decomposition technique, called low-rank and sparse matrix decomposition (LRS), that decomposes the contact matrix into a low-rank and a sparse component, representing background noise and true correlations, respectively.<br />
Inferring contacts from the sparse component works astonishing well, improving precision further over <a href="#abbrev">APC</a> independent of the underlying statistical model.</p>
<p>Dr Stefan Seemayer could show that the main component of background noise can be attributed to entropic effects and that a substantial part of <a href="#abbrev">APC</a> amounts to correcting for these entropic biases (unpublished). In his doctoral thesis, he developed a proper entropy correction, computed as the geometric mean of per-column entropies, that correlates well with the <a href="#abbrev">APC</a> correction term and yields similar precision for predicted contacts. The entropy correction has the advantage that it is computed from input statistics and therefore is independent of the statistical model used to infer the couplings. In contrast, <a href="#abbrev">APC</a> and other denoising techniques such as LRS <span class="citation">[<a href="#ref-Zhang2016">72</a>]</span> discussed above, estimate a background model from the final contact matrix, thus depending on the statistical model used to infer the contact matrix.</p>
<p>The general “smoothing” effect observed when applying <a href="#abbrev">APC</a> that can mainly be attributed to removing entropy bias is illustrated in Figure @ref(fig:apc-correction).</p>
<p>(ref:caption-apc-correction) Contact Matrices computed from pseudo-likelihood couplings. <strong>a</strong>: Contact map computed with Frobenius norm as in eq. @ref(eq:frobenius-norm). Overall coupling values are dominated by entropic effects, i.e. the amount of variation for a <a href="#abbrev">MSA</a> position, leading to striped patterns. <strong>b</strong>: Contact map from (a) corrected for background noise with the <a href="#abbrev">APC</a> as in eq. @ref(eq:apc).</p>
<div class="figure">
<img src="img/intro/apc_correction_transparent.png" alt="(ref:caption-apc-correction)" width="100%" />
<p class="caption">
(#fig:apc-correction)(ref:caption-apc-correction)
</p>
</div>
</div>
</div>
<div id="challenges" class="section level3">
<h3><span class="header-section-number">1.3.6</span> Challenges in Coevolutionary Inference</h3>
<p>Coevolutionary methods face several challenges when interpreting the covariation signals obtained from <a href="#abbrev">MSA</a> that will be addressed in the following. Some of these challenges have been successfully met (e.g. transitive effects with global statistical models), others are still open and again others open up new possibilities, such as dissecting different sources of coevolution.</p>
<div id="phylogenetic-bias" class="section level4 unnumbered">
<h4>Phylogenetic Bias</h4>
<p>Sequences in <a href="#abbrev">MSAs</a> do not represent independent samples of a protein family. In fact, there is selection bias from sequencing species of special interest (e.g human pathogens) or sequencing closely related species, e.g multiple strains. This uneven sampling of a protein family’s sequence space thus leaves certain regions unexplored whereas others are statistically overrepresented <span class="citation">[<a href="#ref-Morcos2011">57</a>,<a href="#ref-Marks2012">73</a>]</span>.</p>
<p>Furthermore, due to their evolutionary relationships, sequences have a complicated dependence structure. Closely related sequences can cause spurious correlations between positions, as there was not sufficient time for the sequences to diverge from their common ancestor <span class="citation">[<a href="#ref-Gouveia_Oliveira2007">41</a>,<a href="#ref-Lapedes1999">46</a>,<a href="#ref-Burger2010">48</a>]</span>. Figure @ref(fig:phylogenetic-effect) illustrates a simplified example, where dependence of sequences due to phylogeny leads to a covariation signal.</p>
<p>(ref:caption-phylogenetic-effect) The phylogenetic dependence of closely related sequences can produce covariation signals. Here, two independent mutation events in two branches of the tree result in a perfect covariation signal for two positions.</p>
<div class="figure" style="text-align: center">
<img src="img/intro/phylogenetic_effect.png" alt="(ref:caption-phylogenetic-effect)" width="50%" />
<p class="caption">
(#fig:phylogenetic-effect)(ref:caption-phylogenetic-effect)
</p>
</div>
<p>To reduce the effects of redundant sequences, a popular sequence reweighting strategy has been found to improve contact prediction performance, where every sequence receives a weight that is the inverse of the number of similar sequences according to an identity threshold (see section @ref(seq-reweighting)) <span class="citation">[<a href="#ref-Jones2012">45</a>,<a href="#ref-Morcos2011">57</a>,<a href="#ref-Buslje2009">74</a>]</span>.</p>
</div>
<div id="entropic-bias" class="section level4 unnumbered">
<h4>Entropic bias</h4>
<p>Another source for noise is entropy bias that is closely linked to phylogenetic effects. By nature, methods detecting signals from correlated mutations rely on a certain degree of covariation between sequence positions <span class="citation">[<a href="#ref-Burger2010">48</a>]</span>. Highly conserved interactions pose a conceptual challenge, as changes from one amino acid to another cannot be detected if sequences do not vary. This results in generally higher co-evolution signals from positions with high entropy and underestimated signals for highly conserved interactions <span class="citation">[<a href="#ref-Fodor2004">39</a>]</span>.</p>
<p>Several heuristics have been proposed to reduce entropy effects, such as Row-Column-Weighting (RCW) <span class="citation">[<a href="#ref-Gouveia_Oliveira2007">41</a>]</span> or Average Product Correction (APC) <span class="citation">[<a href="#ref-Dunn2008">30</a>]</span> (see section @ref(post-processing-heuristics)).</p>
</div>
<div id="finite-sampling-effects" class="section level4 unnumbered">
<h4>Finite Sampling Effects</h4>
<p>Spurious correlations can arise from random statistical noise and blur true co-evolution signals especially in low data scenarios. Consequently, false positive predictions attributable to random noise accumulate for protein families comprising low numbers of homologous sequences. This relationship was confirmed in many studies and as a rule of thumb it has been argued that proteins with <span class="math inline">\(L\)</span> residues need at least <em>5L</em> sequences in order to obtain confident predictions useful for protein structure prediction <span class="citation">[<a href="#ref-Kamisetty2013">65</a>,<a href="#ref-Marks2012">73</a>]</span>. Recently it was shown that precision of predicted contacts saturates for protein families with more than <span class="math inline">\(10^3\)</span> diverse sequences and that precision is only dependent on protein length for families with small number of sequences <span class="citation">[<a href="#ref-Anishchenko2017">52</a>]</span>.</p>
<p>Interesting targets for contact prediction are protein families without any associated structural information. As can be seen in Figure @ref(fig:pfam), those protein families generally comprise low numbers of homologous sequences with a median of 185 sequences per family and are thus susceptible to finite sampling effects.</p>
<p>With the rapidly increasing size of protein sequence databases (see section @ref(general-intro)) the number of protein families with enough sequences for accuarate contact predictions will also increase steadily <span class="citation">[<a href="#ref-TheUniProtConsortium2013">10</a>,<a href="#ref-Kamisetty2013">65</a>]</span>. Nevertheless, because of the already mentioned sequencing biases, better and more sensitive statistical models are indespensible to extend the applicability domain of coevolutionary methods.</p>
<p>(ref:caption-pfam) Distribution of PFAM family sizes. Less than half of the families in PFAM (7990 compared to 8489 families) do not have an annotated structure. The median family size in number of sequences for families with and without annotated structures is 185 and 827 respectively. Data taken from PFAM 31.0 (March 2017, 16712 entries) <span class="citation">[<a href="#ref-Finn2016">75</a>]</span>.</p>
<div class="figure">
<iframe src="img/pfam_pdb_notitle.html" width="100%" height="650px">
</iframe>
<p class="caption">
(#fig:pfam)(ref:caption-pfam)
</p>
</div>
</div>
<div id="transitive-effects" class="section level4 unnumbered">
<h4>Transitive Effects</h4>
<p>One important shortcoming of traditional covariance approaches arises from the fact that chains of amino acid interactions are very common in protein structures and lead to direct as well as indirect correlation signals <span class="citation">[<a href="#ref-Weigt2009">31</a>,<a href="#ref-Lapedes1999">46</a>,<a href="#ref-Burger2010">48</a>]</span>.</p>
<p>Considering three residues <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span>, where <span class="math inline">\(i\)</span> interacts with <span class="math inline">\(j\)</span> and <span class="math inline">\(j\)</span> interacts with <span class="math inline">\(k\)</span>. Even when there is no physical interaction between <span class="math inline">\(i\)</span> and <span class="math inline">\(k\)</span>, there will be a correlation between <span class="math inline">\(i\)</span> and <span class="math inline">\(k\)</span> due to the correlation versus causation phenomenon. Strong statistical dependence between pairs <span class="math inline">\((i,j)\)</span> and <span class="math inline">\((j,k)\)</span> can induce strong indirect signals which can be even larger than signals of other directly interacting pairs and thus lead to false predictions <span class="citation">[<a href="#ref-Burger2010">48</a>]</span>.</p>
<p>Local statistical methods, being introduced in section @ref(local-methods), are unable to disentangle these transitive effects as they consider residue pairs independent of one another. In contrast, global statistical models presented in section @ref(global-methods) learn a joint probability over all residues allowing to dissect direct and indirect correlations <span class="citation">[<a href="#ref-Weigt2009">31</a>,<a href="#ref-Burger2010">48</a>]</span>.</p>
</div>
<div id="multiple-sequence-alignments" class="section level4 unnumbered">
<h4>Multiple Sequence Alignments</h4>
<p>Obviously, a correct <a href="#abbrev">MSA</a> is the essential starting point for correlated mutation analysis as incorrectly aligned residues will confound the true covariation signal. Highly sensitive and accurate tools such as HHblits generate high quality alignments suitable for contact prediction <span class="citation">[<a href="#ref-Remmert2012">76</a>]</span>. However, there are certain subtleties to be kept in mind when generating alignments.</p>
<p>For example, proteins with repeated stretches of amino acids or with regions of low complexity are notoriously hard to align. Especially, repeat proteins have been found to account for a considerable fraction of false positive predictions <span class="citation">[<a href="#ref-Anishchenko2017">52</a>]</span>. Therefore, <a href="#abbrev">MSAs</a> need to be generated with great care and covariation methods need to be tailored to these specific problems <span class="citation">[<a href="#ref-Espada2014">77</a>,<a href="#ref-Toth-Petroczy2016">78</a>]</span>.</p>
<p>Sensitivity of sequence search is critically dependent on the research question and the protein family of interest. While many diverse sequences generally increase precision of predictions, co-evolutionary signals specific to a subfamily might be averaged out when alignments become too deep. Therefore a trade-off between specificity and diversity of the alignment is required to reach optimal results <span class="citation">[<a href="#ref-Hopf2012">79</a>]</span>.</p>
<p>Another intrinsic characteristic of <a href="#abbrev">MSAs</a> are repeated stretches of gaps that result from commonly utilized gap-penalty schemes assigning large penalties to insert a gap and lower penalties to gap extensions. Most statistical models treat gaps as the 21st amino acid, thus introducing an imbalance as gaps and amino acid express different behaviours which often results in gap-induced artefacts <span class="citation">[<a href="#ref-Feinauer2014">71</a>]</span>.</p>
</div>
<div id="evaluation-strategy" class="section level4 unnumbered">
<h4>Evaluation Strategy</h4>
<p>Contact prediction methods are typically evaluated based on a rigid definition of a residue contact that does not reflect true biological interactions between amino acids as discussed in section @ref(intro-cp-evaluation). Choosing different distance cutoffs or different reference atoms for defining a true contact changes the evaluation outcome.</p>
<p>Related to the problem of choosing the right trade-off between sensitivity and specificity when generating alignemnts is the issue of structural variation within a protein family. Evolutionary couplings are inferred from all family memebers in the <a href="#abbrev">MSA</a> and thus might be physical contacts in one family member but not in another. Anishchenko et al. could show that more than <span class="math inline">\(80\%\)</span> of false positives at intermediate distances (minimal heavy atom distance <span class="math inline">\(5-15 \AA \;\;\)</span>) are true contacts in at least one homolog structure <span class="citation">[<a href="#ref-Anishchenko2017">52</a>]</span>.</p>
</div>
<div id="alternative-sources-of-co-evolution" class="section level4 unnumbered">
<h4>Alternative Sources of Co-evolution</h4>
<p>Co-evolutionary signals can not only arise from intra-domain contacts, but also from other sources, like homo-oligomeric contacts, alternative conformations, ligand-mediated interactions or even contacts over hetero-oligomeric interfaces (see Figure @ref(fig:sources-coevolution)) <span class="citation">[<a href="#ref-Marks2012">73</a>]</span>. With the objective to predict physical contacts it is therefore necessary to identify and filter these alternative sources for co-evolutionary couplings.</p>
<p>(ref:caption-sources-coevolution) Possible causes of coevolution. <strong>a)</strong> Physical interactions between intra-domain residues. <strong>b)</strong> Interactions across the interface of predominantly homo-oligomeric complexes. <strong>c)</strong> Interactions mediated by ligands or metal atoms. <strong>d)</strong> Transient interactions due to conformational flexibility.</p>
<div class="figure" style="text-align: center">
<img src="img/intro/sources_of_coevolution.png" alt="(ref:caption-sources-coevolution)" width="80%" />
<p class="caption">
(#fig:sources-coevolution)(ref:caption-sources-coevolution)
</p>
</div>
<p>Many proteins form homo-oligomers with evolutionary conserved interaction surfaces. Currently it is hard to reliably distinguish intra- and inter-molecular contacts. Anishchenko et al. found that approximately one third of strong co-evolutionary signals between residue pairs at long distances (minimal heavy atom distance <span class="math inline">\(&gt;15 \AA \;\;\)</span>) can be attributed to interactions across homo-oligomeric interfaces <span class="citation">[<a href="#ref-Anishchenko2017">52</a>]</span>. Several studies specifically analysed co-evolution across homo-oligomeric interfaces for proteins of known structure by filtering for residue pairs with strong couplings at long distances <span class="citation">[<a href="#ref-Hopf2012">79</a>–<a href="#ref-Jana2014">83</a>]</span> or used co-evolutionary signals to predict homo-dimeric complexes <span class="citation">[<a href="#ref-DosSantos2015a">84</a>]</span>.</p>
<p>It has been proposed that co-evolutionary signals can also arise from ligand or atom mediated interactions between residues or from critical interactions in intermediate folding states <span class="citation">[<a href="#ref-Buslje2009">74</a>,<a href="#ref-Ovchinnikov2015b">85</a>]</span>. Confirming this hypothesis, a study showed that the cumulative strength of couplings for a particular residue can be used to predict functional sites <span class="citation">[<a href="#ref-Marks2012">73</a>,<a href="#ref-Hopf2012">79</a>]</span>.</p>
<p>Another important aspect is conformational flexibility. PDB structures used to evaluate co-evolution analysis represent only rigid snapshots taken in an unnatural crystalline environment. Yet proteins possess huge conformational plasticity and can adopt distinct alternative conformations or adapt shape when interacting with other proteins in an induced fit manner <span class="citation">[<a href="#ref-Noel2016">86</a>]</span>. Several studies demonstrated successfully that co-evolutionary signals can capture interactions specific to different distinct conformations <span class="citation">[<a href="#ref-Morcos2011">57</a>,<a href="#ref-Hopf2012">79</a>,<a href="#ref-Jana2014">83</a>,<a href="#ref-Sfriso2016">87</a>]</span>.</p>
</div>
</div>
</div>
<div id="developing-a-bayesian-model-for-contact-prediction" class="section level2">
<h2><span class="header-section-number">1.4</span> Developing a Bayesian Model for Contact Prediction</h2>
<p>The most popular and successfull methods for contact prediction optimize the pseudo-log-likelihood of the <a href="#abbrev">MSA</a> and use several heuristics to calculate a contact score (see section @ref(post-processing-heuristics)).</p>
<p>By doing so valuable information in contact matrices is lost. Analyses in section 1 shows what information is contained in coupling matrices and that the signal in coupling matrices varies with <span class="math inline">\(\Cb\)</span> distance.</p>
<p>This thesis introcudes a principled Bayesian statistical approach that eradicates these heuristics to fully exploit the information in coupling matrices. Instead of transforming the model parameters <span class="math inline">\(\w\)</span> into heuristic contact scores, one can compute the posterior probability distributions of the distances <span class="math inline">\(r_{ij}\)</span> between <span class="math inline">\(\Cb\)</span> atoms of all residues pairs <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, given the <a href="#abbrev">MSA</a> <span class="math inline">\(\X\)</span>. The coupling parameters <span class="math inline">\(\w\)</span> are treated as hidden variables that will be integrated out analytically. This approach also allows for extraction of information contained in the particular types of amino acids, since each pair of amino acids will have a different preference to be coupled at certain distances.</p>
<p>TODO Figure ! !</p>
<p>In section 2 introduces max ent model for protein families that will produce the model parameters for the Bayesian model.</p>
<p>In section 3 describes in detail how the posterior distribution of distances can be computed.</p>
<p>Section 4 presents the optimizaton of the coupling prior.</p>
<p>And the Bayesian model will be evalutated in section 5.</p>
<p>The outlook describes an extension of the model to predict inter-residue distances. Development is ongoing.</p>
<!--chapter:end:03-intro.Rmd-->
</div>
</div>
<div id="interpreting-coupling-matrices" class="section level1">
<h1><span class="header-section-number">2</span> Interpretation of Coupling Matrices</h1>
<p>State-of-the-art contact prediction methods map the 20 x 20 coupling matrices <span class="math inline">\(w_{ij}\)</span> onto scalar values to obtain contact scores for each residue pair (see section @ref(post-processing-heuristics)). By doing so, the full information contained in coupling matrices is lost:</p>
<ul>
<li>the contribution of individual couplings <span class="math inline">\(\wijab\)</span></li>
<li>the direction of couplings (positive or negative)</li>
<li>the correlation between couplings <span class="math inline">\(\wijab\)</span> and <span class="math inline">\(\wijcd\)</span></li>
<li>intrinsic biological meaning</li>
</ul>
<p>The following analyses give some intuition for the information contained in coupling matrices.</p>
<div id="single-coupling-values-carry-evidence-of-contacts" class="section level2">
<h2><span class="header-section-number">2.1</span> Single Coupling Values Carry Evidence of Contacts</h2>
<p>Given the success of <a href="#abbrev">DCA</a> methods, it is clear that contact scores are good indicators of spatial proximity for residue pairs. As described in section @ref(post-processing-heuristics), a contact score for a residue pair is commonly computed as the square root over the sum of squared coupling values.</p>
<p>Figure @ref(fig:sq-coupling-correlation) shows the correlation between squared coupling values and contact class. All couplings have a positive class correlation, meaning the stronger their squared value, the more likely a contact can be inferred. Generally, couplings that involve any aliphatic amino acid (I, L, V) or alanine express the strongest class correlation. In contrast, C-C or aromatic pairings (involving Y, F, W) correlate only weakly with contact class. Therefore, these couplings often might contribute to false positive predictions.</p>
<p>(ref:caption-sq-coupling) Correlation of squared coupling values <span class="math inline">\((\wijab)^2\)</span> with contact class (contact=1, non-contact=0) for approximately 100 000 residue pairs per class (details see section @ref(method-coupling-correlation)). Contacts defined as residue pairs with <span class="math inline">\(\Cb &lt; 8 \AA\)</span> and non-contacts as residue pairs with <span class="math inline">\(\Cb &gt; 25 \AA\)</span>.</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/correlation_squared_couplings_with_contact_class_notitle.html" width="100%" height="750px">
</iframe>
<p class="caption">
(#fig:sq-coupling-correlation)(ref:caption-sq-coupling)
</p>
</div>
<p>Apparantly, distinct couplings are of varying importance for contact inference. Without squaring the coupling values, these charateristics become even more pronounced.</p>
<p>Figure @ref(fig:coupling-correlation) shows the correlation of raw coupling values with contact class. Interestingly, in contrast to the finding with squared coupling values, only couplings for charged pairs have strong correlation (positive and negative) with class value, whereas couplings for hydrophobic pairs correlate to a much lesser extent (mostly negative). This implies that absolute (squared) coupling strength is much more indicative of a contact for hydrophobic pairings than the direction of coupling. On the contrary, for charged residue pairs the direction of a coupling value is a stronger indicator than the strength of the squared coupling value.</p>
As with squared couplings, raw couplings for aromatic pairs or C-C pairs correlate only weakly with contact class. For these pairings, neither coupling strength, nor direction of coupling seems to be a good indicator for a contact.
<div class="figure">
<iframe src="img/coupling_matrix_analysis/correlation_couplings_with_contact_class_notitle.html" width="750px" height="750px">
</iframe>
<p class="caption">
(#fig:coupling-correlation)(ref:caption-coupling-correlation)
</p>
</div>
<p>Of course, looking only at correlations can be misleading if there are non-linear patterns in the data, for example higher order dependencies between couplings. For this reason it is advisable to take a more detailed view at coupling matrices and the distributions of their values.</p>
</div>
<div id="physico-chemical-fingerprints-in-coupling-matrices" class="section level2">
<h2><span class="header-section-number">2.2</span> Physico-Chemical Fingerprints in Coupling Matrices</h2>
<p>The correlation analysis of raw coupling matrices in the last section revealed that certain coupling values tend to indicate a contact more strongly than others. Single coupling matrices of residue contacts often display striking patterns that agree with the previuos findings. Most often, these patterns suggest biological relevant details of the interdependency between both residues.</p>
<p>Figure @ref(fig:coupling-matrix-ionic-interaction) visualizes the inferred coupling matrix for a residue pair (residues 6 and 82) in protein 1awq, chain A. Clearly visible is a cluster of strong coupling values for charged and polar residues (E,D,K,R,Q). Positive coupling values can be observed between positively charged residues (R,K) and negatively charged residues (E,D), whereas coupling values between equally charged residues are negative. The coupling matrix perfectly reflects the interaction preference for residues forming salt bridges. Indeed, in the protein structure residue 6 (glutamic acid) forms a salt bridge with residue 82 (lysine) as can be seen in figure @ref(fig:coupling-matrix-pymol).</p>
<p>(ref:caption-coupling-matrix-ionic-interaction) Coupling matrix for residues 6 and 82 in protein 1awq chain A. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis and y-axis represent the corresponding single potentials for both residues. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values.</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/coupling_matrix_1a9xA05_6_82_notitle.html" width="750px" height="750px">
</iframe>
<p class="caption">
(#fig:coupling-matrix-ionic-interaction)(ref:caption-coupling-matrix-ionic-interaction)
</p>
</div>
<p>Figure @ref(fig:coupling-matrix-hydrophobic-interaction) visualizes the coupling matrix for a pair of hydrophobic residues (residues 29 and 39) in protein 1ae9 chain A. Hydrophobic pairings have strong coupling values but the couplings also reflect a sterical constraint: alanine as a small hydrophobic residue is favoured at either position 29 or position 39, but disfavoured to appear at both positions. Figure @ref(fig:coupling-matrix-pymol) illustrates the location of the two residues in the protein core. Here, hydrophobic residues are densely packed and the limited space allows for only small hydrophobic residues.</p>
<p>(ref:caption-coupling-matrix-hydrophobic-interaction) Coupling matrix for residues 29 and 39 in protein 1ae9 chain A. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis and y-axis represent the corresponding single potentials for both residues. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values.</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/coupling_matrix_1ae9A00_29_39_notitle.html" width="750px" height="750px">
</iframe>
<p class="caption">
(#fig:coupling-matrix-hydrophobic-interaction)(ref:caption-coupling-matrix-hydrophobic-interaction)
</p>
</div>
<!---
<iframe width="750" height="750" frameborder="0" scrolling="no" src="img/coupling_matrix_analysis/coupling_matrix_1awqA00_92_129_notitle.html">
-->
<p>(ref:caption-coupling-matrix-pymol) Interactions between protein side chains. Left: residue 6 (glutamic acid) forming a salt bridge with residue 82 (lysine) in protein 1awq, chain A. Right: residue 29 (alanine) and residue 39 (leucine) within the hydrophobic core of protein 1ae9 chain A.</p>
<div class="figure">
<img src="img/coupling_matrix_analysis/1a9xA05_6_82.png" alt="(ref:caption-coupling-matrix-pymol)" width="50%" /><img src="img/coupling_matrix_analysis/1ae9A00_29_39.png" alt="(ref:caption-coupling-matrix-pymol)" width="50%" />
<p class="caption">
(#fig:coupling-matrix-pymol)(ref:caption-coupling-matrix-pymol)
</p>
</div>
<p>Many more biological interpretable signals can be identified from coupling matrices, including pi-cation interactions (see Appendix @ref(pi-cation)), aromatic-proline interactions (see Appendix @ref(aromatic-proline)), sulfur-aromatic interactions or disulphide bonds (see Appendix @ref(disulfide)).</p>
<p>Coucke and collegues <span class="citation">[<a href="#ref-Coucke2016">88</a>]</span> performed a thorough quantitative analysis of coupling matrices selected from confidently predicted residue pairs. They showed that eigenmodes obtained from a spectral analysis of averaged coupling matrices are closely related to physico-chemical properties of amino acid interactions, like electrostaticity, hydrophobicity, steric interactions or disulphide bonds. By looking at specific populations of residue pairs, like buried and exposed residues or residues pairs from specific protein classes (small, mainly <span class="math inline">\(\alpha\)</span>, etc), the eigenmodes capture very characteristic interactions for each class, e.g. rare disulfide contacts withins small proteins and hydrophilic contacts between exposed residues. Their study confirms our qualitative observation that amino acid interactions can leave characteristic physico-chemical fingerprints in coupling matrices.</p>
</div>
<div id="coupling-profiles-vary-with-distance" class="section level2">
<h2><span class="header-section-number">2.3</span> Coupling Profiles Vary with Distance</h2>
<p>Analyses in the previous sections showed that certain coupling values correlate more or less strong with contact class and that coupling matrices for contacts express biological meaningpull patterns.</p>
<p>More insights can be obtained by looking at the distribution of distinct coupling values for contacts, non-contacts and arbitrary populations of residue pairs. To avoid uninformative couplings, we consider only residue pairs with a sequence separation &gt; 10 and with enough evidence for a certain amino acid pairing (see Methods section @ref(method-coupling-profile) for details).</p>
<p>Figure @ref(fig:1d-coupling-profile-0-5) shows the distribution of selected couplings for residue pairs within a <span class="math inline">\(\Cb\)</span> distance <span class="math inline">\(&lt; 5\AA\)</span>. The distribution of R-E and E-E coupling values is shifted and skewed towards positive and negative values respectively. This is in accordance with attracting electrostatic interactions between the positively charged side chain of arginine and the negatively charged side chain of gluatamic acid and also with repulsive interactions between the two negatively charged gluatamic acid side chains. Couling values for C-C pairs have a broad distribution that is skewed towards positive values, reflecting the strong signals obtained from covalent disulphide bonds. Hydrophobic pairs like V-I have an almost symmetric coupling distribution, confirming the finding that the direction of coupling is not indicative of a true contact whereas the strength of the coupling is. Hydrophobic interactions arising from the hydrophobic effect are not specific or directed and can easily be substituted by other hydrophobic residues, which explains the not very pronounced positive coupling signal compared to more specific interactions, e.g ionic interactions. The distribution of aromatic coupling values like F-W is slightly skewed towards negative values, accounting for steric hindrance of their large side chains at small distances.</p>
<p>(ref:caption-1d-coupling-profile-0-5) Distribution of selected couplings for approximately 10000 filtered residue pairs with <span class="math inline">\(\Cb\)</span> distance <span class="math inline">\(&lt; 5\AA\)</span> (see Methods section @ref(method-coupling-profile) for details).</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/1d_coupling_profile_0_5.html" width="100%" height="400px">
</iframe>
<p class="caption">
(#fig:1d-coupling-profile-0-5)(ref:caption-1d-coupling-profile-0-5)
</p>
</div>
<p>In an intermediate <span class="math inline">\(\Cb\)</span> distance range between <span class="math inline">\(8\AA\)</span> and <span class="math inline">\(12\AA\)</span> the distributions for all coupling values are centered close to zero and are less broad. The distributions are still shifted and skewed as for <span class="math inline">\(\Cb\)</span> distance <span class="math inline">\(&lt; 5\AA\)</span> but much less pronounced. For aromatic pairs like F-W, the distribution of coupling values has very long tails, suggesting strong couplings for aroamtic side chains at this distance.</p>
<p>(ref:caption-1d-coupling-profile-8-12) Distribution of selected couplings for approximately 10000 filtered residue pairs with <span class="math inline">\(\Cb\)</span> distance <span class="math inline">\(&lt; 5\AA\)</span> (see Methods section @ref(method-coupling-profile) for details).</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/1d_coupling_profile_8_12.html" width="100%" height="400px">
</iframe>
<p class="caption">
(#fig:1d-coupling-profile-8-12)(ref:caption-1d-coupling-profile-8-12)
</p>
</div>
<p>Figure @ref(fig:1d-coupling-profile-20-50) shows the distribution of selected couplings for residue pairs far apart in the protein structure (<span class="math inline">\(\Cb\)</span> distance <span class="math inline">\(&gt; 20\AA\)</span>).<br />
The distribution for all couplings is centered at zero and has small variance. For C-C coupling values, the distribution has a long tail for positve values, presumably arising from the fact that the maximum entropy model cannot distuinguish highly conserved signals of multiple disulphide bonds within a protein. This observation also agrees with the previous finding that C-C couplings correlate only weakly with contact class. The same arguments apply to couplings of aromatic pairs that have a comparably broad distribution and do not correlate strongly with contact class. The strong coevolution signals for aromatic pairs even at high distance ranges might be to insufficient disentanglig of transitive effects, as aromatic residues are known to form network-like structures in the protein core that stabilize protein structure (see Figure @ref(fig:aromatic-network) in Appendix)<span class="citation">[<a href="#ref-Burley1985">13</a>]</span>.</p>
<p>(ref:caption-1d-coupling-profile-20-50) Distribution of selected couplings for approximately 10000 filtered residue pairs with <span class="math inline">\(\Cb\)</span> distance <span class="math inline">\(&gt; 25\AA\)</span> (see Methods section @ref(method-coupling-profile) for details).</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/1d_coupling_profile_20_50.html" width="100%" height="400px">
</iframe>
<p class="caption">
(#fig:1d-coupling-profile-20-50)(ref:caption-1d-coupling-profile-20-50)
</p>
</div>
</div>
<div id="higher-order-dependencies-between-couplings" class="section level2">
<h2><span class="header-section-number">2.4</span> Higher Order Dependencies Between Couplings</h2>
<p>The analyses in the previous sections focused on single coupling values of the <span class="math inline">\(20 \times 20\)</span>-dimensional coupling matrices. As mentioned before, looking at single variables might be misleading if they are dependent on another. Unfortunately, it is not possible to reasonably visualize the high dimensional coupling matrices. But there are several ways to identify interesting dimensions.</p>
<p>First of all, 2-dimensional scatter plots of couplings for biological relevant pairings confirm the previous trend that couplings reflect amino acid interactions.</p>
<p>Figure @ref(fig:2d-coupling-profile-re-er-0-8) and @ref(fig:2d-coupling-profile-re-ee-0-8) illustrate the distribution of attractive and repulsive ionic interactions at <span class="math inline">\(\Cb\)</span> distances less than <span class="math inline">\(8\AA\)</span>. Whereas coupling values for R-E and E-R are positively correlated, coupling values for R-E and E-E are negatively correlated. Hydrophobic coupling values for residue pairs at <span class="math inline">\(\Cb\)</span> distances less than <span class="math inline">\(8\AA\)</span> are symmetrically distributed around zero, in agreement with all previous analyses (Figure @ref(fig:2d-coupling-profile-vi-il-0-8)).</p>
<p>(ref:caption-2d-coupling-profile-re-er-0-8) Two-dimensional distribution of coupling values R-E and E-R for approximately 10000 residue pairs with <span class="math inline">\(\Delta\Cb &lt; 8\AA\)</span>. The distribution is almost symmetrical and the coupling values are positively correlated. Residue pairs have been filtered for sequence separation, percentage of gaps and evidence in alignment (see Methods @ref(method-coupling-profile)).</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/pairwise_couplings_R-E_E-R_Cbdistance_0_8_notitle.html" width="75%" height="400px">
</iframe>
<p class="caption">
(#fig:2d-coupling-profile-re-er-0-8)(ref:caption-2d-coupling-profile-re-er-0-8)
</p>
</div>
<p>(ref:caption-2d-coupling-profile-re-ee-0-8) Two-dimensional distribution of coupling values R-E and E-E for approximately 10000 residue pairs with <span class="math inline">\(\Delta\Cb &lt; 8\AA\)</span>. The coupling values are negatively correlated. Residue paris have been filtered for sequence separation, percentage of gaps and evidence in alignment (see Methods @ref(method-coupling-profile)).</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/pairwise_couplings_R-E_E-E_Cbdistance_0_8_notitle.html" width="75%" height="400px">
</iframe>
<p class="caption">
(#fig:2d-coupling-profile-re-ee-0-8)(ref:caption-2d-coupling-profile-re-ee-0-8)
</p>
</div>
<p>(ref:caption-2d-coupling-profile-vi-il-0-8) Two-dimensional distribution of coupling values V-I and I-L for approximately 10000 residue pairs with <span class="math inline">\(\Delta\Cb &lt; 8\AA\)</span>. The coupling values are symmetrically distributed around zero. Residue paris have been filtered for sequence separation, percentage of gaps and evidence in alignment (see Methods @ref(method-coupling-profile)).</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/pairwise_couplings_V-I_I-L_Cbdistance_0_8_notitle.html" width="75%" height="400px">
</iframe>
<p class="caption">
(#fig:2d-coupling-profile-vi-il-0-8)(ref:caption-2d-coupling-profile-vi-il-0-8)
</p>
</div>
<!--chapter:end:05-interpretation-coupling-matrices.Rmd-->
</div>
</div>
<div id="optimizing-full-likelihood" class="section level1">
<h1><span class="header-section-number">3</span> Optimizing the Full-Likelihood</h1>
<div id="likelihood-of-the-sequences-as-a-potts-model" class="section level2">
<h2><span class="header-section-number">3.1</span> Likelihood of the sequences as a Potts model</h2>
<p>We denote the <span class="math inline">\(N\)</span> sequences in the <a href="#abbrev">MSA</a> <span class="math inline">\(\X\)</span> with <span class="math inline">\({\seq_1, ..., \seq_N}\)</span>. Each sequence <span class="math inline">\(\seq_n = (\seq_{n1}, ..., \seq_{nL})\)</span> is a string of <span class="math inline">\(L\)</span> letters from an alphabet indexed by <span class="math inline">\(\{0, ..., 20\}\)</span>, where 0 stands for a gap and <span class="math inline">\(\{1, ... , 20\}\)</span> stand for the 20 types of amino acids. The goal is to predict from <span class="math inline">\(\X\)</span> the distances <span class="math inline">\(r_{ij}\)</span> between the <span class="math inline">\(\Cb\)</span> atoms of all pairs of residues <span class="math inline">\((i, j) \in \{1, ..., L\}\)</span>. The link between the <a href="#abbrev">MSA</a> <span class="math inline">\(\X\)</span> and the vector <span class="math inline">\(\mathbf{r}\)</span> of all inter-<span class="math inline">\(\Cb\)</span> distances is described via the evolutionary couplings of residue pairs that are the <span class="math inline">\(20^2\)</span>-dimensional vectors <span class="math inline">\(w_{ij}\)</span>.</p>
<p>As already described in detail in section @ref(maxent), we model the likelihood of the sequences in an <a href="#abbrev">MSA</a> with a Potts Model, also known as <a href="#abbrev">MRF</a>:</p>
<span class="math display">\[\begin{equation}
    p(\X | \v, \w) = \prod_{n=1}^N p(\seq_n | \v, \w) = \prod_{n=1}^N \frac{1}{Z(\v, \w)} \exp \left( \sum_{i=1}^L v_i(x_{ni}) \sum_{1 \leq i &lt; j \leq L} w_{ij}(x_{ni}, x_{nj}) \right)
\end{equation}\]</span>
<p>The coefficients <span class="math inline">\(\via\)</span> are the single potentials and <span class="math inline">\(\wijab\)</span> denote the coupling strengths for pairs of residues. <span class="math inline">\(Z(\v, \w)\)</span> is the so-called partition sum that normalizes the probability distribution <span class="math inline">\(p(\seq_n |\v, \w)\)</span>:</p>
<span class="math display">\[\begin{equation}
  Z(\v, \w) = \sum_{y_1, ..., y_L = 1}^{20} \exp \left( \sum_{i=1}^L v_i(y_i) \sum_{1 \leq i &lt; j \leq L} w_{ij}(y_i, y_j)  \right)
\end{equation}\]</span>
<p>TODO: this is irrelevant for CD, isn’t it? For an efficient computational implementation, we might sum over all <span class="math inline">\(1 \le i, j \le L\)</span> without demanding <span class="math inline">\(i &lt; j\)</span> and enforce trivial constraints <span class="math inline">\(\wijab = w_{jiba}\)</span> during the optimization.</p>
</div>
<div id="gap-treatment" class="section level2">
<h2><span class="header-section-number">3.2</span> Treating Gaps as Missing Information</h2>
<p>Treating gaps explicitly as 0’th letter of the alphabet would lead to couplings between columns that are not in physical contact. To see why, imagine a hypothetical alignment consisting of two sets of sequences as it is illustrated in Figure @ref(fig:gap-treatment). The first set has sequences covering only the left half of columns in the MSA, while the second set has sequences covering only the right half of columns. The two blocks could correspond to protein domains that were aligned to a single query sequence.</p>
<p>Now consider couplings between a pair of columns <span class="math inline">\(i, j\)</span> with <span class="math inline">\(i\)</span> from the left half and <span class="math inline">\(j\)</span> from the right half. Since no sequence (except the single query sequence) overlaps both domains, the empirical amino acid pair frequencies <span class="math inline">\(q(x_i = a, x_j = b)\)</span> will vanish for all <span class="math inline">\(a, b \in \{1,... , L\}\)</span>.</p>
<p>(ref:caption-gap-treatment) Hypothetical <a href="#abbrev">MSA</a> consisting of two sets of sequences: the first set has sequences covering only the left half of columns, while the second set has sequences covering only the right half of columns. The two blocks could correspond to protein domains that were aligned to a single query sequence. Empirical amino acid pair frequencies <span class="math inline">\(q(x_i \eq a, x_j \eq b)\)</span> will vanish for positions <span class="math inline">\(i\)</span> from the left half and <span class="math inline">\(j\)</span> from the right half of the alignment.</p>
<div class="figure">
<img src="img/gap_treatment.png" alt="(ref:caption-gap-treatment)"  />
<p class="caption">
(#fig:gap-treatment)(ref:caption-gap-treatment)
</p>
</div>
<p>The gradient of the log likelihood for couplings is</p>
<span class="math display">\[\begin{align}
\frac{\partial LL}{\partial \wijab} &amp;= \sum_{n=1}^N I(x_{ni}=a, x_{nj}=b)  - N \frac{\partial}{\partial \wijab} \log Z(\v,\w) \\
                                        &amp;= \sum_{n=1}^N I(x_{ni} \eq a, x_{nj} \eq b) \\
                                        &amp; - N \sum_{y_1,\ldots,y_L=1}^{20} \!\! \frac{ \exp \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L} w_{ij}(y_i,y_j) \right)}{Z(\v,\w)}  I(y_i \eq a, y_j \eq b) \\
                                        &amp;=  N q(x_{i} \eq a, x_{j} \eq b) - N \sum_{y_1,\ldots,y_L=1}^{20} p(y_1, \ldots, y_L | \v,\w) \, I(y_i \eq a, y_j \eq b) \\
                                        &amp;=  N q(x_{i} \eq a, x_{j} \eq b) - N p(x_i \eq a, x_j \eq b | \v,\w) 
(\#eq:gradient-LLreg-gaps-single)
\end{align}\]</span>
<p>Note that the empirical frequencies are equal to the model probabilities at the maximum of the likelihood when the gradient vanishes. Therefore, <span class="math inline">\(p(x_i \eq a, x_j \eq b | \v, \w)\)</span> would have to be zero in the optimum when the empirical amino acid frequencies <span class="math inline">\(q(x_i \eq a, x_j \eq b)\)</span> vanish for pairs of columns as described above. However, <span class="math inline">\(p(x_i \eq a, x_j \eq b | \v, \w)\)</span> can only become zero, when the exponential term in <span class="math inline">\(p(x_i \eq a, x_j \eq b | \v, \w)\)</span> ammounts to zero, which would only be possible if <span class="math inline">\(\wijab\)</span> goes to <span class="math inline">\(−\infty\)</span>. This is clearly undesirable, as we want to deduce physical contacts from the size of the couplings.</p>
<p>The solution is to treat gaps as missing information. This means that the normalisation of <span class="math inline">\(p(\seq_n | \v, \w)\)</span> should not run over all positions <span class="math inline">\(i \in \{1,... , L\}\)</span> but only over those <span class="math inline">\(i\)</span> that are not gaps in <span class="math inline">\(\seq_n\)</span>. Therefore we define the set of sequences <span class="math inline">\(\Sn\)</span> used for normalization of <span class="math inline">\(p(\seq_n | \v, \w)\)</span> as:</p>
<span class="math display">\[\begin{equation}
\Sn := \{(y_1,... , y_L): 0 \leq y_i \leq 20 \land (y_i \eq 0 \textrm{ iff } x_{ni} \eq 0) \}
\end{equation}\]</span>
<p>and the partition function becomes:</p>
<span class="math display">\[\begin{equation}
  Z_n(\v, \w) = \sum_{\mathbf{y} \in \Sn} \exp \left( \sum_{i=1}^L v_i(y_i) \sum_{1 \leq i &lt; j \leq L} w_{ij}(y_i, y_j)  \right)
\end{equation}\]</span>
<p>To ensure that the gaps in <span class="math inline">\(x_n\)</span> do not contribute anything to the sums, we fix all parameters associated with a gap to 0:</p>
<p><span class="math inline">\(v_i(0) = 0\)</span> and <span class="math inline">\(w_{ij}(0, b) = w_{ij}(a, 0) = 0\)</span> for all <span class="math inline">\(i, j \in \{1, ..., L\}\)</span> and <span class="math inline">\(a, b \in \{0, ..., 20\}\)</span>.</p>
<p>Furthermore, we redefine the empirical amino acid frequencies <span class="math inline">\(q_{ia}\)</span> and <span class="math inline">\(q_{ijab}\)</span> such that they are normalised over <span class="math inline">\(\{1, ..., 20\}\)</span>:</p>
<span class="math display">\[\begin{align}
   N_i :=&amp; \sum_{n=1}^N  I(x_{ni} \!\ne\! 0) &amp;  q_{ia} = q(x_i \eq a) :=&amp; \frac{1}{N_i} \sum_{n=1}^N I(x_{ni} \eq a)   \\
   N_{ij} :=&amp; \sum_{n=1}^N  I(x_{ni} \!\ne\! 0, x_{nj} \!\ne\! 0)  &amp;  q_{ijab} = q(x_i \eq a, x_j \eq b) :=&amp; \frac{1}{N_{ij}} \sum_{n=1}^N I(x_{ni} \eq a, x_{nj} \eq b)
\end{align}\]</span>
<p>With this definition, empirical amino acid frequencies are normalized without gaps, so that</p>
<span class="math display">\[\begin{equation}
    \sum_{a=1}^{20} q_{ia} = 1      \; , \;     \sum_{a,b=1}^{20} q_{ijab} = 1.
(\#eq:normalized-emp-freq)
\end{equation}\]</span>
</div>
<div id="gauge-transformation" class="section level2">
<h2><span class="header-section-number">3.3</span> Gauge transformation</h2>
<p>The model contains <span class="math inline">\(L \times 20 + \frac{L(L − 1)}{2} \times 20^2\)</span> parameters, but the parameters are not uniquely determined. For example, for any fixed position <span class="math inline">\(i\)</span> and amino acid a we can add a constant to <span class="math inline">\(\via\)</span> and subtract the same constant from the <span class="math inline">\(20L\)</span> coefficients <span class="math inline">\(\wijab\)</span> with <span class="math inline">\(b \in \{1, ..., 20\}\)</span> and <span class="math inline">\(j \in \{1, ..., L\}\)</span>. This overparametrization, the so-called gauge transformation, would leave the probabilities for all sequences under the model unchanged.</p>
<p>We could eliminate parameters by enforcing the restraints <span class="math inline">\(\sum_{a=1}^{20} v_{ia} = 0\)</span> and <span class="math inline">\(\sum_{a=1}^{20} \wijab = 0 = \sum_{a=1}^{20} w_{ijba}\)</span>. However, it is easier to rather formulate carefully the link between the distribution of <span class="math inline">\(\w_{ij}\)</span> vectors and the distance <span class="math inline">\(r_ij\)</span> while taking the non-uniqueness of parameters into acount, as we will see below.</p>
</div>
<div id="the-regularized-log-likelihood-function-llregvw" class="section level2">
<h2><span class="header-section-number">3.4</span> The regularized log likelihood function LLreg(v,w)</h2>
<p>In pseudo-likelihood based methods, a regularisation is commonly used that can be interpreted to arise from a prior probability. We will do the same here, constraining <span class="math inline">\(\v\)</span> and <span class="math inline">\(\w\)</span> by Gaussian priors <span class="math inline">\(\mathcal{N}( \v | \v^*, \lambda_v^{-1} \I)\)</span> and <span class="math inline">\(\mathcal{N}( \w |\boldsymbol 0, \lambda_w^{-1} \I)\)</span>. The choice of <span class="math inline">\(v^*\)</span> will be discussed in the section @ref(prior-v). By including the logarithm of this prior into the log likelihood using the gap treatment described in section @ref{gap-treatment}, we obtain the regularised likelihood,</p>
<span class="math display">\[\begin{equation}
    \LLreg(\v,\w)  = \log \left[ p(\X | \v,\w) \;  \Gauss (\v | \v^*, \lambda_v^{-1} \I)  \; \Gauss( \w | \boldsymbol 0, \lambda_w^{-1} \I) \right] 
\end{equation}\]</span>
<p>or explicitely,</p>
<span class="math display">\[\begin{align}
    \LLreg(\v,\w) =&amp; \sum_{n=1}^N  \left[ \sum_{i=1}^L v_i(x_{ni}) + \sum_{1\le i&lt;j\le L} w_{ij}(x_{ni},x_{nj}) - \log Z_n(\v,\w) \right] \\
                    &amp; - \frac{\lambda_v}{2} \!\! \sum_{i=1}^L \sum_{a=1}^{20} (\via - \via^*)^2  - \frac{\lambda_w}{2}  \sum_{1 \le i &lt; j \le L} \sum_{a,b=1}^{20} \wijab^2 .
\end{align}\]</span>
</div>
<div id="the-gradient-of-the-regularized-log-likelihood" class="section level2">
<h2><span class="header-section-number">3.5</span> The gradient of the regularized log likelihood</h2>
<p>The gradient of the regularized log likelihood has single components</p>
<span class="math display">\[\begin{align}
    \frac{\partial \LLreg}{\partial \via} =&amp; \sum_{n=1}^N I(x_{ni}=a) - \sum_{n=1}^N \frac{\partial}{\partial \via} \, \log Z_n(\v,\w) - \lambda_v (\via - \via^*)\\
                                          =&amp; \; N_i q(x_i \eq a) \\
                                          &amp; - \sum_{n=1}^N \sum_{\mathbf{y} \in \Sn} \frac{  \exp \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i&lt;j \le L}^L w_{ij}(y_i,y_j) \right) }{Z_n(\v,\w)}  I(y_i=a) \\
                                          &amp; - \lambda_v (\via - \via^*) 
(\#eq:gradient-LLreg-single)
\end{align}\]</span>
<p>and pair components</p>
<span class="math display">\[\begin{align}
    \frac{\partial \LLreg}{\partial \wijab} =&amp; \sum_{n=1}^N I(x_{ni} \eq a, x_{nj} \eq b) - \sum_{n=1}^N \frac{\partial}{\partial \wijab} \log Z_n(\v,\w)  - \lambda_w \wijab \\
                                            =&amp; \; N_{ij} q(x_i \eq a, x_j=b) \\
                                            &amp; - \sum_{n=1}^N \sum_{\mathbf{y} \in \Sn} \frac{ \exp \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i&lt;j \le L}^L w_{ij}(y_i,y_j) \right) }{Z_n(\v,\w)} I(y_i \eq a, y_j \eq b) \\
                                            &amp; - \lambda_w \wijab  
(\#eq:gradient-LLreg-pair)
\end{align}\]</span>
<p>Note that (without regulariation <span class="math inline">\(\lambda_v = \lambda_w = 0\)</span>) the empirical frequencies <span class="math inline">\(q(x_i \eq a)\)</span> and <span class="math inline">\(q(x_i \eq a, x_j=b)\)</span> are equal to the model probabilities at the maximum of the likelihood.</p>
<p>If the proportion of gap positions in <span class="math inline">\(\X\)</span> is small (e.g. <span class="math inline">\(&lt;5\%\)</span>, also compare percentage of gaps in dataset in Appendix Figure @ref(fig:dataset-gaps)), we can approximate the sums over <span class="math inline">\(\mathbf{y} \in \Sn\)</span> in eqs. @ref(eq:gradient-LLreg-single) and @ref(eq:gradient-LLreg-pair) by <span class="math inline">\(p(x_i=a | \v,\w) I(x_{ni} \ne 0)\)</span> and <span class="math inline">\(p(x_i=a, x_j=b | \v,\w) I(x_{ni} \ne 0, x_{nj} \ne 0)\)</span>, respectively, and the partial derivatives become</p>
<span class="math display">\[\begin{align}
  \frac{\partial \LLreg}{\partial \via}   =&amp; \; N_i q(x_i \eq a) -  N_i \; p(x_i \eq a  | \v,\w)  - \lambda_v (\via - \via^*)  \\
  \frac{\partial \LLreg}{\partial \wijab} =&amp; \; N_{ij} q(x_i \eq a, x_j=b) - N_{ij} \; p(x_i \eq a, x_j \eq b | \v,\w) - \lambda_w \wijab
  (\#eq:gradient-LLreg-approx)
\end{align}\]</span>
<p>Note that the couplings between columns <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in our hypothetical MSA (see section @ref(gap-treatment)) will now vanish since <span class="math inline">\(N_{ij} \eq 0\)</span> and the gradient with respect to <span class="math inline">\(\wijab\)</span> is equal to <span class="math inline">\(-\lambda_w \wijab\)</span>.</p>
</div>
<div id="prior-v" class="section level2">
<h2><span class="header-section-number">3.6</span> The prior on <span class="math inline">\(\v\)</span></h2>
<p>Most previous approaches chose a prior around the origin, <span class="math inline">\(p(\v) = \Gauss ( \v| \mathbf{0}, \lambda_v \I)\)</span>. This choice has an obvious draw-back. To see why, we take the sum over <span class="math inline">\(b=1,\ldots, 20\)</span> of the gradient of couplings in eq. @ref(eq:gradient-LLreg-approx) at the optimum, where the gradient vanishes.</p>
This yields
<span class="math display">\[\begin{equation}
    0 =   N_{ij}\, q(x_i \eq a, x_j \ne 0)   - N_{ij}\, p(x_i \eq a | \v, \w)  - \lambda_w \sum_{b=1}^{20} \wijab.
\end{equation}\]</span>
Incidentally, we note that by taking the sum over <span class="math inline">\(a\)</span> we find
<span class="math display">\[\begin{equation}
    \sum_{a,b=1}^{20} \wijab  = 0.
(\#eq:zero-sum-wij)
\end{equation}\]</span>
<p>At the optimum the gradient with respect to <span class="math inline">\(v_{ia}\)</span> vanishes and we can substitute <span class="math inline">\(p(x_i=a|\v,\w) = q(x_i=a) - \lambda_v (\via - \via^*) / N_i\)</span>, yielding</p>
<span class="math display">\[\begin{equation}
    0 =  N_{ij} \, q(x_i \eq a, x_j \ne 0)  - N_{ij} \, q(x_i=a) + \frac{N_{ij}}{N_i}\lambda_v (\via - \via^*)  - \lambda_w \sum_{b=1}^{20} \wijab .
(\#eq:gauge-opt-1)
\end{equation}\]</span>
<p>for all <span class="math inline">\(i,j \in \{1,\ldots,L\}\)</span> and all <span class="math inline">\(a \in \{1,\ldots,20\}\)</span>. To show that the choice <span class="math inline">\(\v^*= \mathbf{0}\)</span> leads to undesirable results, we take an <a href="#abbrev">MSA</a> without gaps. The first two terms <span class="math inline">\(N_{ij} \, q(x_i \eq a, x_j \ne 0) - N_{ij} \, q(x_i=a)\)</span> vanish as they add up to zero, which leaves</p>
<span class="math display">\[\begin{equation}
    0 =  \lambda_v (\via - \via^*)  - \lambda_w \sum_{b=1}^{20} \wijab .
(\#eq:gauge-opt-2)
\end{equation}\]</span>
<p>Consider a column <span class="math inline">\(i\)</span> that is not coupled to any other and assume that amino acid <span class="math inline">\(a\)</span> was frequent in column <span class="math inline">\(i\)</span> and therefore <span class="math inline">\(\via\)</span> would be large and positive. Then according to eq. @ref(eq:gauge-opt-2), for any other column <span class="math inline">\(j\)</span> the 20 coefficients <span class="math inline">\(\wijab\)</span> for <span class="math inline">\(b \in \{1,\ldots,20\}\)</span> would have to take up the bill and deviate from zero!</p>
To correct this unwanted behaviour, we instead chose a Gaussian prior centered around <span class="math inline">\(\v^*\)</span> obeying
<span class="math display">\[\begin{equation}
  \frac{\exp(\via^*)}{\sum_{a&#39;=1}^{20} \exp(v_{ia&#39;}^*)} = q(x_i=a) .
\end{equation}\]</span>
<p>This choice ensures that if no columns are coupled, i.e. <span class="math inline">\(p(\seq | \v,\w) = \prod_{i=1}^L p(x_i)\)</span>, <span class="math inline">\(\v=\v^*\)</span> and <span class="math inline">\(\w= \mathbf{0}\)</span> gives the correct probability model for the sequences in the MSA. If we impose the restraint <span class="math inline">\(\sum_{a=1}^{20} \via = 0\)</span> to fix the gauge of the <span class="math inline">\(\via\)</span> (i.e. to remove the indeterminacy), we get</p>
<span class="math display">\[\begin{align}
\via^* = \log q(x_i=a) - \frac{1}{20} \sum_{a&#39;=1}^{20} \log q(x_i=a&#39;) .
(\#eq:prior-v)
\end{align}\]</span>
<p>For this choice, <span class="math inline">\(\via - \via^*\)</span> will be approximately zero and will certainly be much smaller than <span class="math inline">\(\via\)</span>, hence the sum over coupling coefficients in eq. @ref(eq:gauge-opt-2) will be close to zero, as it should be.</p>
<p>Another way to understand the choice of <span class="math inline">\(\v^*\)</span> in eq. @ref(eq:prior-v) as opposed to <span class="math inline">\(\v^*=\mathbf{0}\)</span> is by noting that in that case <span class="math inline">\(q(x_i \eq a) \approx p(x_i \eq a|\v^*,\w^*)\)</span>. Therefore, if <span class="math inline">\(q(x_i \eq a,x_j \eq b) = q(x_i \eq a) \, q(x_j \eq b)\)</span> it follows that <span class="math inline">\(p(x_i \eq a, x_j \eq b | \v,\w) \approx q(x_i \eq a, x_j \eq b) = p(x_i \eq a | \v^*,\w^*)\, p(x_j \eq b | \v^*,\w^*)\)</span>, i.e. we would correctly conclude that <span class="math inline">\(\wijab=0\)</span> and <span class="math inline">\((i,a)\)</span> and <span class="math inline">\((j,b)\)</span> are not coupled.</p>
<div id="full-likelihood" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Full-likelihood</h3>
<p>Computing the gradient of the likelihood analytically according to the previous equations is infeasible, because computing <span class="math inline">\(p(x_i \eq a, x_j \eq b | \v, \w) = \sum_{y_1, \dots, y_L =1}^{20} p(y_1, \dots, y_L | \v, \w) I(y_i \eq a, y_j \eq b)\)</span> would require summing over <span class="math inline">\(20^L\)</span> sequences <span class="math inline">\((y_1,\ldots,y_L)\)</span>. Several approaches have been used to get around this problem as described in section @ref(infering-max-ent-models). The most popular one for protein contact prediction is to optimize the pseudo likelihood instead (see section @ref(pseudo-likelihood)). Its gradient involves a sum over just the 20 amino acids instead of over all possible sequences of length <span class="math inline">\(L\)</span>.</p>
<p>It is possible though to optimize the true likelihood by employing an approach called “persistent contrastive divergence” <a href="#abbrev">PCD</a> that extends the “contrastive divergence” <a href="#abbrev">CD</a> approach by G.E.~Hinton introduced in “Training products of experts by minimizing contrastive divergence”,  (2002).</p>
<p>In CD, we initialise <span class="math inline">\(N\)</span> Markov chains, one with each of the <span class="math inline">\(N\)</span> sequences from our MSA, and we generate <span class="math inline">\(N\)</span> new samples by a single step of Gibbs sampling from each of the <span class="math inline">\(N\)</span> sequences. From the <span class="math inline">\(N\)</span> new sequences we can estimate the frequencies of pairs <span class="math inline">\((x_{i}\!=\! a, x_{j}=b)\)</span> to approximate the second term in , just as the first term is computed from the original <span class="math inline">\(N\)</span> sequences. Even though the approximation for the second term is very bad, it can be seen that this approximate gradient will become zero approximately where the true gradient of the likelihood also becomes zero. To see this, imagine <span class="math inline">\((\v^*, \w^*)\)</span> is the maximum of the likelihood. Then, starting from the sequences in the MSA, the Gibbs sampling step should not lead away from the empirical distribution, because the parameters <span class="math inline">\((\v^*, \w^*)\)</span> already describe the empirical distribution correctly. This equality of the two maxima is accurate to the extent that the empirical distribution with its finite number of sequences <span class="math inline">\(N\)</span> can represent the true distribution given by parameters <span class="math inline">\((\v^*, \w^*)\)</span>. Therefore, the larger <span class="math inline">\(N\)</span>, the better CD will optimise into the maximum of the true likelihood. It can be shown that CD using a single-step Gibbs sampling is exactly equivalent to optimising the pseudo likelihood.</p>
<p>For <a href="#abbrev">PCD</a>, the Markov chains are not restarted from the <span class="math inline">\(N\)</span> sequences in the MSA every time a new gradient is computed. Instead the Markov chains are evolved between successive gradient computations without resetting them. This ensures that, as we approach the maximum <span class="math inline">\((\v^*, \w^*)\)</span>, we acquire more and more samples from the distribution corresponding to parameters <span class="math inline">\((\v,\w)\)</span> near the optimum. Hence our approximation to the gradient of the likelihood gets better the longer we sample, independent of the number of sequences <span class="math inline">\(N\)</span> in the MSA.</p>
<p>The optimization of the true likelihood with <a href="#abbrev">CD</a> and <a href="#abbrev">PCD</a> is discussed in section @ref{optimizing-full-likelihood}.</p>
<p>Dr Stefan Seemayer provided a Python implementation of CCMpred that was extended to optimize the full-likelihood of the <a href="#abbrev">MRF</a>.</p>
<p>The full likelihood of the maximum entropy model cannot be optimized with <a href="#abbrev">ML</a> methods due to the exponential complexity of the partition function (see section @ref(maxent)). As elaborated in the introduction, many approximations to maximum likelihood inference have been developed that resolve the computational intractability of the partition function. Pseudo-likelihood methods are now the state-of-the-art model for contact prediction that outperformed other approximations like mean-field methods or methods based on the Bethe-approximation or sparse inverse covariance. Even though pseudo-likelihood maximation has been shown to be a consistent estimator in the limit of infinite data <span class="citation">[<a href="#ref-Besag1975">69</a>]</span>, it is not clear how well pseudo-likelihood approximation is for real-world datasets.</p>
</div>
<div id="likelihood-gradient" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Likelihood Gradient</h3>
</div>
<div id="contrastive-divergence" class="section level3">
<h3><span class="header-section-number">3.6.3</span> Contrastive Divergence</h3>
<p>CD is about the difference between the original data set and a perturbed data set perturbed data set : The contrasting data set needs to represent A data sample characteristic of the current PARAMETERS –&gt; Gibbs Sampling starting from data Note: as contrasting dataset towards true_parameters, the elements of the gradient converge to the gradient of the max log likelihood – At the limit of the Markov chain, the CD converges to the actual MLE</p>
<!--chapter:end:06-optimizing_full_likelihood.Rmd-->
</div>
</div>
</div>
<div id="a-bayesian-statistical-model-for-residue-residue-contact-prediction" class="section level1">
<h1><span class="header-section-number">4</span> A Bayesian Statistical Model for Residue-Residue Contact Prediction</h1>
<p>All methods so far predict contacts by finding the one solution of parameters <span class="math inline">\(\via\)</span> and <span class="math inline">\(\wijab\)</span> that maximizes a regularized version of the log likelihood of the <a href="#abbrev">MSA</a> and in a second step transforming the <a href="#abbrev">MAP</a> estimates of the couplings <span class="math inline">\(\w^*\)</span> into heuristic contact scores (see Introduction @ref(pseudo-likelihood)). Apart from the heuristic transformation that omits meaningful information comprised in the coupling matrices <span class="math inline">\(\wij\)</span> as discussed in section @ref(interpreting-coupling-matrices), using the <a href="#abbrev">MAP</a> estimate of the parameters instead of the true distribution has the decisive disadvantage of concealing the uncertainty of the estimates.</p>
<p>The next sections present the derivation of a principled Bayesian statistical approach for contact prediction eradicating these deficiencies. The model provides estimates of the probability distributions of the distances <span class="math inline">\(\rij\)</span> between <span class="math inline">\(\Cb\)</span> atoms of all residues pairs <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, given the <a href="#abbrev">MSA</a> <span class="math inline">\(\X\)</span>. The parameters <span class="math inline">\((\v, \w)\)</span> of the <a href="#abbrev">MRF</a> model describing the probability distribution of the sequences in the <a href="#abbrev">MSA</a> are treated as hidden parameters that can be integrated out using an approximation to the posterior distribution of couplings <span class="math inline">\(\w\)</span>. This approach also allows to explictely model the distance-dependence of coupling coeffcients <span class="math inline">\(\wij\)</span> as a mixture of Gaussians with distance-dependent mixture weights and thus can even learn correlations between couplings.</p>
<div id="overview-posterior-distances" class="section level2">
<h2><span class="header-section-number">4.1</span> Computing the Posterior Distribution of Distances <span class="math inline">\(p(\r | \X)\)</span></h2>
<p>The joint probability of distances and <a href="#abbrev">MRF</a> model parameters <span class="math inline">\((\v, \w)\)</span> given the <a href="#abbrev">MSA</a> <span class="math inline">\(\X\)</span> and a set of sequence derived features <span class="math inline">\(\phi\)</span> (described in detail in section @ref(contact-prior)), can be written as a hierarchical Bayesian model of the form:</p>
<span class="math display">\[\begin{align}
        p(\r, \v, \w | \X, \phi) &amp;\propto p(\X | \v, \w) p(\v, \w | \r) \, p(\r | \phi ) \, .
(\#eq:hierarchical-bayesian-model)
\end{align}\]</span>
<p>The ultimate goal is to compute the posterior probability of the distances, <span class="math inline">\(p(\r | \X, \phi)\)</span>, that can be obtained by treating the parameters <span class="math inline">\((\v, \w)\)</span> as hidden variables and marginalizing over these parameters,</p>
<span class="math display">\[\begin{align}
    p(\r | \X , \phi) &amp;\propto  p(\X | \r) p(\r | \phi)\\
    p(\X | \r) &amp;= \int \int p(\X | \v,\w) \, p(\v, \w | \r) \,d\v\,d\w  \; .
(\#eq:integrate-out-vw)
\end{align}\]</span>
<p>The single potentials <span class="math inline">\(\v\)</span> will be fixed at their best estimate <span class="math inline">\(\v^*\)</span> (see section @ref(prior-v)) by using a very tight prior <span class="math inline">\(p(\v) = \Gauss(\v|\v^*,\lambda_v^{-1} \I) \rightarrow \delta(\v-\v*)\)</span> for <span class="math inline">\(\lambda_v \rightarrow \infty\)</span> that acts as a delta function. This allows the replacement of the intergral over <span class="math inline">\(\v\)</span> with the value of the integrand at its mode <span class="math inline">\(\v^*\)</span>.</p>
<p>Computing the integral over <span class="math inline">\(\w\)</span> can be achieved by factorizing the integrand into factors over <span class="math inline">\((i,j)\)</span> and performing each integration over the coupling coefficients <span class="math inline">\(\wij\)</span> for <span class="math inline">\((i,j)\)</span> separately.</p>
<p>For that account, the prior over <span class="math inline">\(\w\)</span> will be modelled as a product over independent contributions over <span class="math inline">\(\wij\)</span> with <span class="math inline">\(\wij\)</span> depending only on the distance <span class="math inline">\(\rij\)</span>, which is described in detail in the next section @ref(coupling-prior). The prior over <a href="#abbrev">MRF</a> model parameters then yields,</p>
<span class="math display">\[\begin{equation}
  p(\v,\w|\r) = \Gauss(\v|\v^*,\lambda_v^{-1} \I) \, \prod_{1\le i&lt;j\le L} p(\wij|\rij) \; .
(\#eq:definition-parameter-prior)
\end{equation}\]</span>
<p>Furthermore, section @ref(laplace-approx) proposes an approximation to the regularised likelihood, <span class="math inline">\(p(\X | \v,\w) \, p(\v, \w)\)</span>, with a Gaussian distribution that facilitates the analytical solution of the integral in eq. @ref(eq:integrate-out-vw) and is covered in section @ref(likelihood-fct-distances).</p>
<p>Finally, the marginals <span class="math inline">\(p(\rij | \X, \phi) = \int p(\r | \X, \phi) d \r_{\backslash ij}\)</span>, where <span class="math inline">\(\r_{\backslash ij}\)</span> is the vector containing all coordinates of <span class="math inline">\(\r\)</span> except <span class="math inline">\(\rij\)</span> will be computed in @ref(posterior-of-rij).</p>
</div>
<div id="coupling-prior" class="section level2">
<h2><span class="header-section-number">4.2</span> Modelling the prior over couplings with dependence on <span class="math inline">\(\rij\)</span></h2>
<p>The prior over couplings <span class="math inline">\(p(\wij|\rij)\)</span> will be modelled as a mixture of <span class="math inline">\(K\!+\!1\)</span> 400-dimensional Gaussians, with means <span class="math inline">\(\muk \in \R^{400}\)</span>, precision matrices <span class="math inline">\(\Lk \in \R^{400\times 400}\)</span>, and distance-dependent, normalised weights <span class="math inline">\(g_k(\rij)\)</span>,</p>
<span class="math display">\[\begin{align}   
      p(\wij | \rij) = \sum_{k=0}^K g_k(\rij) \, \Gauss(\wij | \muk, \Lk^{-1}) \,.
(\#eq:definition-mixture-coupling-prior)
\end{align}\]</span>
<p>The mixture weights <span class="math inline">\(g_k(\rij)\)</span> in eq. @ref(eq:definition-mixture-coupling-prior) are modelled as softmax:</p>
<span class="math display">\[\begin{equation}
    g_k(\rij) = \frac{\exp \gamma_k(\rij)}{\sum_{k&#39;=0}^K \exp \gamma_{k&#39;}(\rij)} 
(\#eq:def-g-k-binary)
\end{equation}\]</span>
<p>The functions <span class="math inline">\(g_k(\rij)\)</span> remain invariant when adding an offset to all <span class="math inline">\(\gamma_k(\rij)\)</span>. This degeneracy can be removed by setting <span class="math inline">\(\gamma_0(\rij)=1\)</span>.</p>
</div>
<div id="laplace-approx" class="section level2">
<h2><span class="header-section-number">4.3</span> Gaussian approximation to the posterior of couplings</h2>
<p>From sampling experiments done by Markus Gruber we know that the regularized pseudo-log-likelihood for realistic examples of protein MSAs obeys the equipartition theorem. The equipartition theorem states that in a harmonic potential (where third and higher order derivatives around the energy minimum vanish) the mean potential energy per degree of freedom (i.e. per eigendirection of the Hessian of the potential) is equal to <span class="math inline">\(k_B T/2\)</span>, which is of course equal to the mean kinetic energy per degree of freedom. Hence we have a strong indication that in realistic examples the pseudo log likelihood is well approximated by a harmonic potential. We assume here that this will also be true for the regularized log likelihood.</p>
<p>The posterior distribution of couplings <span class="math inline">\(\w\)</span> is given by</p>
<span class="math display">\[\begin{equation}
p(\w | \X , \v^*) = p(\X | \v^*, \w) \Gauss (\w | \mathbf{0}, \lambda_w^{-1} \I)
\end{equation}\]</span>
<p>where the single potentials <span class="math inline">\(\v\)</span> are set to the target vector <span class="math inline">\(\v^*\)</span> as discussed in section @ref(overview-posterior-distances).</p>
<p>The posterior distribution can be approximated with a so called “Laplace Approximation”<span class="citation">[<a href="#ref-Murphy2012">56</a>]</span> as follows. By performing a second order Taylor expansion around the mode <span class="math inline">\(\w^*\)</span> of the log posterior it can be written as</p>
<span class="math display">\[\begin{align}
    \log p(\w | \X , \v^*) \overset{!}{\approx} &amp;  \;  \log p(\w^* | \X , \v^*) \\
                &amp; + \nabla_\w \log p(\w | \X , \v^*)|_{\w^*}(\w-\w^*) \\ 
                &amp; - \frac{1}{2} (\w-\w^*)^{\mathrm{T}} \H (\w-\w^*)  \; .
\end{align}\]</span>
<p>where <span class="math inline">\(\H\)</span> signifies the <em>negative</em> Hessian matrix with respect to the components of <span class="math inline">\(\w\)</span>,</p>
<span class="math display">\[\begin{equation}
    (\H)_{klcd, ijab} = - \left. \frac{\partial^2  \log p(\w | \X , \v^{*})}{\partial \w_{klcd} \, \partial \wijab  } \right|_{(\w^{*})} \; .
\end{equation}\]</span>
<p>The mode <span class="math inline">\(\w^*\)</span> will be determined with the <a href="#abbrev">CD</a> approach described in detail in section @ref(optimizing-full-likelihood). Since the gradient vanishes at the mode maximum, <span class="math inline">\(\nabla_\w \log p(\w | \X , \v^*)|_{\w^*} = 0\)</span>, the second order approximation can be written as</p>
<span class="math display">\[\begin{equation}
    \log p(\w | \X , \v^*) {\approx}  \log p(\w^* | \X , \v^*)  - \frac{1}{2} (\w-\w^*)^{\mathrm{T}} \, \H \, (\w-\w^*)  \;.
\end{equation}\]</span>
<p>Hence, the posterior of couplings can be approximated with a Gaussian</p>
<span class="math display">\[\begin{align}
   p(\w | \X , \v^*) &amp;\approx p(\w^* | \X , \v^*) \exp \left( - \frac{1}{2} (\w-\w^*)^{\mathrm{T}} \H  (\w -\w^*) \right) \nonumber \\
              &amp;= p(\w^* | \X , \v^*) \frac{(2 \pi)^\frac{D}{2}} { |\H|^\frac{D}{2}} \times \Gauss (\w | \w^*, \H^{-1} )  \\
              &amp;\propto  \Gauss (\w | \w^*, \H^{-1}) \,,
(\#eq:reg-lik-gauss-approx)
\end{align}\]</span>
<p>with proportionality constant that depends only on the data and with a precision matrix equal to the negative Hessian matrix. The surprisingly easy computation of the Hessian can be found in Methods section @ref(neg-Hessian-computation).</p>
<div id="laplace-approx-improvement" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Iterative improvement of Laplace approximation</h3>
<p>The quality of the Gaussian approximation to the posterior distribution of couplings <span class="math inline">\(p(\w | \X , \v^*)\)</span> depends on two points,</p>
<ol style="list-style-type: decimal">
<li>how well is the posterior distribution of couplings approximated by a Gaussian</li>
<li>how closely does the mode of the posterior distribution of couplings lie near the mode of the integrand in equation @ref(eq:).</li>
</ol>
<p>The second point can be addressed quite effectively in the following way.</p>
<p>(see Murphy page 658 eq. 18.137 and eq 18.138)</p>
<p>Supppose the optimal prior parameters <span class="math inline">\((\tilde{\muk}, \tilde{\Lk})\)</span> have been trained as described in Methods section @ref(training-hyperparameters), using the standard isotropic regularisation prior <span class="math inline">\(\Gauss(\w_{ij} | \mathbf{0}, \lambda_w^{-1} \I)\)</span>. An improved regularisation prior <span class="math inline">\(\Gauss( \wij | \mu(r_{ij}), \mathbf{\Sigma}(r_{ij}))\)</span> can then be selected using the knowledge of the true, optimised prior, by matching the mean and variance of the improved regularisation with those of the true prior from the first optimisation:</p>
<span class="math display">\[\begin{align} 
    \mathbf{\mu}(r_{ij}) &amp;= \operatorname{E}_{p( \wij | \rij, \tilde{\mathbf{\mu}}, \tilde{\Lambda})} \left[  \wij \right]  \\
    &amp;= \int \wij \, p( \wij | \rij, \tilde{\mathbf{\mu}}, \tilde{\Lambda}) d \w  \\
    &amp;= \int \wij \sum_{k=0}^K g_k(\rij) \, \Gauss(\wij | \tilde{\muk}, \tilde{\Lambda}_k^{-1})  d \w \\
    &amp;= \sum_{k=0}^K g_k(\rij) \int \wij  \, \Gauss(\wij | \tilde{\muk}, \tilde{\Lambda}_k^{-1})  d \w  \\
    \mathbf{\mu}(r_{ij}) &amp;= \sum_{k=0}^K g_k(\rij) \, \tilde{\muk}
\end{align}\]</span>
<p>and similarly,</p>
<span class="math display">\[\begin{align}
    \mathbf{\Sigma}(r_{ij}) &amp;= \operatorname{var}_{ p(\wij | \rij, \tilde{\mathbf{\mu}}, \tilde{\Lambda} )}  \left[ \wij \right] \\
    &amp;= \int (\wij - \mathbf{\mu}(r_{ij})) (\wij - \mathbf{\mu}(r_{ij}))^\mathrm{T} \, p( \wij | \rij, \tilde{\mathbf{\mu}}, \tilde{\Lambda}) d \w \\
    &amp;= \sum_{k=0}^K g_k(\rij) \int  (\wij - \mathbf{\mu}(r_{ij})) (\wij - \mathbf{\mu}(r_{ij}))^\mathrm{T} \, \Gauss(\wij | \tilde{\muk}, \tilde{\Lk}^{-1}) d \w \\
    &amp;= \sum_{k=0}^K g_k(\rij) \int  (\wij - \mathbf{\mu}(r_{ij}) + \tilde{\muk}) (\wij - \mathbf{\mu}(r_{ij}) + \tilde{\muk})^\mathrm{T} \, \Gauss(\wij | \mathbf{0} , \tilde{\Lk}^{-1}) d \w \\
    \mathbf{\Sigma}(r_{ij}) &amp;= \sum_{k=0}^K g_k(\rij) \left( \tilde{\Lk}^{-1} + (\mathbf{\mu}(r_{ij}) - \tilde{\muk}) (\mathbf{\mu}(r_{ij}) - \tilde{\muk})^\mathrm{T}\right) \,. 
\end{align}\]</span>
<p>We can now run a second optimisation with better regularisation prior, in which the <span class="math inline">\(\tilde{\mathbf{\mu}}\)</span> and <span class="math inline">\(\tilde{\Lambda}\)</span> are fixed and will not be optimised. Instead we optimise the marginal likelihood as a function of <span class="math inline">\(\muk\)</span> and <span class="math inline">\(\Lk\)</span>. Since the new regularisation prior will be very close to the mode of the integrand in the marginal likelihood, our approximation for the second iteration has improved in comparison to the first iteration. In principle, a third iteration can be done in which our regularisation prior derived from the prior that was found by optimisation in the second iteration. However this is unlikely to further improve the predictions.</p>
</div>
</div>
<div id="likelihood-fct-distances" class="section level2">
<h2><span class="header-section-number">4.4</span> Computing the likelihood function of distances <span class="math inline">\(p(\X | \r)\)</span></h2>
<p>In order to compute the likelihood function of the distances, one needs to solve the integral over <span class="math inline">\((\v, \w)\)</span>,</p>
<span class="math display">\[\begin{equation}
    p(\X | \r) = \int \int p(\X | \v,\w) \, p(\v, \w | \r) \,d\v\,d\w \; .
(\#eq:likelihood-distances)
\end{equation}\]</span>
<p>Inserting the prior over parameters <span class="math inline">\(p(\v, \w | \r)\)</span> from eq. @ref(eq:definition-parameter-prior) into the previous equation and performing the integral over <span class="math inline">\(\v\)</span>, as discussed earlier in section @ref(overview-posterior-distances), yields</p>
<span class="math display">\[\begin{eqnarray}
    p(\X | \r) &amp;=&amp; \int \left( \int  p(\X | \v,\w) \, \Gauss(\v|\v^*,\lambda_v^{-1} \I) \,d\v \right) \, \prod_{1\le i&lt;j\le L} p(\wij|\rij) \, d\w  \\
    p(\X | \r) &amp;=&amp; \int  p(\X | \v^*,\w) \, \prod_{1\le i&lt;j\le L} p(\wij|\rij) \, d\w  
\label{eq:in_over_w_1}
\end{eqnarray}\]</span>
<p>Next, the likelihood will be multiplied with the regularisation prior and the distance-dependent prior will be divided by the regularisation prior again:</p>
<span class="math display">\[\begin{eqnarray}
      p(\X | \r) &amp;=&amp; \int p(\X | \v^*,\w) \, \Gauss(\w|\mathbf{0}, \lambda_w^{-1} \I) \, \prod_{1\le i&lt;j\le L} \frac{p(\wij|\rij)}{\Gauss(\wij|\mathbf{0}, \lambda_w^{-1} \I)} \,d\w \, .
\end{eqnarray}\]</span>
<p>Now the crucial advantage of our likelihood regularisation is borne out: We can chose the strength of the regularisation prior, <span class="math inline">\(\lambda_w\)</span>, such that the mode <span class="math inline">\(\w^*\)</span> of the regularised likelihood is near to the mode of the integrand in the last integral. The regularisation prior <span class="math inline">\(\Gauss(\wij|\mathbf{0}, \lambda_w^{-1} \I)\)</span> is then a simpler, approximate version of the real, distance-dependent prior <span class="math inline">\(\prod_{1\le i&lt;j\le L} p(\wij|\rij)\)</span>. This allows us to approximate the regularised likelihood with a Gaussian distribution (eq. @ref(eq:reg-lik-gauss-approx)), because this approximation will be fairly accurate in the region around its mode, which is near the region around the mode of the integrand and this again is in the region that contributes most to the integral:</p>
<span class="math display">\[\begin{eqnarray}
      p(\X | \r) &amp;\propto&amp; \int \Gauss (\w | \w^*, \H^{-1} ) \, \prod_{1 \le i&lt;j \le L} \frac{p(\wij | \rij)}{\Gauss(\wij|\mathbf{0}, \lambda_w^{-1} \I)} d\w \,.
(\#eq:int-over-w)
\end{eqnarray}\]</span>
<p>The matrix <span class="math inline">\(\H\)</span> has dimensions <span class="math inline">\((L^2 \times 20^2) \times (L^2 \times 20^2)\)</span>. Computing it is obviously infeasible, even if there was a way to compute <span class="math inline">\(p(x_i \eq a, x_j \eq b| \v^*,\w^*)\)</span> efficiently. In Methods section @ref(Hessian-offdiagonal) is shown that in practice, the off-diagonal block matrices with <span class="math inline">\((i,j) \ne (k,l)\)</span> are negligible in comparison to the diagonal block matrices. For the purpose of computing the integral in eq. @ref(eq:int-over-w), it is therefore a good approximation to simply set the off-diagonal block matrices (case 3 in @ref(eq:Hw-offdiag)) to zero!</p>
<p>The first term in the integrand of eq. @ref(eq:int-over-w) now factorizes over <span class="math inline">\((i,j)\)</span>,</p>
<span class="math display">\[\begin{equation}
  \Gauss (\w | \w^{*}, \H^{-1}) \approx \prod_{1 \le i &lt; j \le L} \Gauss (\wij | \wij^{*}, \H_{ij}^{-1}) ,
\end{equation}\]</span>
<p>with the diagonal block matrices are <span class="math inline">\((\H_{ij})_{ab,cd} := (\H)_{ijab,ijcd}\)</span>.</p>
<p>Now the product over all residue indices can be moved in front of the integral and each integral can be performed over <span class="math inline">\(\wij\)</span> separately,</p>
<span class="math display">\[\begin{eqnarray}
  p(\X | \r) &amp;\propto&amp; \int \prod_{1 \le i &lt; j \le L} \Gauss (\wij | \wij^{*}, \H_{ij}^{-1}) \prod_{1 \le i&lt;j \le L} \frac{p(\wij | \rij)}{\Gauss(\wij|\mathbf{0}, \lambda_w^{-1} \I)} d\w  \\
  p(\X | \r) &amp;\propto&amp; \int \prod_{1\le i&lt;j\le L} \left(  \Gauss (\wij | \wij^*, \H_{ij}^{-1}) \, \frac{p(\wij | \rij)}{\Gauss(\wij | \mathbf{0}, \lambda_w^{-1} \I)} \right) d\w \\
  p(\X | \r) &amp;\propto&amp; \prod_{1\le i&lt;j\le L}  \int \Gauss (\wij | \wij^*, \H_{ij}^{-1}) \frac{p(\wij | \rij)}{\Gauss (\wij | \mathbf{0}, \lambda_w^{-1} \I)} d \wij 
(\#eq:int-over-w-2)
\end{eqnarray}\]</span>
<p>Inserting the distance-dependent coupling prior defined in eq. @ref(eq:definition-mixture-coupling-prior) yields</p>
<span class="math display">\[\begin{eqnarray}
   p(\X | \r) &amp;\propto&amp; \prod_{1\le i&lt;j\le L} \int \Gauss (\wij | \wij^*, \H_{ij}^{-1}) \frac{\sum_{k=0}^K g_{k}(\rij) \Gauss(\wij | \muk, \Lk^{-1})}{\Gauss (\wij | \mathbf{0}, \lambda_w^{-1} \I)} d \wij \\
   p(\X | \r) &amp;\propto&amp; \prod_{1\le i&lt;j\le L} \sum_{k=0}^K g_{k}(\rij) \int \frac{\Gauss (\wij | \wij^*, \H_{ij}^{-1})}{\Gauss (\wij | \mathbf{0}, \lambda_w^{-1} \I)} \Gauss(\wij | \muk, \Lk^{-1}) d\wij \; .
(\#eq:int-over-w-3)
\end{eqnarray}\]</span>
The integral can be carried out using the following formula:
<span class="math display">\[\begin{equation}
    \int d\seq \, \frac{ \Gauss( \seq | \mathbf{\mu}_1, \mathbf{\Lambda}_1^{-1}) }{\Gauss(\seq|\mathbf{0},\mathbf{\Lambda}3^{-1})} \, \Gauss(\seq|\mathbf{\mu}_2,\mathbf{\Lambda}_2^{-1}) = \\
    \frac{\Gauss(\mathbf{0}| \mathbf{\mu}_1, \mathbf{\Lambda}_{1}^{-1}) \Gauss(\mathbf{0}| \mathbf{\mu}_2, \mathbf{\Lambda}_{2}^{-1})}{\Gauss(\mathbf{0}|\mathbf{0}, \mathbf{\Lambda}_{3}^{-1}) \Gauss(\mathbf{0}| \mathbf{\mu}_{12}, \mathbf{\Lambda}_{123}^{-1})} 
\end{equation}\]</span>
with
<span class="math display">\[\begin{eqnarray}
    \mathbf{\Lambda}_{123} &amp;:=&amp; \mathbf{\Lambda}_1 - \mathbf{\Lambda}_3 + \mathbf{\Lambda}_2 \\
    \mathbf{\mu}_{12}  &amp;:=&amp; \mathbf{\Lambda}_{123}^{-1}(\mathbf{\Lambda}_1 \mathbf{\mu}_1 + \mathbf{\Lambda}_2 \mathbf{\mu}_2).
\end{eqnarray}\]</span>
We define
<span class="math display">\[\begin{align}
    \Lijk   &amp;:= \H_{ij} - \lambda_w \I + \Lk \\ 
    \muijk  &amp;:= \Lijk^{-1}(\H_{ij} \wij^* + \Lk \muk) \,.
(\#eq:def-Jkij)
\end{align}\]</span>
<p>and obtain</p>
<span class="math display">\[\begin{align}
p(\X | \r) \propto \prod_{1 \le i &lt; j \le L}  \sum_{k=0}^K g_{k}(\rij) \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})}  \,.
(\#eq:pXr-final)
\end{align}\]</span>
<p><span class="math inline">\(\Gauss( \mathbf{0} | \mathbf{0}, \lambda_w^{-1} \I)\)</span> and <span class="math inline">\(\Gauss( \mathbf{0} | \wij^*, \H_{ij}^{-1})\)</span> are constants that depend only on <span class="math inline">\(\X\)</span> and <span class="math inline">\(\lambda_w\)</span> and can be omitted.</p>
</div>
<div id="posterior-of-rij" class="section level2">
<h2><span class="header-section-number">4.5</span> The posterior probability distribution for <span class="math inline">\(\rij\)</span></h2>
<p>The posterior distribution for <span class="math inline">\(r_{ij}\)</span> can be computed by marginalizing over all other distances, which are summarized in the vector <span class="math inline">\(\r_{\backslash ij}\)</span>:</p>
<span class="math display">\[\begin{eqnarray}
    p(\rij | \X, \phi) &amp;=&amp; \int d \r_{\backslash ij} \, p(\r |\X, \mathbf{\phi})\\
                &amp;\propto &amp; \int d \r_{\backslash ij} \, p(\X|\r) \, p(\r | \phi) \\
                &amp;\propto &amp; \int d \r_{\backslash ij} \prod_{i&#39;&lt;j&#39;} \sum_{k=0}^K g_{k}(r_{i&#39;j&#39;}) \, \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})}
 \, \prod_{i&#39;&lt;j&#39;} p(r_{i&#39;j&#39;} |\phi_{i&#39;j&#39;})  \,,
 \end{eqnarray}\]</span>
<p>and, by pulling out of the integral over <span class="math inline">\(\r_{\backslash ij}\)</span> the term depending only on <span class="math inline">\(\rij\)</span>,</p>
<span class="math display">\[\begin{eqnarray}
    p(\rij | \X, \phi) &amp; \propto &amp; 
            p(\rij |\phi_{ij}) \, \sum_{k=0}^K g_{k}(\rij) \, \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})} \\
            &amp; \times  &amp; \prod_{i&#39;&lt;j&#39;, (i&#39;,j&#39;) \ne (i,j)} \int d r_{i&#39;j&#39;} \, p(r_{i&#39;j&#39;} |\phi_{i&#39;j&#39;}) \, \sum_{k=0}^K g_{k}(r_{i&#39;j&#39;}) \, \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})}
\end{eqnarray}\]</span>
<p>Since the second factor involving the integrals over <span class="math inline">\(r_{i&#39;j&#39;}\)</span> is a constant with respect to <span class="math inline">\(\rij\)</span>, we find</p>
<span class="math display">\[\begin{equation}
    p(\rij | \X, \phi) \propto  
    p(\rij |\phi_{ij}) \,  \sum_{k=0}^K g_{k}(\rij) \, \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})}  \, .
(\#eq:posterior-marginal-rij)
\end{equation}\]</span>
<!--chapter:end:07-posterior-distribution.Rmd-->
</div>
</div>
<div id="contact-prior" class="section level1">
<h1><span class="header-section-number">5</span> Contact Prior</h1>
<p>Up to now the only source of information to predict contacts was the <a href="#abbrev">MSA</a> <span class="math inline">\(\X\)</span>. There are other sources of information that can be exploited. As shown in subsection , much information about the distance <span class="math inline">\(\rij\)</span> is typically contained in features of 1D properties at positions <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> predicted from local sequence profiles, such as secondary structure, solvent accessibility, contact number or context states, and in features of predicted 2D properties such as the contact prediction scores for <span class="math inline">\((i,j)\)</span> from a profile-based method.</p>
<p>This information can be introduced directly as prior <span class="math inline">\(p(\r |\phi)\)</span> for the correlated mutations analysis to improve the overall prediction accuracy in terms of posterior probabilities.</p>
<!--chapter:end:08-contact-prior.Rmd-->
</div>
<div id="methods" class="section level1">
<h1><span class="header-section-number">6</span> Methods</h1>
<p>all you need to know</p>
<div id="dataset" class="section level2">
<h2><span class="header-section-number">6.1</span> Dataset</h2>
<p>A protein dataset has been constructed from the CATH (v4.1) <span class="citation">[<a href="#ref-Sillitoe2015">90</a>]</span> database for classification of protein domains. All CATH domains from classes 1(mainly <span class="math inline">\(\alpha\)</span>), 2(mainly <span class="math inline">\(\beta\)</span>), 3(<span class="math inline">\(\alpha+\beta\)</span>) have been selected and filtered for internal redundancy at the sequence level using the <code>pdbfilter</code> script from the HH-suite<span class="citation">[<a href="#ref-Remmert2012">76</a>]</span> with an E-value cutoff=0.1. The dataset has been split into ten subsets aiming at the best possible balance between CATH classes 1,2,3 in the subsets. All domains from a given CATH topology (=fold) go into the same subsets, so that any two subsets are non-redundant at the fold level. Some overrepresented folds (e.g. Rossman Fold) have been subsampled ensuring that in every subset each class contains at max 50% domains of the same fold. Consequently, a fold is not allowed to dominate a subset or even a class in a subset. In total there are 6741 domains in the dataset.</p>
<p>Multiple sequence alignments were built from the CATH domain sequences (<a href="http://www.cathdb.info/version/current/domain/3cdjA03/sequence">COMBS</a>) using HHblits <span class="citation">[<a href="#ref-Remmert2012">76</a>]</span> with parameters to maximize the detection of homologous sequences:</p>
<p><code>hhblits -maxfilt 100000 -realign_max 100000 -B 100000 -Z 100000 -n 5 -e 0.1 -all</code> <code>hhfilter -id 90 -neff 15 -qsc -30</code></p>
<p>The COMBS sequences are derived from the SEQRES records of the PDB file and sometimes contain extra residues that are not resolved in the structure. Therefore, residues in PDB files have been renumbered to match the COMBS sequences. The process of renumbering residues in PDB files yielded ambigious solutions for 293 proteins, that were removed from the dataset. Another filtering step was applied to remove 80 proteins that do not hold the following properties:</p>
<ul>
<li>more than 10 sequences in the multiple sequence alignment (<span class="math inline">\(N&gt;10\)</span>)</li>
<li>protein length between 30 and 600 residues (<span class="math inline">\(30 \leq L \leq 600\)</span>)</li>
<li>less than 80% gaps in the multiple sequence alignment (percent gaps &lt; 0.8)</li>
<li>at least one residue-pair in contact at <span class="math inline">\(C_\beta &lt; 8\AA\)</span> and minimum sequence separation of 6 positions</li>
</ul>
<p>The final dataset is comprised of <strong>6368</strong> proteins with almost evenly distributed CATH classes over the ten subsets (Figure @ref(fig:dataset-cath-topologies)).</p>
<p>(ref:caption-dataset-cath-topologies) Distribution of CATH classes (1=mainly <span class="math inline">\(\alpha\)</span>, 2=mainly <span class="math inline">\(\beta\)</span>, 3=<span class="math inline">\(\alpha-\beta\)</span>) in the dataset and the ten subsets.</p>
<div class="figure">
<iframe src="img/dataset_statistics/cath_topologies_stacked_reative_notitle.html" width="100%" height="400px">
</iframe>
<p class="caption">
(#fig:dataset-cath-topologies)(ref:caption-dataset-cath-topologies)
</p>
</div>
</div>
<div id="optimizing-pseudo-likelihood" class="section level2">
<h2><span class="header-section-number">6.2</span> Optimizing Pseudo-Likelihood</h2>
<p>Dr Stefan Seemayer has reimplementated the open-source software CCMpred <span class="citation">[<a href="#ref-Seemayer2014">63</a>]</span> in Python. Based on a fork of his private github repository I continued development and extended the software, which is now called CCMpredPy. It will soon be available at <a href="https://github.com/soedinglab/CCMpredPy" class="uri">https://github.com/soedinglab/CCMpredPy</a>. All computations in this thesis are performed with CCMpredPy unless stated otherwise.</p>
<div id="pseudo-likelihood-objective-function-and-its-gradients" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Pseudo-Likelihood Objective Function and its Gradients</h3>
<p>CCMpred optimizes the regularized negative pseudo-log-likelihood using conjugate gradients optimizer.</p>
<p>The negative pseudo-log-likelihood, abbreviated <span class="math inline">\(\mathcal{npll}\)</span>, is defined as:</p>
<span class="math display">\[\begin{equation}
  \mathcal{npll}(\mathbf{X} | \v,\w) =   - \sum_{n=1}^N \sum_{i=1}^L  \left(  v_i(x_i^{(n)}) + \sum_{\substack{j=1 \\ j \neq i}}^L w_{ij}(x_i^{(n)}, x_j^{(n)})  - \log Z_i^{(n)} \right)
\end{equation}\]</span>
<p>The normalization term <span class="math inline">\(Z_i\)</span> sums over all assignments to one position <span class="math inline">\(i\)</span> in sequence:</p>
<span class="math display">\[\begin{equation}
  Z_i^{(n)} = \sum_{a=1}^{20} \exp \left( v_i(a) + \sum_{\substack{j=1 \\ j \neq i}}^L w_{ij}(a, x_j^{(n)}) \right)
\end{equation}\]</span>
</div>
<div id="diff-ccmpred-ccmpredpy" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Differences between CCMpred and CCMpredpy</h3>
<p>CCMpredPy differs from CCMpred <span class="citation">[<a href="#ref-Seemayer2014">63</a>]</span> which is available at <a href="https://github.com/soedinglab/CCMpred" class="uri">https://github.com/soedinglab/CCMpred</a> in several details:</p>
<ul>
<li>Initialization of potentials <span class="math inline">\(\v\)</span> and <span class="math inline">\(\w\)</span>
<ul>
<li>CCMpred initializes single potentials <span class="math inline">\(\v_i(a) = \log f_i(a) - \log f_i(a= &quot;-&quot;)\)</span> with <span class="math inline">\(f_i(a)\)</span> being the frequency of amino acid a at position i and <span class="math inline">\(a=&quot;-&quot;\)</span> representing a gap. A single pseudo-count has been added before computing the frequencies. Pair potentials <span class="math inline">\(\w\)</span> are intialized at 0.</li>
<li>CCMpredPy initializes single potentials <span class="math inline">\(\v\)</span> with the <a href="#abbrev">ML</a> estimate of single potentials (see section @ref(regularization)) using amino acid frequencies computed as described in section @ref(amino-acid-frequencies). Pair potentials <span class="math inline">\(\w\)</span> are initialized at 0.</li>
</ul></li>
<li>Regularization
<ul>
<li>CCMpred uses a Gaussian regularization prior centered at zero for both single and pair potentials. The regularization coefficient for single potentials <span class="math inline">\(\lambda_v = 0.01\)</span> and for pair potentials <span class="math inline">\(\lambda_w = 0.2 * (L-1)\)</span> with <span class="math inline">\(L\)</span> being protein length.</li>
<li>CCMpredPy uses a Gaussian regularization prior centered at zero for the pair potentials. For the single potentials the Gaussian regularization prior is centered at the <a href="#abbrev">ML</a> estimate of single potentials (see section @ref(regularization)) using amino acid frequencies computed as described in section @ref(amino-acid-frequencies). The regularization coefficient for single potentials <span class="math inline">\(\lambda_v = 10\)</span> and for pair potentials <span class="math inline">\(\lambda_w = 0.2 * (L-1)\)</span> with <span class="math inline">\(L\)</span> being protein length.</li>
</ul></li>
</ul>
<p>Default settings for CCMpredPy have been chosen to best reproduce CCMpred results. A benchmark over a subset of approximately 3000 proteins confirms that performance measured as <a href="#abbrev">PPV</a> for both methods is almost identical (see Figure @ref(fig:cmmpredvanilla-vs-ccmpredpy)).</p>
<p>(ref:caption-cmmpredvanilla-vs-ccmpredpy) Benchmark for CCMpred and CCMpredPy on a dataset of 3124 proteins. ccmpred-vanilla+apc: CCMpred <span class="citation">[<a href="#ref-Seemayer2014">63</a>]</span> with <a href="#abbrev">APC</a>. ccmpred-pll-centerv+apc: CCMpredPy with <a href="#abbrev">APC</a>. Specific flags that have been used to run both methods are described in detail in the text (see section @ref(diff-ccmpred-ccmpredpy)).</p>
<div class="figure">
<iframe src="img/ccmpredvanilla_vs_ccmpredpy_precision_vs_rank.html" width="100%" height="400px">
</iframe>
<p class="caption">
(#fig:cmmpredvanilla-vs-ccmpredpy)(ref:caption-cmmpredvanilla-vs-ccmpredpy)
</p>
</div>
<p>The benchmark in Figure @ref(fig:cmmpredvanilla-vs-ccmpredpy) as well as all contacts predicted with CCMpred and CCMPredPy (using pseudo-likelihood) in my thesis have been computed using the following flags:</p>
<p>Flags used with CCMpredPy (using pseudo-likelihood objective function):</p>
<pre><code>--maxit 250                       # Compute a maximum of MAXIT operations
--center-v                        # Use a Gaussian prior for single potentials centered at ML estimate v*
--reg-l2-lambda-single 10         # regularization coefficient for single potentials
--reg-l2-lambda-pair-factor 0.2   # regularization coefficient for pairwise potentials computed as reg-l2-lambda-pair-factor * (L-1)
--pc-uniform                      # use uniform pseudocounts (1/21 for 20 amino acids + 1 gap state) 
--pc-count 1                      # defining pseudo count admixture coefficient rho = pc-count/( pc-count+ Neff)
--epsilon 1e-5                    # convergence criterion for minimum decrease in the last K iterations
--ofn-pll                         # using pseudo-likelihood as objective function
--alg-cg                          # using conjugate gradient to optimize objective function</code></pre>
<p>Flags used with CCMpred:</p>
<pre><code>-n 250    # NUMITER:  Compute a maximum of NUMITER operations
-l 0.2    # LFACTOR:  Set pairwise regularization coefficients to LFACTOR * (L-1) 
-w 0.8    # IDTHRES:  Set sequence reweighting identity threshold to IDTHRES
-e 1e-5   # EPSILON:  Set convergence criterion for minimum decrease in the last K iterations to EPSILON</code></pre>
</div>
<div id="seq-reweighting" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Sequence Reweighting</h3>
<p>As discussed in section @ref(challenges), sequences in a <a href="#abbrev">MSA</a> do not represent independent draws from a probabilistic model. To reduce the effects of overrepresented sequences, typically a simple weighting strategy is applied that assigns a weight to each sequence that is the inverse of the number of similar sequences according to an identity threshold <span class="citation">[<a href="#ref-Stein2015a">62</a>]</span>. It has been found that reweighting improves contact prediction performance <span class="citation">[<a href="#ref-Jones2012">45</a>,<a href="#ref-Morcos2011">57</a>,<a href="#ref-Buslje2009">74</a>]</span> significantly but results are robust against the choice of the identity threshold in a range between 0.7 and 0.9 <span class="citation">[<a href="#ref-Morcos2011">57</a>]</span>. We chose an identity threshold of 0.8.</p>
<p>Every sequence <span class="math inline">\(x_n\)</span> of length <span class="math inline">\(L\)</span> in an alignment with <span class="math inline">\(N\)</span> sequences has an associated weight <span class="math inline">\(w_n = 1/m_n\)</span>, where <span class="math inline">\(m_n\)</span> represents the number of similar sequences:</p>
<span class="math display">\[\begin{equation} 
  w_n = \frac{1}{m_n}, m_n = \sum_{m=1}^N I \left( ID(x_n, x_m) \geq 0.8 \right) \\
  ID(x_n, x_m)=\frac{1}{L} \sum_{i=1}^L I(x_n^i = x_m^i)
  (\#eq:seqweight)
\end{equation}\]</span>
<p>The number of effective sequences <span class="math inline">\(\mathbf{\neff}\)</span> of an alignment is then the number of sequence clusters computed as:</p>
<span class="math display">\[\begin{equation} 
  \neff = \sum_{n=1}^N w_n
  (\#eq:neff)
\end{equation}\]</span>

</div>
<div id="amino-acid-frequencies" class="section level3">
<h3><span class="header-section-number">6.2.4</span> Computing Amino Acid Frequencies</h3>
<p>Single and pairwise amino acid frequencies are computed from the alignment by weighting amino acid counts (see section @ref(seq-reweighting)) and adding pseudocounts for numerical stability.</p>
<p>Let <span class="math inline">\(a,b \in \{1,\ldots,20\}\)</span> be amino acids, <span class="math inline">\(q(x_i=a), q(x_i=a, x_j=b)\)</span> and <span class="math inline">\(q_0(x_i=a), q_0(x_i=a,x_j=b)\)</span> be the empirical single and pair frequencies with and without pseudocounts, respectively. We define</p>
<span class="math display">\[\begin{align}
    q(x_i \eq a) :=&amp; (1-\tau) \;  q_0(x_i \eq a) + \tau \tilde{q}(x_i\eq a) \\
    q(x_i \eq a, x_j \eq b) :=&amp; (1-\tau)^2  \; [ q_0(x_i \eq a, x_j \eq b) - q_0(x_i \eq a)  q_0(x_j \eq b) ] + \\
                            &amp; q(x_i \eq a) \; q(x_j \eq b) 
(\#eq:pseudocounts)
\end{align}\]</span>
<p>with <span class="math inline">\(\tilde{q}(x_i \eq a) := f(a)\)</span> being background amino acid frequencies and <span class="math inline">\(\tau \in [0,1]\)</span> is a pseudocount admixture coefficient, which is a function of the diversity of the multiple sequence alignment:</p>
<span class="math display">\[\begin{equation}
    \tau = \frac{N_\mathrm{pc}}{(N_\mathrm{eff} + N_\mathrm{pc})}
(\#eq:tau)
\end{equation}\]</span>
<p>where <span class="math inline">\(N_{pc} &gt; 0\)</span>.</p>
<p>The formula for <span class="math inline">\(q(x_i \eq a, x_j \eq b)\)</span> in the second line in eq @ref(eq:pseudocounts) was chosen such that for <span class="math inline">\(\tau \eq0\)</span> we obtain <span class="math inline">\(q(x_i \eq a, x_j \eq b) = q_0(x_i \eq a, x_j \eq b)\)</span>, and furthermore <span class="math inline">\(q(x_i \eq a, x_j \eq b) = q(x_i \eq a) q(x_j \eq b)\)</span> exactly if <span class="math inline">\(q_0(x_i \eq a, x_j \eq b) = q_0(x_i \eq a) q_0(x_j \eq b)\)</span>.</p>
</div>
<div id="regularization" class="section level3">
<h3><span class="header-section-number">6.2.5</span> Regularization</h3>
<p>As the model is overparameterized, regularization is an alternative solution compared to choosing a gauge. Furthermore it helps preventing overfitting.</p>
<p>L2-regularization which corresponds to using a Gaussian prior, has proven to work better than L1 regularization <span class="citation">[<span class="citeproc-not-found" data-reference-id="cite"><strong>???</strong></span>]</span>.</p>
<span class="math display">\[\begin{equation}
  R(\v, \w) = \mathcal{N}(\v |  \vec{0}, \lambda_v \I^{-1})  + \mathcal{N}(\w | \vec{0}, \lambda_w \I^{-1}) 
\end{equation}\]</span>
<span class="math display">\[\begin{align}
 \mathcal{N}(\v |  \vec{0}, \lambda_v \I^{-1})   &amp;= \lambda_v ||\v||_2^2 \\
                                                &amp;= \frac{\lambda_v}{2} \sum_{i=1}^L \sum_{a=1}^{20}  \via^2
\end{align}\]</span>
<span class="math display">\[\begin{align}
\mathcal{N}(\w | \vec{0}, \lambda_w \I^{-1}) &amp;= \lambda_w ||\w||_2^2 \\
                                            &amp;= \frac{\lambda_w}{2} \sum_{i=1}^L \sum_{\substack{j=1 \\ i \neq j}}^L  \sum_{a,b=1}^{20} \wijab^2
\end{align}\]</span>
<p>However, it makes sense to use a Gaussian prior for single emission potentials that is centered at the <a href="#abbrev">ML</a> estimate of the single potentials. Consider, …..</p>
<span class="math display">\[\begin{align}
 \mathcal{N}(\v |  \v^{*}, \lambda_v \I^{-1}) &amp;= \lambda_v ||\v - \v^{*}||_2^2 \\
                                               &amp;= \frac{\lambda_v}{2} \sum_{i=1}^L \sum_{a=1}^{20}  (\via - \via^{*})^2   
\end{align}\]</span>
<span class="math display">\[\begin{equation}
  \via^* = \log q(x_i=a) - \frac{1}{20} \sum_{a&#39;=1}^{20} \log q(x_i=a&#39;)
\end{equation}\]</span>
</div>
</div>
<div id="analysis-of-coupling-matrices" class="section level2">
<h2><span class="header-section-number">6.3</span> Analysis of Coupling Matrices</h2>
<div id="method-coupling-correlation" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Correlation of Couplings with Contact Class</h3>
<p>Approximately 100000 residue pairs have been filtered for contacts and non-contacts respectively according to the following criteria:</p>
<ul>
<li>consider only residue pairs separated by at least 10 positions in sequence</li>
<li>minimal diversity (<span class="math inline">\(=\frac{\sqrt{N}}{L}\)</span>) of alignment = 0.3</li>
<li>minimal number of non-gapped sequences = 1000</li>
<li><span class="math inline">\(\Cb\)</span> distance threshold for contact: <span class="math inline">\(&lt;8\AA\)</span></li>
<li><span class="math inline">\(\Cb\)</span> distance threshold for noncontact: <span class="math inline">\(&gt;25\AA\)</span></li>
</ul>
</div>
<div id="method-coupling-profile" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Coupling Distribution Plots</h3>
<p>For one-dimensional coupling distribution plots the residue pairs and respective pseudo-log-likelihood coupling values <span class="math inline">\(\wijab\)</span> have been selected as follows:</p>
<ul>
<li>consider only residue pairs separated by at least 10 positions in sequence</li>
<li>discard residues that have more than 30% gaps in the alignment</li>
<li>discard residue pairs that have insufficient evidence in the alignment: <span class="math inline">\(N_{ij} \cdot q_i(a) \cdot q_j(b) &lt; 100\)</span> with:
<ul>
<li><span class="math inline">\(N_{ij}\)</span> is the number of sequences with neither a gap at position i nor at position j</li>
<li><span class="math inline">\(q_i(a)\)</span> and <span class="math inline">\(q_j(b)\)</span> are the frequencies of amino acids a and b at positions i and j (computed as described in section @ref(amino-acid-frequencies))</li>
</ul></li>
</ul>
<p>The same criteria have been applied for selecting couplings for the two-dimensional distribution plots with the difference that evidence for a single coupling term has to be <span class="math inline">\(N_{ij} \cdot q_i(a) \cdot q_j(b) &lt; 80.\)</span></p>
</div>
<div id="bayesian-model-for-residue-resdiue-contact-prediction" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Bayesian Model for Residue-Resdiue Contact Prediction</h3>
</div>
</div>
<div id="Hessian-offdiagonal" class="section level2">
<h2><span class="header-section-number">6.4</span> Off-diagonal elements in <span class="math inline">\(\H\)</span></h2>
</div>
<div id="neg-Hessian-computation" class="section level2">
<h2><span class="header-section-number">6.5</span> Efficiently Computing the negative Hessian of the regularized log-likelihood</h2>
<p>Surprisingly, the elements of the Hessian at the mode <span class="math inline">\(\w^*\)</span> are easy to compute. Let <span class="math inline">\(i,j,k,l \in \{1,\ldots,L\}\)</span> be columns in the <a href="#abbrev">MSA</a> and let <span class="math inline">\(a, b, c, d \in \{1,\ldots,20\}\)</span> represent amino acids.</p>
<p>The partial derivative <span class="math inline">\(\partial / \partial \w_{klcd}\)</span> of the second term in the gradient of the couplings in eq. @ref(eq:gradient-LLreg-pair) is</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab } 
    &amp;=&amp;  - \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} \frac{\partial \left( \frac{\exp \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}^L w_{ij}(y_i,y_j) \right) }{Z_n(\v,\w)} \right)}{\partial \wklcd}   I(y_i \eq a, y_j \eq b) \\
    &amp;&amp;- \lambda_w \delta_{ijab,klcd} \,,
\end{eqnarray}\]</span>
<p>where <span class="math inline">\(\delta_{ijab,klcd} = I(ijab=klcd)\)</span> is the Kronecker delta. Applying the product rule, we find</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab  } 
    &amp;=&amp;  - \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} \frac{\exp \left(\sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}^L w_{ij}(y_i,y_j)  \right)}{Z_n(\v,\w)}  I(y_i \eq a, y_j \eq b) \\
    &amp; \times &amp; \left[ \frac{\partial}{\partial \wklcd} \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}  w_{ij}(y_i,y_j)  \right) 
                  - \frac{1}{Z_n(\v,\w)} \frac{\partial  Z_n(\v,\w) }{\partial\wklcd} \right] \\
    &amp;-&amp; \lambda_w \delta_{ijab,klcd} \\

    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab  } 
    &amp;=&amp;  - \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} \frac{\exp \left(\sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L}^L w_{ij}(y_i,y_j)  \right)}{Z_n(\v,\w)}  I(y_i \eq a, y_j \eq b) \\
    &amp; \times &amp; \left[ I(y_k \eq c, y_l \eq d) - \frac{\partial}{\partial \wklcd} \log Z_n(\v,\w) \right] \\
    &amp;-&amp; \lambda_w \delta_{ijab,klcd} \,.
\end{eqnarray}\]</span>
<p>We simplify this expression using</p>
<span class="math display">\[\begin{equation}
    p(\mathbf{y} | \v,\w) = \frac{\exp \left( \sum_{i=1}^L v_i(y_i) + \sum_{1 \le i &lt; j \le L} w_{ij}(y_i,y_j) \right)}{Z_n(\v,\w)}  ,
\end{equation}\]</span>
<p>yielding</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab} 
    &amp;=&amp;  -  \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \Sn} p(\mathbf{y} | \v,\w) \, I(y_i \eq a, y_j \eq b, y_k \eq c, y_l \eq d)  \\
    &amp;+&amp; \sum_{n=1}^{N} \, \sum_{\mathbf{y} \in \mathcal{S}_n} p(\mathbf{y} | \v,\w) \, I(y_i \eq a, y_j \eq b ) \sum_{\mathbf{y} \in \Sn} p(\mathbf{y} | \v,\w)  I(y_k \eq c, y_l \eq d ) \\
    &amp;-&amp; \lambda_w \delta_{ijab,klcd} \,.
\end{eqnarray}\]</span>
<p>If <span class="math inline">\(\X\)</span> does not contain too many gaps, this expression can be approximated by</p>
<span class="math display">\[\begin{eqnarray}
    \frac{\partial^2 \LLreg(\v^*,\w)}{\partial \wklcd \, \partial \wijab  } 
    &amp;=&amp; - N_{ijkl} \: p(x_i \eq a, x_j \eq b, x_k \eq c, x_l \eq d | \v,\w)  \nonumber \\
    &amp;&amp; +  N_{ijkl} \: p(x_i \eq a, x_j \eq b | \v,\w) \, p(x_k \eq c, x_l \eq d | \v,\w) - \lambda_w \delta_{ijab,klcd} \,,
\end{eqnarray}\]</span>
<p>where <span class="math inline">\(N_{ijkl}\)</span> is the number of sequences that have a residue in <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span>, <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span>.</p>
<p>Looking at three cases separately:</p>
<ul>
<li>case 1: <span class="math inline">\((k,l) = (i,j)\)</span> and <span class="math inline">\((c,d) = (a,b)\)</span></li>
<li>case 2: <span class="math inline">\((k,l) = (i,j)\)</span> and <span class="math inline">\((c,d) \ne (a,b)\)</span></li>
<li>case 3: <span class="math inline">\((k,l) \ne (i,j)\)</span> and <span class="math inline">\((c,d) \ne (a,b)\)</span>,</li>
</ul>
<p>the elements of <span class="math inline">\(\H\)</span>, which are the negative second partial derivatives of <span class="math inline">\(\LLreg(\v^*,\w)\)</span> with respect to the components of <span class="math inline">\(\w\)</span>, are</p>
<span class="math display">\[\begin{eqnarray}
    \mathrm{case~1:} (\H)_{ijab, ijab}  
    &amp;=&amp;  N_{ij} \, p(x_i \eq a, x_j \eq b| \v^*,\w^*) \, ( 1 - p(x_i \eq a, x_j \eq b| \v^*,\w^*) \,) \\
    &amp;&amp;   + \lambda_w \\
    \mathrm{case~2:} (\H)_{ijcd, ijab}  
    &amp;=&amp;  - N_{ij} \, p(x_i \eq a, x_j \eq b |\v^*,\w^*) \, p(x_i \eq c, x_j \eq d |\v^*,\w^*) \\
    \mathrm{case~3:} (\H)_{klcd, ijab}  
    &amp;=&amp;   N_{ijkl} \, p(x_i \eq a, x_j \eq b, x_k \eq c, x_l \eq d  | \v^*,\w^*) \nonumber \\
    &amp;&amp;    - N_{ijkl} \, p(x_i \eq a, x_j \eq b | \v^*,\w^*)\, p(x_k \eq c, x_l \eq d | \v^*,\w^*) \,.
(\#eq:Hw-offdiag)
\end{eqnarray}\]</span>
<p>We know from eq. @ref(eq:gradient-LLreg-approx) that at the mode <span class="math inline">\(\w^*\)</span> the model probabilities match the empirical frequencies up to a small regularization term,</p>
<span class="math display">\[\begin{equation}
    p(x_i \eq a, x_j \eq b | \v^*,\w^*) = q(x_i \eq a, x_j \eq b) - \frac{\lambda_w}{N_{ij}}  \wijab^* \,,
\end{equation}\]</span>
<p>and therefore the negative Hessian elements in cases 1 and 2 can be expressed as</p>
<!-- The first term ($N_{ij} \left(\,q(x_i\!=\!a, x_j\!=\!b)  - \frac{\lambda_w}{N_{ij}} \wijab^* \right)$) actually is accurate and only the second $(\dots)$ iss approximated according to the gap approximation in eq. \@ref(eq:gradient-LLreg-approx) -->
<span class="math display">\[\begin{align}
   (\H)_{ijab, ijab} =&amp; N_{ij} \left( q(x_i \eq a, x_j \eq b)  - \frac{\lambda_w}{N_{ij}} \wijab^* \right) \left( 1 - q(x_i \eq a, x_j \eq b) +\frac{\lambda_w}{N_{ij}} \wijab^* \right) \\
   &amp; + \lambda_w \\
   (\H)_{ijcd, ijab} =&amp; -N_{ij} \left(\,q(x_i \eq a, x_j \eq b)  - \frac{\lambda_w}{N_{ij}} \wijab^* \right) \left( q(x_i \eq c, x_j \eq d) -\frac{\lambda_w}{N_{ij}} \wijcd^* \right) .
(\#eq:Hw-diag)
\end{align}\]</span>
<p>In order to write the previous eq. @ref(eq:Hw-diag) in matrix form, the <em>regularised</em> empirical frequencies <span class="math inline">\(\qij\)</span> will be defined as</p>
<span class="math display">\[\begin{equation}
    (\qij)_{ab} = q&#39;_{ijab} := q(x_i \eq a, x_j \eq b) - \lambda_w  \wijab^* / N_{ij} \,,
\end{equation}\]</span>
<p>and the <span class="math inline">\(400 \times 400\)</span> diagonal matrix <span class="math inline">\(\Qij\)</span> will be defined as</p>
<span class="math display">\[\begin{equation}
    \Qij := \text{diag}(\qij) \; .
\end{equation}\]</span>
<p>Now eq. @ref(eq:Hw-diag) can be written in matrix form</p>
<span class="math display">\[\begin{equation}
     \H_{ij} = N_{ij} \left( \Qij -  \qij \qij^{\mathrm{T}} \right)  + \lambda_w \I \; .
(\#eq:mat-Hij)
\end{equation}\]</span>
</div>
<div id="inv-lambda-ij-k" class="section level2">
<h2><span class="header-section-number">6.6</span> Efficiently Computing the Inverse of Matrix <span class="math inline">\(\Lijk\)</span></h2>
<p>It is possible to efficiently invert the matrix <span class="math inline">\(\Lijk = \H_{ij} - \lambda_w \I + \Lambda_k\)</span>, that is introduced in @ref(coupling-prior) where <span class="math inline">\(\H_{ij}\)</span> is the <span class="math inline">\(400 \times 400\)</span> diagonal block submatrix <span class="math inline">\((\H_{ij})_{ab,cd} := (\H)_{ijab,ijcd}\)</span> and <span class="math inline">\(\Lambda_k\)</span> is an invertible diagonal precision matrix that is introduced in section @ref(modeling-dep-of-wij).</p>
<p>Equation @ref(eq:mat-Hij) can be used to write <span class="math inline">\(\Lijk\)</span> in matrix form as</p>
<span class="math display">\[\begin{equation}
     \Lijk = \H_{ij} - \lambda_w \I + \Lk = N_{ij} \Qij- N_{ij} \qij \qij^{\mathrm{T}} + \Lk \,.
(\#eq:mat-Lijk)
\end{equation}\]</span>
<p>Owing to eqs. @ref(eq:normalized-emp-freq) and @ref(eq:zero-sum-wij), <span class="math inline">\(\sum_{a,b=1}^{20} q&#39;_{ijab} = 1\)</span>. The previous equation @ref(eq:mat-Lijk) facilitates the calculation of the inverse of this matrix using the <em>Woodbury identity</em> for matrices</p>
<span class="math display">\[\begin{equation}
    (\mathbf{A} + \mathbf{B} \mathbf{D}^{-1} \mathbf{C})^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{B} (\mathbf{D} + \mathbf{C} \mathbf{A}^{-1} \mathbf{B}) ^{-1} \mathbf{C} \mathbf{A}^{-1} \;. 
\end{equation}\]</span>
<p>by setting</p>
<span class="math display">\[\begin{align}
  \mathbf{A} &amp;= N_{ij} \Qij + \Lk \\
  \mathbf{B} &amp;= \qij \\
  \mathbf{C} &amp;= \qij^\mathrm{T} \\
  \mathbf{D} &amp;=- N_{ij}^{-1} \\
\end{align}\]</span>
<span class="math display">\[\begin{align}
      \left( \H_{ij} - \lambda_w \I + \Lk \right)^{-1} &amp; = \mathbf{A}^{-1} - \mathbf{A}^{-1} \qij  \left( -N_{ij}^{-1}  + \qij^\mathrm{T} \mathbf{A}^{-1} \qij \right)^{-1}  \qij^\mathrm{T} \mathbf{A}^{-1} \\
     &amp; = \mathbf{A}^{-1} + \frac{ (\mathbf{A}^{-1} \qij) (\mathbf{A}^{-1} \qij)^{\mathrm{T}} }{ N_{ij}^{-1} - \qij^\mathrm{T} \mathbf{A}^{-1} \qij} \,.
(\#eq:fast-inverse-mat-Lijk)
\end{align}\]</span>
<p>Note that <span class="math inline">\(\mathbf{A}\)</span> is diagonal as <span class="math inline">\(\Qij\)</span> and <span class="math inline">\(\Lk\)</span> are diagonal matrices: <span class="math inline">\(\mathbf{A} = \text{diag}(N_{ij} q&#39;_{ijab} + (\Lk)_{ab,ab})\)</span>. Moreover, <span class="math inline">\(\mathbf{A}\)</span> has only positive diagonal elements, because <span class="math inline">\(\Lk\)</span> is invertible and has only positive diagonal elements and because <span class="math inline">\(q&#39;_{ijab} = p(x_i \eq a, x_j \eq b | \v^*,\w^*) \ge 0\)</span>.</p>
<p>Therefore <span class="math inline">\(\mathbf{A}\)</span> is invertible: <span class="math inline">\(\mathbf{A}^{-1} = \text{diag}(N_{ij} q&#39;_{ijab} + (\Lk)_{ab,ab} )^{-1}\)</span>.</p>
<p>Because <span class="math inline">\(\sum_{a,b=1}^{20} q&#39;_{ijab} = 1\)</span>, the denominator of the second term is</p>
<span class="math display">\[\begin{equation}
    N_{ij}^{-1} - \sum_{a,b=1}^{20}  \frac{{q&#39;}_{ijab}^2}{N_{ij} q&#39;_{ijab} + {(\Lk)}_{ab,ab} } &gt; N_{ij}^{-1} - \sum_{a,b=1}^{20} \frac{{q&#39;}^2_{ijab}}{N_{ij} q&#39;_{ijab}} = 0
\end{equation}\]</span>
<p>and therefore the inverse of <span class="math inline">\(\Lijk\)</span> in eq. @ref(eq:fast-inverse-mat-Lijk) is well defined.</p>
<p>The log determinant of <span class="math inline">\(\Lijk\)</span> is necessary to compute the ratio of Gaussians (see equation @ref(eq:p-X-r-final)) and can be computed using the matrix determinant lemma:</p>
<span class="math display">\[\begin{equation}
  \det(\mathbf{A} + \mathbf{uv}^\mathrm{T}) = (1+\mathbf{v}^\mathrm{T} \mathbf{A}^{-1} \mathbf{u}) \det(\mathbf{A})
\end{equation}\]</span>
<p>Setting <span class="math inline">\(\mathbf{A} = N_{ij} \Qij + \Lk\)</span> and <span class="math inline">\(\v = \qij\)</span> and <span class="math inline">\(\mathbf{u} = - N_{ij} \qij\)</span> yields</p>
<span class="math display">\[\begin{equation}
  \det(\Lijk ) = \det(\H_{ij} - \lambda_w \I + \Lk) = (1 - N_{ij}\qij^\mathrm{T} \mathbf{A}^{-1}\qij) \det(\mathbf{A}) \,.
\end{equation}\]</span>
<p><span class="math inline">\(\mathbf{A}\)</span> is diagonal and has only positive diagonal elements so that <span class="math inline">\(\log(\det(\mathbf{A})) = \sum \log \left( \text{diag}(\mathbf{A}) \right)\)</span>.</p>
</div>
<div id="training-hyperparameters" class="section level2">
<h2><span class="header-section-number">6.7</span> Training the Hyperparameters <span class="math inline">\(\muk\)</span>, <span class="math inline">\(\Lk\)</span> and <span class="math inline">\(\gamma_k\)</span></h2>
<p>The model parameters <span class="math inline">\(\mathbf{\mu} = (\mathbf{\mu}_{1},\ldots,\mathbf{\mu}_K)\)</span>, <span class="math inline">\(\mathbf{\Lambda} = (\mathbf{\Lambda}_1,\ldots,\mathbf{\Lambda}_K)\)</span> and <span class="math inline">\(\mathbf{\gamma} = (\mathbf{\gamma}_1,\ldots,\mathbf{\gamma}_K)\)</span> will be trained by maximizing the logarithm of the full likelihood over a set of training <a href="#abbrev">MSAs</a> <span class="math inline">\(\X^1,\ldots,\X^N\)</span> and associated structures with distance vectors <span class="math inline">\(\r^1,\ldots,\r^N\)</span> plus a regularizer <span class="math inline">\(R(\mathbf{\mu}, \mathbf{\Lambda})\)</span>:</p>
<span class="math display">\[\begin{equation}
    L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \mathbf{\gamma}) + R(\mathbf{\mu}, \mathbf{\Lambda}) = \sum_{n=1}^N  \log p(\X^n | \r^n, \mathbf{\mu}, \mathbf{\Lambda}, \mathbf{\gamma} ) + R(\mathbf{\mu}, \mathbf{\Lambda})  \rightarrow \max \, .
\end{equation}\]</span>
<p>The regulariser penalizes values of <span class="math inline">\(\muk\)</span> and <span class="math inline">\(\Lk\)</span> that deviate too far from zero:</p>
<span class="math display">\[\begin{align}
    R(\mathbf{\mu}, \mathbf{\Lambda}) = -\frac{1}{2 \sigma_{\mu}^2} \sum_{k=1}^K \sum_{ab=1}^{400} \mu_{k,ab}^2 
                        -\frac{1}{2 \sigma_\text{diag}^2} \sum_{k=1}^K \sum_{ab=1}^{400} \Lambda_{k,ab,ab}^2
(\#eq:reg)
\end{align}\]</span>
<p>Reasonable values are <span class="math inline">\(\sigma_{\mu}=0.1\)</span>, <span class="math inline">\(\sigma_\text{diag} = 100\)</span>.</p>
<p>The log likelihood can be optimized using LBFG-S-B<span class="citation">[<span class="citeproc-not-found" data-reference-id="CITE"><strong>???</strong></span>]</span>, which requires the computation of the gradient of the log likelihood. For simplicity of notation, the following calculations consider the contribution of the log likelihood for just one protein, which allows to drop the index <span class="math inline">\(n\)</span> in <span class="math inline">\(\rij^n\)</span>, <span class="math inline">\((\wij^n)^*\)</span> and <span class="math inline">\(\Hij^n\)</span>.</p>
<p>From eq. @ref(eq:pXr-final) the log likelihood for a single protein is</p>
<span class="math display">\[\begin{equation}
    L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k) =  \sum_{1 \le i &lt; j \le L}  \log \sum_{k=0}^K g_{k}(\rij) \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})}  + R(\mathbf{\mu}, \mathbf{\Lambda}) + \text{const.}\,.
(\#eq:ll-coupling-prior)
\end{equation}\]</span>
<div id="the-gradient-of-the-log-likelihood-with-respect-to-mathbfmu" class="section level3">
<h3><span class="header-section-number">6.7.1</span> The gradient of the log likelihood with respect to <span class="math inline">\(\mathbf{\mu}\)</span></h3>
<p>By applying the formula <span class="math inline">\(d f(x) / dx = f(x) \, d \log f(x) / dx\)</span> to compute the gradient of eq. @ref(eq:ll-coupling-prior) (neglecting the regularization term) with respect to <span class="math inline">\(\mu_{k,ab}\)</span>, one obtains</p>
<span class="math display">\[\begin{equation}
 \frac{\partial}{\partial \mu_{k,ab}} L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    = \sum_{1\le i&lt;j\le L}  
    \frac{ 
        g_{k}(\rij) \frac{  \Gauss ( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} 
             \frac{\partial}{\partial \mu_{k,ab}}  \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right)  
     } { \sum_{k&#39;=0}^K g_{k&#39;}(\rij) \, \frac{ \Gauss(\mathbf{0} | \muk&#39;, \Lk&#39;^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})}  } .
(\#eq:gradient-mukab)
\end{equation}\]</span>
<p>To simplify this expression, we define the responsibility of component <span class="math inline">\(k\)</span> for the posterior distribution of <span class="math inline">\(\wij\)</span>, the probability that <span class="math inline">\(\wij\)</span> has been generated by component <span class="math inline">\(k\)</span>:</p>
<span class="math display">\[\begin{align}
      p(k|ij)  = 
      \frac{ g_{k}(\rij) \frac{ \Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss(\mathbf{0} | \muijk, \Lijk^{-1})} } 
    {\sum_{k&#39;=0}^K g_{k&#39;}(\rij) \frac{ \Gauss(\mathbf{0} | \muk&#39;, \Lk&#39;^{-1})}{\Gauss( \mathbf{0} | \muijk&#39;, \Lijk&#39;^{-1})} }  \,.
(\#eq:responsibilities)
\end{align}\]</span>
<p>By substituting the definition for responsibility, @ref(eq:gradient-mukab) simplifies</p>
<span class="math display">\[\begin{equation}
  \frac{\partial}{\partial \mu_{k,ab}}  L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    = \sum_{1\le i&lt;j\le L}  p(k | ij)  \frac{\partial}{\partial \mu_{k,ab}} \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right) ,
(\#eq:gradient-LL-mukab)
\end{equation}\]</span>
<p>and analogously for partial derivatives with respect to <span class="math inline">\(\Lambda_{k,ab,cd}\)</span>.</p>
The partial derivative inside the sum can be written
<span class="math display">\[\begin{equation}
     \frac{\partial}{\partial \mu_{k,ab}} \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right)
    = \frac{1}{2}  \frac{\partial}{\partial \mu_{k,ab}}   \left( \log | \Lk | - \muk^\mathrm{T} \Lk \muk - \log | \Lijk | + \muijk^\mathrm{T} \Lijk \muijk \right)\,.
\end{equation}\]</span>
Using the following formula for a matrix <span class="math inline">\(\mathbf{A}\)</span>, a real variable <span class="math inline">\(x\)</span> and a vector <span class="math inline">\(\mathbf{y}\)</span> that depends on <span class="math inline">\(x\)</span>,
<span class="math display">\[\begin{equation}
    \frac{\partial}{\partial x} \left( \mathbf{y}^\mathrm{T} \mathbf{A} \mathbf{y} \right) = \frac{\partial \mathbf{y}^\mathrm{T}}{\partial x}  \mathbf{A} \mathbf{y} + \mathbf{y}^\mathrm{T} \mathbf{A} \frac{\partial \mathbf{y}}{\partial x}  =  \mathbf{y}^\mathrm{T} (\mathbf{A} + \mathbf{A}^\mathrm{T}) \frac{\partial \mathbf{y}}{\partial x} 
(\#eq:matrix-gradient)
\end{equation}\]</span>
<p>the partial derivative therefore becomes</p>
<span class="math display">\[\begin{align}
     \frac{\partial}{\partial \mu_{k,ab}} \log \left( \frac{ \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \right)
    =&amp; \left( -\muk^\mathrm{T} \Lk \mathbf{e}_{ab} \, +  \muijk^\mathrm{T} \Lijk \Lijk^{-1} \Lk \mathbf{e}_{ab} \right) \\
    =&amp; \mathbf{e}^\mathrm{T}_{ab} \Lk ( \muijk - \muk ) \; . 
\end{align}\]</span>
<p>Finally, the gradient of the log likelihood with respect to <span class="math inline">\(\mathbf{\mu}\)</span> becomes</p>
<span class="math display">\[\begin{align}
    \nabla_{\muk} L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    =  \sum_{1\le i&lt;j\le L}  p(k|ij)  \,  \Lk \left(  \muijk  - \muk \right) \; .
(\#eq:gradient-muk-final)
\end{align}\]</span>
</div>
<div id="the-gradient-of-the-log-likelihood-with-respect-to-lk" class="section level3">
<h3><span class="header-section-number">6.7.2</span> The gradient of the log likelihood with respect to <span class="math inline">\(\Lk\)</span></h3>
<p>Analogously to eq. @ref(eq:gradient-LL-mukab) one first needs to solve</p>
<span class="math display">\[\begin{align}
     &amp; \frac{\partial}{\partial \Lambda_{k,ab,cd}} \log \frac{\Gauss( \mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} 
    = \\
    &amp;\frac{1}{2}  \frac{\partial}{\partial \Lambda_{k,ab,cd}}  \left( \log |\Lk| - \muk^\mathrm{T} \Lk \muk - \log |\Lijk| + \muijk^\mathrm{T} \Lijk \muijk \right) \,,
(\#eq:grad-log-N-N-lambdakabcd)
\end{align}\]</span>
<p>by applying eq. @ref(eq:matrix-gradient) as before as well as the formulas</p>
<span class="math display">\[\begin{align}
    \frac{\partial}{\partial x} \log |\mathbf{A} | &amp;= \text{Tr}\left( \mathbf{A}^{-1} \frac{\partial \mathbf{A}}{\partial x}  \right) , \\
    \frac{\partial \mathbf{A}^{-1}}{\partial x} &amp;= - \mathbf{A}^{-1} \frac{\partial \mathbf{A}}{\partial x} \mathbf{A}^{-1} \,.
\end{align}\]</span>
<p>This yields</p>
<span class="math display">\[\begin{align}
\frac{\partial}{\partial \Lambda_{k,ab,cd}}  \log |\Lk|
     &amp;= \text{Tr} \left( \Lk^{-1} \frac{\partial \Lk}{\partial \Lambda_{k,ab,cd}} \right) 
     = \text{Tr} \left( \Lk^{-1} \mathbf{e}_{ab} \mathbf{e}_{cd}^\mathrm{T} \right) 
     = \Lambda^{-1}_{k,cd,ab} \\
\frac{\partial}{\partial \Lambda_{k,ab,cd}}  \log |\Lijk|
     &amp;= \text{Tr} \left( \Lijk^{-1} \frac{\partial (\H_{ij} - \lambda_w \I + \Lk)}{\partial \Lambda_{k,ab,cd}}   \right) 
     = \Lambda^{-1}_{ij,k,cd,ab} \\
\frac{\partial (\muk^\mathrm{T} \Lk \muk)}{\partial \Lambda_{k,ab,cd}} 
    &amp;= \muk^\mathrm{T} \mathbf{e}_{ab} \mathbf{e}_{cd}^\mathrm{T} \muk 
    = \mathbf{e}_{ab}^\mathrm{T} \muk \muk^\mathrm{T} \mathbf{e}_{cd} = (\muk \muk^\mathrm{T})_{ab,cd} \\
\frac{\partial ( \muijk^\mathrm{T} \Lijk \muijk) }{\partial \Lambda_{k,ab,cd}} 
    &amp;= \muijk^\mathrm{T} \frac{\partial \Lijk}{\partial \Lambda_{k,ab,cd}} \muijk 
    + 2 \muijk^\mathrm{T} \Lijk \frac{\partial \Lijk^{-1}}{\partial \Lambda_{k,ab,cd}}  (\Hij \wij^* + \Lk \muk) 
    + 2 \muijk^\mathrm{T} \frac{\partial \Lk}{\partial \Lambda_{k,ab,cd}} \muk \nonumber \\
    &amp;= (\muijk \muijk^\mathrm{T} + 2 \muijk \muk^\mathrm{T})_{ab,cd} 
    - 2 \muijk^\mathrm{T} \Lijk  \Lijk^{-1} \frac{\partial \Lijk}{\partial \Lambda_{k,ab,cd}} \Lijk^{-1} (\Hij\wij^* + \Lk \muk) \\
    &amp;= (\muijk \muijk^\mathrm{T} + 2 \muijk \muk^\mathrm{T})_{ab,cd} 
    - 2 \muijk^\mathrm{T}  \frac{\partial \Lijk}{\partial \Lambda_{k,ab,cd}} \muijk\\
    &amp;= (- \muijk \muijk^\mathrm{T} + 2 \muijk \muk^\mathrm{T})_{ab,cd} \,.
\end{align}\]</span>
<p>Inserting these results into eq. @ref(eq:grad-log-N-N-lambdakabcd) yields</p>
<span class="math display">\[\begin{align}
     \frac{\partial}{\partial \Lambda_{k,ab,cd}} \log \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} 
    = \frac{1}{2} \left( \Lk^{-1} - \Lijk^{-1} - (\muijk - \muk) (\muijk - \muk)^\mathrm{T} \right)_{ab,cd}\,.
\end{align}\]</span>
<p>Substituting this expression into the equation @ref(eq:gradient-LL-mukab) analogous to the derivation of gradient for <span class="math inline">\(\mu_{k,ab}\)</span> yields the equation</p>
<span class="math display">\[\begin{align}
    \nabla_{\Lk}  L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k)
    =  \frac{1}{2} \sum_{1\le i&lt;j\le L}  p(k|ij)  \, 
        \left( \Lk^{-1} - \Lijk^{-1} - (\muijk - \muk) (\muijk - \muk)^\mathrm{T} \right). 
(\#eq:gradient-lambdak-final)
\end{align}\]</span>
</div>
<div id="the-gradient-of-the-log-likelihood-with-respect-to-gamma_k" class="section level3">
<h3><span class="header-section-number">6.7.3</span> The gradient of the log likelihood with respect to <span class="math inline">\(\gamma_k\)</span></h3>
<p>With <span class="math inline">\(\rij \in \{0,1\}\)</span> defining a residue pair in physical contact or not in contact, the mixing weights can be modelled as a softmax function according to eq. @ref(eq:def-g-k-binary). The derivative of the mixing weights <span class="math inline">\(g_k(\rij)\)</span> is:</p>
<span class="math display">\[\begin{eqnarray}
\frac{\partial g_{k&#39;}(\rij)} {\partial \gamma_k} = \left\{
  \begin{array}{lr}
    g_k(\rij) (1 - g_k(\rij)) &amp; : k&#39; = k\\
    g_{k&#39;}(\rij) - g_k(\rij)  &amp; : k&#39; \neq k
  \end{array}
  \right.
\end{eqnarray}\]</span>
<p>The partial derivative of the likelihood function with respect to <span class="math inline">\(\gamma_k\)</span> is:</p>
<span class="math display">\[\begin{align}
\frac{\partial} {\partial \gamma_k}     L\!L(\mathbf{\mu}, \mathbf{\Lambda}, \gamma_k) 
  =&amp;  \sum_{1\le i&lt;j\le L} \frac{\sum_{k&#39;=0}^K  \frac{\partial}{\partial \gamma_k} g_{k&#39;}(\rij)  
  \frac{\Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( 0 | \muijk, \Lijk^{-1})}}
  {\sum_{k&#39;=0}^K g_{k&#39;}(\rij)  \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})}} \\
  =&amp;  \sum_{1\le i&lt;j\le L} \frac{\sum_{k&#39;=0}^K  g_{k&#39;}(\rij)  
  \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})} \cdot 
  \begin{cases} 
   1-g_k(\rij) &amp; \text{if } k&#39; = k \\
   -g_k(\rij)  &amp; \text{if } k&#39; \neq k
  \end{cases}}
  {\sum_{k&#39;=0}^K g_{k&#39;}(\rij)  \frac{  \Gauss(\mathbf{0} | \muk, \Lk^{-1})}{\Gauss( \mathbf{0} | \muijk, \Lijk^{-1})}} \\
  =&amp; \sum_{1\le i&lt;j\le L} \sum_{k&#39;=0}^K p(k&#39;|ij) 
  \begin{cases} 
    1-g_k(\rij) &amp; \text{if } k&#39; = k \\
    -g_k(\rij)  &amp; \text{if } k&#39; \neq k 
  \end{cases} \\
  =&amp; \sum_{1 \leq i&lt;j\leq L} p(k|ij) - g_k(\rij) \sum_{k&#39;=0}^K p(k&#39;|ij) \nonumber\\
  =&amp; \sum_{1 \leq i&lt;j\leq L} p(k|ij) - g_k(\rij)
\end{align}\]</span>
</div>
</div>
<div id="bayesian-statistical-model-for-prediction-of-protein-residue-residue-distances" class="section level2">
<h2><span class="header-section-number">6.8</span> Bayesian Statistical Model for Prediction of Protein Residue-Residue Distances</h2>
</div>
<div id="modelling-the-dependence-of-wij-on-distance" class="section level2">
<h2><span class="header-section-number">6.9</span> Modelling the dependence of <span class="math inline">\(\wij\)</span> on distance</h2>
<p>It is straightforward to extend the model presented in @ref(coupling-prior) for distances.</p>
<p>The mixture weights <span class="math inline">\(g_k(\rij)\)</span> in eq. @ref(eq:definition-mixture-coupling-prior) are modelled as softmax over linear functions <span class="math inline">\(\gamma_k(\rij)\)</span> (Figure (fig:softmax-linear-fct):</p>
<span class="math display">\[\begin{align}
      g_k(\rij)        &amp;= \frac{\exp \gamma_k(\rij)}{\sum_{k&#39;=0}^K \exp \gamma_{k&#39;}(\rij)} \, , \\
      \gamma_k(\rij)   &amp;= - \sum_{k&#39;=0}^{k} \alpha_{k&#39;} ( \rij - \rho_{k&#39;}) .
(\#eq:definition-mixture-weights)
\end{align}\]</span>
<p>(ref:caption-softmax-linear-fct) The Gaussian mixture coefficients <span class="math inline">\(g_k(\rij)\)</span> of <span class="math inline">\(p(\wij|\rij)\)</span> are modelled as softmax over linear functions <span class="math inline">\(\gamma_k(\rij)\)</span>. <span class="math inline">\(\rho_k\)</span> sets the transition point between neighbouring components <span class="math inline">\(g_{k-1}(\rij)\)</span> and <span class="math inline">\(g_k(\rij)\)</span>, while <span class="math inline">\(\alpha_k\)</span> quantifies the abruptness of the transition between <span class="math inline">\(g_{k-1}(\rij)\)</span> and <span class="math inline">\(g_k(\rij)\)</span>.</p>
<div class="figure">
<img src="img/theory/softmax_linear_fct.png" alt="(ref:caption-softmax-linear-fct)" width="50%" />
<p class="caption">
(#fig:softmax-linear-fct)(ref:caption-softmax-linear-fct)
</p>
</div>
<p>The functions <span class="math inline">\(g_k(\rij)\)</span> remain invariant when adding an offset to all <span class="math inline">\(\gamma_k(\rij)\)</span>. This degeneracy can be removed by setting <span class="math inline">\(\gamma_0(\rij) = 0\)</span> (i.e., <span class="math inline">\(\alpha_0 = 0\)</span> and <span class="math inline">\(\rho_0=0\)</span>). Further, the components are ordered, <span class="math inline">\(\rho_1&gt; \ldots &gt; \rho_K\)</span> and it is demanded that <span class="math inline">\(\alpha_k &gt; 0\)</span> for all <span class="math inline">\(k\)</span>. This ensures that for <span class="math inline">\(\rij \rightarrow \infty\)</span> we will obtain <span class="math inline">\(g_0(\rij) \rightarrow 1\)</span> and hence <span class="math inline">\(p(\w | \X) \rightarrow \Gauss(0, \sigma_0^2 \I )\)</span>.</p>
<p>The parameters <span class="math inline">\(\rho_k\)</span> mark the transition points between the two Gaussian mixture components <span class="math inline">\(k-1\)</span> and <span class="math inline">\(k\)</span>, i.e., the points at which the two components obtain equal weights. This follows from <span class="math inline">\(\gamma_k(\rij) - \gamma_{k-1}(r) = \alpha_{t} ( \rij - \rho_{t})\)</span> and hence <span class="math inline">\(\gamma_{k-1}(\rho_k) = \gamma_k(\rho_k)\)</span>. A change in <span class="math inline">\(\rho_k\)</span> or <span class="math inline">\(\alpha_k\)</span> only changes the behaviour of <span class="math inline">\(g_{k-1}(\rij)\)</span> and <span class="math inline">\(g_k(\rij)\)</span> in the transition region around <span class="math inline">\(\rho_k\)</span>. Therefore, this particular definition of <span class="math inline">\(\gamma_k(\rij)\)</span> makes the parameters <span class="math inline">\(\alpha_k\)</span> and <span class="math inline">\(\rho_k\)</span> as independent of each other as possible, rendering the optimisation of these parameters more efficient.</p>
<div id="training-the-hyperparameters-rho_k-and-alpha_k-for-distance-dependent-prior" class="section level3">
<h3><span class="header-section-number">6.9.1</span> Training the Hyperparameters <span class="math inline">\(\rho_k\)</span> and <span class="math inline">\(\alpha_k\)</span> for distance-dependent prior</h3>
<!--chapter:end:04-methods.Rmd-->
</div>
</div>
</div>
<div id="appendix-appendix" class="section level1 unnumbered">
<h1>(APPENDIX) Appendix</h1>
</div>
<div id="abbrev" class="section level1">
<h1><span class="header-section-number">7</span> Abbreviations</h1>
<p><strong>APC</strong> Avarage Product Correction</p>
<p><strong>CASP</strong> Critical Assessment of protein Structure Prediction</p>
<p><strong>CD</strong> Contrastive Divergence</p>
<p><strong>DCA</strong> Direct Coupling Analysis</p>
<p><strong>DI</strong> Direct Information</p>
<p><strong>EM</strong> electron microscopy</p>
<p><strong>IDP</strong> intrinsically disordered proteins</p>
<p><strong>MAP</strong> Maximum a posteriori</p>
<p><strong>MI</strong> mutual information</p>
<p><strong>ML</strong> Maximum-Likelihood</p>
<p><strong>MLE</strong> Maximum-Likelihood Estimate</p>
<p><strong>MRF</strong> Markov-Random Field</p>
<p><strong>MSA</strong> Multiple Sequence Alignment</p>
<p><strong>PCD</strong> Persistent Contrastive Divergence</p>
<p><strong>PDB</strong> protein data bank</p>
<p>%%%% used as: <a href="#abbrev">MRF</a></p>
</div>
<div id="dataset-properties" class="section level1">
<h1><span class="header-section-number">8</span> Dataset Properties</h1>
<p>The following figures display various statistics about the dataset used throughout this thesis. See section @ref(dataset) for information on how this dataset has been generated.</p>
<div id="alignment-diversity" class="section level2">
<h2><span class="header-section-number">8.1</span> Alignment Diversity</h2>
<p>(ref:caption-dataset-diversity) Distribution of alignment diversity (<span class="math inline">\(=\sqrt(\frac{N}{L})\)</span>) in the dataset an its ten subsets.</p>
<div class="figure">
<iframe src="img/dataset_statistics/diversity_dataset_boxplot_notitle.html" width="100%" height="400px">
</iframe>
<p class="caption">
(#fig:dataset-diversity)(ref:caption-dataset-diversity)
</p>
</div>
</div>
<div id="proportion-of-gaps-in-alignment" class="section level2">
<h2><span class="header-section-number">8.2</span> Proportion of Gaps in Alignment</h2>
<p>(ref:caption-gaps) Distribution of gap percentage of alignments in the dataset an its ten subsets.</p>
<div class="figure">
<iframe src="img/dataset_statistics/gap_percentage_boxplot_notitle.html" width="100%" height="400px">
</iframe>
<p class="caption">
(#fig:dataset-gaps)(ref:caption-gaps)
</p>
</div>
</div>
<div id="alignment-size-number-of-sequences" class="section level2">
<h2><span class="header-section-number">8.3</span> Alignment Size (number of sequences)</h2>
<p>(ref:caption-alignment-size) Distribution of alignment size (number of sequences N) in the dataset an its ten subsets.</p>
<div class="figure">
<iframe src="img/dataset_statistics/msa_size_dataset_boxplot_notitle.html" width="100%" height="400px">
</iframe>
<p class="caption">
(#fig:dataset-alignment-size)(ref:caption-alignment-size)
</p>
</div>
</div>
<div id="protein-length" class="section level2">
<h2><span class="header-section-number">8.4</span> Protein Length</h2>
<p>(ref:caption-protein-length) Distribution of protein length L in the dataset an its ten subsets.</p>
<div class="figure">
<iframe src="img/dataset_statistics/protein_length_dataset_boxplot_notitle.html" width="100%" height="400px">
</iframe>
<p class="caption">
(#fig:dataset-protein-length)(ref:caption-protein-length)
</p>
</div>
</div>
</div>
<div id="amino-acid-interaction-preferences-reflected-in-coupling-matrices" class="section level1">
<h1><span class="header-section-number">9</span> Amino Acid Interaction Preferences Reflected in Coupling Matrices</h1>
<div id="pi-cation" class="section level2">
<h2><span class="header-section-number">9.1</span> Pi-Cation interactions</h2>
<p>Figure @ref(fig:coupling-matrix-pication-pymol) shows a Tyrosine and a Lysine residue forming a cation-<span class="math inline">\(\pi\)</span> interaction in protein 2ayd. The corresponding coupling matrix in figure @ref(fig:coupling-matrix-pication-interaction) reflects the strong interaction preference.</p>
<p>(ref:caption-coupling-matrix-pication-pymol) Tyrosing (residue 37) and Lysine (residue 48) forming a cation-<span class="math inline">\(\pi\)</span> interaction in protein 2ayd.</p>
<div class="figure">
<img src="img/coupling_matrix_analysis/2ayda01_37_48.png" alt="(ref:caption-coupling-matrix-pication-pymol)" width="50%" />
<p class="caption">
(#fig:coupling-matrix-pication-pymol)(ref:caption-coupling-matrix-pication-pymol)
</p>
</div>
<p>(ref:caption-coupling-matrix-pication-interaction) Coupling Matrix for residue pair i=37 and j=48 of PDB 2ayd chain A domain 1. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis represent single potentials for residue i=37 and bars at the y-axis represent single potentials for residue j=48. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values.</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/coupling_matrix_2aydA01_37_48_notitle.html" width="750px" height="750px">
</iframe>
<p class="caption">
(#fig:coupling-matrix-pication-interaction)(ref:caption-coupling-matrix-pication-interaction)
</p>
</div>
</div>
<div id="disulfide" class="section level2">
<h2><span class="header-section-number">9.2</span> Disulfide Bonds</h2>
<p>Figure @ref(fig:coupling-matrix-disulfide-pymol) shows two cysteine residues forming a covalent disulfide bond in protein 1alu. The corresponding coupling matrix in figure @ref(fig:coupling-matrix-disulfide-interaction) reflects the strong interaction preference of cysteines.</p>
<p>(ref:caption-coupling-matrix-disulfide-pymol) Two cystein residues (residues 54 and 64) forming a covalent disulfide bond in protein 1alu.</p>
<div class="figure">
<img src="img/coupling_matrix_analysis/1aluA00_54_64.png" alt="(ref:caption-coupling-matrix-disulfide-pymol)" width="50%" />
<p class="caption">
(#fig:coupling-matrix-disulfide-pymol)(ref:caption-coupling-matrix-disulfide-pymol)
</p>
</div>
<p>(ref:caption-coupling-matrix-disulfide-interaction) Coupling Matrix for residue pair i=54 and j=64 of PDB 1alu chain A. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis represent single potentials for residue i=54 and bars at the y-axis represent single potentials for residue j=64. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values.</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/coupling_matrix_1aluA00_54_64_notitle.html" width="750px" height="750px">
</iframe>
<p class="caption">
(#fig:coupling-matrix-disulfide-interaction)(ref:caption-coupling-matrix-disulfide-interaction)
</p>
</div>
</div>
<div id="aromatic-proline" class="section level2">
<h2><span class="header-section-number">9.3</span> Aromatic-Proline Interactions</h2>
<p>Figure @ref(fig:coupling-matrix-aromatic-proline-pymol )shows a proline and a tryptophan residue forming such a CH/<span class="math inline">\(\pi\)</span> interaction in protein 1aol. The corresponding coupling matrix in figure @ref(fig:coupling-matrix-aromatic-proline) reflects this interaction with strong positive coupling between proline and tryptophan.</p>
<p>(ref:caption-coupling-matrix-aromatic-proline-pymol) Proline and tryptophan (residues 17 and 34) stacked on top of each otherengaging in a CH/<span class="math inline">\(\pi\)</span> interaction in protein 1alu.</p>
<div class="figure">
<img src="img/coupling_matrix_analysis/1aolA00_17_34.png" alt="(ref:caption-coupling-matrix-aromatic-proline-pymol)" width="50%" />
<p class="caption">
(#fig:coupling-matrix-aromatic-proline-pymol)(ref:caption-coupling-matrix-aromatic-proline-pymol)
</p>
</div>
<p>(ref:caption-coupling-matrix-aromatic-proline) Coupling Matrix for residue pair i=17 and j=34 of PDB 1aol chain A. Size of the bubbles represents coupling strength and color represents the direction of coupling: red = positive coupling, blue = negative coupling. Bars at the x-axis represent single potentials for residue i=17 and bars at the y-axis represent single potentials for residue j=34. Height of the bars represents potential strength and color represents positive (red) and negative (blue) values.</p>
<div class="figure">
<iframe src="img/coupling_matrix_analysis/coupling_matrix_1aolA00_17_34_notitle.html" width="750px" height="750px">
</iframe>
<p class="caption">
(#fig:coupling-matrix-aromatic-proline)(ref:caption-coupling-matrix-aromatic-proline)
</p>
</div>
</div>
<div id="aromatic-network" class="section level2">
<h2><span class="header-section-number">9.4</span> Network-like structure of aromatic residues</h2>
<p>(ref:caption-aromatic-network) Network-like structure of aromatic residues in the protein core. 80% of aromatic residues are involved in such networks that are important for protein stability <span class="citation">[<a href="#ref-Burley1985">13</a>]</span>.</p>
<div class="figure">
<img src="img/coupling_matrix_analysis/aromatic_bundle.png" alt="(ref:caption-aromatic-network)" width="50%" />
<p class="caption">
(#fig:aromatic-network)(ref:caption-aromatic-network)
</p>
</div>
<!--chapter:end:10-appendix.Rmd-->

<p></p>
<p></p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<!--chapter:end:12-references.md-->
<div id="refs" class="references">
<div id="ref-Anfinsen1973">
<p>1. Anfinsen, C.B. (1973). Principles that Govern the Folding of Protein Chains. Science (80-. ). <em>181</em>, 223–230. Available at: <a href="http://www.sciencemag.org/content/181/4096/223" class="uri">http://www.sciencemag.org/content/181/4096/223</a>.</p>
</div>
<div id="ref-Wright1999">
<p>2. Wright, P.E., and Dyson, H. (1999). Intrinsically unstructured proteins: re-assessing the protein structure-function paradigm. J. Mol. Biol. <em>293</em>, 321–331. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10550212 http://linkinghub.elsevier.com/retrieve/pii/S0022283699931108" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/10550212 http://linkinghub.elsevier.com/retrieve/pii/S0022283699931108</a>.</p>
</div>
<div id="ref-Samish2015">
<p>3. Samish, I., Bourne, P.E., and Najmanovich, R.J. (2015). Achievements and challenges in structural bioinformatics and computational biophysics. Bioinformatics <em>31</em>, 146–150. Available at: <a href="https://oup.silverchair-cdn.com/oup/backfile/Content{\_}public/Journal/bioinformatics/31/1/10.1093{\_}bioinformatics{\_}btu769/2/btu769.pdf?Expires=1500051331{\&amp;}Signature=WYTe4F7OxZkkcnJmsjt-r8gIyHaHX5ANqUoy6w0PUuete2b{~}5ZU{~}{~}D6KOB1vq5A8MgCnrq3pDHUn0OSgz0QFmtU2RYf907-" class="uri">https://oup.silverchair-cdn.com/oup/backfile/Content{\_}public/Journal/bioinformatics/31/1/10.1093{\_}bioinformatics{\_}btu769/2/btu769.pdf?Expires=1500051331{\&amp;}Signature=WYTe4F7OxZkkcnJmsjt-r8gIyHaHX5ANqUoy6w0PUuete2b{~}5ZU{~}{~}D6KOB1vq5A8MgCnrq3pDHUn0OSgz0QFmtU2RYf907-</a>.</p>
</div>
<div id="ref-Lesk1980">
<p>4. Lesk, A.M., and Chothia, C. (1980). How different amino acid sequences determine similar protein structures: The structure and evolutionary dynamics of the globins. J. Mol. Biol. <em>136</em>, 225–270. Available at: <a href="http://linkinghub.elsevier.com/retrieve/pii/0022283680903733" class="uri">http://linkinghub.elsevier.com/retrieve/pii/0022283680903733</a>.</p>
</div>
<div id="ref-Sander1991">
<p>5. Sander, C., and Schneider, R. (1991). Database of homology-derived protein structures and the structural meaning of sequence alignment. Proteins <em>9</em>, 56–68. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/2017436" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/2017436</a>.</p>
</div>
<div id="ref-Chothia1986">
<p>6. Chothia, C., and Lesk, A.M. (1986). The relation between the divergence of sequence and structure in proteins. EMBO J. <em>5</em>, 823–6. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/3709526 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1166865" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/3709526 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1166865</a>.</p>
</div>
<div id="ref-Marti-Renom2000">
<p>7. Martí-Renom, M.A., Stuart, A.C., Fiser, A., Sánchez, R., Melo, F., and Šali, A. (2000). Comparative Protein Structure Modeling of Genes and Genomes. Annu. Rev. Biophys. Biomol. Struct. <em>29</em>, 291–325. Available at: <a href="http://www.annualreviews.org/doi/10.1146/annurev.biophys.29.1.291" class="uri">http://www.annualreviews.org/doi/10.1146/annurev.biophys.29.1.291</a>.</p>
</div>
<div id="ref-Dorn2014">
<p>8. Dorn, M., Silva, M.B. e, Buriol, L.S., and Lamb, L.C. (2014). Three-dimensional protein structure prediction: Methods and computational strategies. Comput. Biol. Chem. <em>53</em>, 251–276. Available at: <a href="http://linkinghub.elsevier.com/retrieve/pii/S1476927114001248" class="uri">http://linkinghub.elsevier.com/retrieve/pii/S1476927114001248</a>.</p>
</div>
<div id="ref-Berman2000">
<p>9. Berman, H.M. (2000). The Protein Data Bank. Nucleic Acids Res. <em>28</em>, 235–242. Available at: <a href="http://nar.oxfordjournals.org/content/28/1/235.short" class="uri">http://nar.oxfordjournals.org/content/28/1/235.short</a>.</p>
</div>
<div id="ref-TheUniProtConsortium2013">
<p>10. The UniProt Consortium (2013). Update on activities at the Universal Protein Resource (UniProt) in 2013. Nucleic Acids Res. <em>41</em>, D43–7. Available at: <a href="http://nar.oxfordjournals.org/content/41/D1/D43" class="uri">http://nar.oxfordjournals.org/content/41/D1/D43</a>.</p>
</div>
<div id="ref-Egelman2016">
<p>11. Egelman, E.H. (2016). The Current Revolution in Cryo-EM. Biophysj <em>110</em>, 1008–1012. Available at: <a href="http://www.cell.com/biophysj/pdf/S0006-3495(16)00142-9.pdf" class="uri">http://www.cell.com/biophysj/pdf/S0006-3495(16)00142-9.pdf</a>.</p>
</div>
<div id="ref-Waters2002">
<p>12. Waters, M.L. (2002). Aromatic interactions in model systems. Curr. Opin. Chem. Biol. <em>6</em>, 736–741. Available at: <a href="http://dx.doi.org/10.1016/S1367-5931(02)00359-9" class="uri">http://dx.doi.org/10.1016/S1367-5931(02)00359-9</a>.</p>
</div>
<div id="ref-Burley1985">
<p>13. Burley, S., and Petsko, G. (1985). Aromatic-aromatic interaction: a mechanism of protein structure stabilization. Science (80-. ). <em>229</em>, 23–28. Available at: <a href="http://www.sciencemag.org/cgi/doi/10.1126/science.3892686" class="uri">http://www.sciencemag.org/cgi/doi/10.1126/science.3892686</a>.</p>
</div>
<div id="ref-Thornton1981">
<p>14. Thornton, J.M. (1981). Disulphide bridges in globular proteins. J. Mol. Biol. <em>151</em>, 261–87. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/7338898" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/7338898</a>.</p>
</div>
<div id="ref-Bastolla2005">
<p>15. Bastolla, U., and Demetrius, L. (2005). Stability constraints and protein evolution: the role of chain length, composition and disulfide bonds. Protein Eng. Des. Sel. <em>18</em>, 405–415. Available at: <a href="https://academic.oup.com/peds/article-lookup/doi/10.1093/protein/gzi045" class="uri">https://academic.oup.com/peds/article-lookup/doi/10.1093/protein/gzi045</a>.</p>
</div>
<div id="ref-Donald2011">
<p>16. Donald, J.E., Kulp, D.W., and DeGrado, W.F. (2011). Salt bridges: geometrically specific, designable interactions. Proteins <em>79</em>, 898–915. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3069487{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract" class="uri">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3069487{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract</a>.</p>
</div>
<div id="ref-Burley1986">
<p>17. Burley, S., and Petsko, G. (1986). Amino-aromatic interactions in proteins. FEBS Lett. <em>203</em>, 139–143. Available at: <a href="http://dx.doi.org/10.1016/0014-5793(86)80730-X" class="uri">http://dx.doi.org/10.1016/0014-5793(86)80730-X</a>.</p>
</div>
<div id="ref-Crowley2005">
<p>18. Crowley, P.B., and Golovin, A. (2005). Cation-pi interactions in protein-protein interfaces. Proteins <em>59</em>, 231–9. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/15726638" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/15726638</a>.</p>
</div>
<div id="ref-Slutsky2004">
<p>19. Slutsky, M.M., and Marsh, E.N.G. (2004). Cation-pi interactions studied in a model coiled-coil peptide. Protein Sci. <em>13</em>, 2244–51. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2279832{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract" class="uri">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2279832{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract</a>.</p>
</div>
<div id="ref-Livingstone1993">
<p>20. Livingstone, C.D., and Barton, G.J. (1993). Protein sequence alignments: a strategy for the hierarchical analysis of residue conservation. Bioinformatics <em>9</em>, 745–756. Available at: <a href="http://bioinformatics.oxfordjournals.org/content/9/6/745" class="uri">http://bioinformatics.oxfordjournals.org/content/9/6/745</a>.</p>
</div>
<div id="ref-Levinthal1969">
<p>21. Levinthal, C. (1969). How to Fold Graciously. 22–24. Available at: <a href="http://www.citeulike.org/user/FBerkemeier/article/380320" class="uri">http://www.citeulike.org/user/FBerkemeier/article/380320</a>.</p>
</div>
<div id="ref-Rost1999">
<p>22. Rost, B. (1999). Twilight zone of protein sequence alignments. Protein Eng. Des. Sel. <em>12</em>, 85–94. Available at: <a href="http://peds.oxfordjournals.org/content/12/2/85.full" class="uri">http://peds.oxfordjournals.org/content/12/2/85.full</a>.</p>
</div>
<div id="ref-Yan2013">
<p>23. Yan, R., Xu, D., Yang, J., Walker, S., and Zhang, Y. (2013). A comparative assessment and analysis of 20 representative sequence alignment methods for protein structure prediction. Sci. Rep. <em>3</em>, 2619. Available at: <a href="http://www.nature.com/srep/2013/130910/srep02619/full/srep02619.html" class="uri">http://www.nature.com/srep/2013/130910/srep02619/full/srep02619.html</a>.</p>
</div>
<div id="ref-Meier2015">
<p>24. Meier, A. (2015). Automatic Prediction of Protein 3D Structures by Probabilistic Multi-template Homology Modeling. PLoS Comput Biol <em>11</em>, 1–20.</p>
</div>
<div id="ref-Gu2009">
<p>25. Gu, J., and Bourne, P.E. (2009). Structural Bioinformatics (Wiley-Blackwell) Available at: <a href="http://www.amazon.com/Structural-Bioinformatics-Jenny-Gu/dp/0470181052" class="uri">http://www.amazon.com/Structural-Bioinformatics-Jenny-Gu/dp/0470181052</a>.</p>
</div>
<div id="ref-Eswar2007">
<p>26. Eswar, N., Webb, B., Marti-Renom, M.A., Madhusudhan, M.S., Eramian, D., Shen, M.-Y., Pieper, U., and Sali, A. (2007). Comparative protein structure modeling using MODELLER. Curr. Protoc. Protein Sci. <em>Chapter 2</em>, Unit 2.9. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/18429317" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/18429317</a>.</p>
</div>
<div id="ref-Bates2001">
<p>27. Bates, P.A., Kelley, L.A., MacCallum, R.M., and Sternberg, M.J. (2001). Enhancement of protein modeling by human intervention in applying the automatic programs 3D-JIGSAW and 3D-PSSM. Proteins <em>Suppl 5</em>, 39–46. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/11835480" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/11835480</a>.</p>
</div>
<div id="ref-Arnold2006">
<p>28. Arnold, K., Bordoli, L., Kopp, J., and Schwede, T. (2006). The SWISS-MODEL workspace: a web-based environment for protein structure homology modelling. Bioinformatics <em>22</em>, 195–201. Available at: <a href="http://bioinformatics.oxfordjournals.org/content/22/2/195.short" class="uri">http://bioinformatics.oxfordjournals.org/content/22/2/195.short</a>.</p>
</div>
<div id="ref-Gobel1994">
<p>29. Göbel, U., Sander, C., Schneider, R., and Valencia, A. (1994). Correlated mutations and residue contacts in proteins. Proteins <em>18</em>, 309–17. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/8208723" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/8208723</a>.</p>
</div>
<div id="ref-Dunn2008">
<p>30. Dunn, S.D., Wahl, L.M., and Gloor, G.B. (2008). Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction. Bioinformatics <em>24</em>, 333–40. Available at: <a href="http://bioinformatics.oxfordjournals.org/content/24/3/333" class="uri">http://bioinformatics.oxfordjournals.org/content/24/3/333</a>.</p>
</div>
<div id="ref-Weigt2009">
<p>31. Weigt, M., White, R.A., Szurmant, H., Hoch, J.A., and Hwa, T. (2009). Identification of direct residue contacts in protein-protein interaction by message passing. Proc. Natl. Acad. Sci. U. S. A. <em>106</em>, 67–72. Available at: <a href="http://www.pnas.org/content/106/1/67.abstract" class="uri">http://www.pnas.org/content/106/1/67.abstract</a>.</p>
</div>
<div id="ref-Neher1994">
<p>32. Neher, E. (1994). How frequent are correlated changes in families of protein sequences? Proc. Natl. Acad. Sci. U. S. A. <em>91</em>, 98–102. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=42893{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract" class="uri">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=42893{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract</a>.</p>
</div>
<div id="ref-Taylor1994">
<p>33. Taylor, W.R., and Hatrick, K. (1994). Compensating changes in protein multiple sequence alignments. &quot;Protein Eng. Des. Sel. <em>7</em>, 341–348. Available at: <a href="http://peds.oxfordjournals.org/content/7/3/341.abstract" class="uri">http://peds.oxfordjournals.org/content/7/3/341.abstract</a>.</p>
</div>
<div id="ref-Oliveira2002">
<p>34. Oliveira, L., Paiva, A.C.M., and Vriend, G. (2002). Correlated mutation analyses on very large sequence families. Chembiochem <em>3</em>, 1010–7. Available at: <a href="http://onlinelibrary.wiley.com/doi/10.1002/1439-7633(20021004)3:10{\%}3C1010::AID-CBIC1010{\%}3E3.0.CO;2-T/full" class="uri">http://onlinelibrary.wiley.com/doi/10.1002/1439-7633(20021004)3:10{\%}3C1010::AID-CBIC1010{\%}3E3.0.CO;2-T/full</a>.</p>
</div>
<div id="ref-Clarke1995">
<p>35. Clarke, N.D. (1995). Covariation of residues in the homeodomain sequence family. Protein Sci. <em>4</em>, 2269–78. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2143025{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract" class="uri">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2143025{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract</a>.</p>
</div>
<div id="ref-Korber1993">
<p>36. Korber, B. (1993). Covariation of Mutations in the V3 Loop of Human Immunodeficiency Virus Type 1 Envelope Protein: An Information Theoretic Analysis. Proc. Natl. Acad. Sci. <em>90</em>, 7176–7180. Available at: <a href="http://www.pnas.org/content/90/15/7176.abstract?ijkey=6e1c9cbdef66bd0beefedc88ea07591b837f2213{\&amp;}keytype2=tf{\_}ipsecsha" class="uri">http://www.pnas.org/content/90/15/7176.abstract?ijkey=6e1c9cbdef66bd0beefedc88ea07591b837f2213{\&amp;}keytype2=tf{\_}ipsecsha</a>.</p>
</div>
<div id="ref-Martin2005">
<p>37. Martin, L.C., Gloor, G.B., Dunn, S.D., and Wahl, L.M. (2005). Using information theory to search for co-evolving residues in proteins. Bioinformatics <em>21</em>, 4116–24. Available at: <a href="http://bioinformatics.oxfordjournals.org/content/21/22/4116.full" class="uri">http://bioinformatics.oxfordjournals.org/content/21/22/4116.full</a>.</p>
</div>
<div id="ref-Atchley2000">
<p>38. Atchley, W.R., Wollenberg, K.R., Fitch, W.M., Terhalle, W., and Dress, A.W. (2000). Correlations Among Amino Acid Sites in bHLH Protein Domains: An Information Theoretic Analysis. Mol. Biol. Evol. <em>17</em>, 164–178. Available at: <a href="http://mbe.oxfordjournals.org/content/17/1/164.abstract?ijkey=2a1f0a044a8fd2213955e4f2c17fa84682dd2571{\&amp;}keytype2=tf{\_}ipsecsha" class="uri">http://mbe.oxfordjournals.org/content/17/1/164.abstract?ijkey=2a1f0a044a8fd2213955e4f2c17fa84682dd2571{\&amp;}keytype2=tf{\_}ipsecsha</a>.</p>
</div>
<div id="ref-Fodor2004">
<p>39. Fodor, A.A., and Aldrich, R.W. (2004). Influence of conservation on calculations of amino acid covariance in multiple sequence alignments. Proteins <em>56</em>, 211–21. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/15211506" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/15211506</a>.</p>
</div>
<div id="ref-Tillier2003">
<p>40. Tillier, E.R., and Lui, T.W. (2003). Using multiple interdependency to separate functional from phylogenetic correlations in protein alignments. Bioinformatics <em>19</em>, 750–755. Available at: <a href="http://bioinformatics.oxfordjournals.org/content/19/6/750.abstract?ijkey=d19c5a2492bfa46d1a02d661c29a610113da8db3{\&amp;}keytype2=tf{\_}ipsecsha" class="uri">http://bioinformatics.oxfordjournals.org/content/19/6/750.abstract?ijkey=d19c5a2492bfa46d1a02d661c29a610113da8db3{\&amp;}keytype2=tf{\_}ipsecsha</a>.</p>
</div>
<div id="ref-Gouveia_Oliveira2007">
<p>41. Gouveia-Oliveira, R., and Pedersen, A.G. (2007). Finding coevolving amino acid residues using row and column weighting of mutual information and multi-dimensional amino acid representation. Algorithms Mol. Biol. <em>2</em>, 12. Available at: <a href="http://www.almob.org/content/2/1/12" class="uri">http://www.almob.org/content/2/1/12</a>.</p>
</div>
<div id="ref-Kass2002">
<p>42. Kass, I., and Horovitz, A. (2002). Mapping pathways of allosteric communication in GroEL by analysis of correlated mutations. Proteins <em>48</em>, 611–7. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/12211028" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/12211028</a>.</p>
</div>
<div id="ref-Noivirt2005">
<p>43. Noivirt, O., Eisenstein, M., and Horovitz, A. (2005). Detection and reduction of evolutionary noise in correlated mutation analysis. Protein Eng. Des. Sel. <em>18</em>, 247–53. Available at: <a href="http://peds.oxfordjournals.org/content/18/5/247.full" class="uri">http://peds.oxfordjournals.org/content/18/5/247.full</a>.</p>
</div>
<div id="ref-DeJuan2013">
<p>44. Juan, D. de, Pazos, F., and Valencia, A. (2013). Emerging methods in protein co-evolution. Nat. Rev. Genet. <em>14</em>, 249–61. Available at: <a href="http://www.readcube.com/articles/10.1038/nrg3414?locale=en" class="uri">http://www.readcube.com/articles/10.1038/nrg3414?locale=en</a>.</p>
</div>
<div id="ref-Jones2012">
<p>45. Jones, D.T., Buchan, D.W.A., Cozzetto, D., and Pontil, M. (2012). PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments. Bioinformatics <em>28</em>, 184–90. Available at: <a href="http://bioinformatics.oxfordjournals.org/content/28/2/184.full" class="uri">http://bioinformatics.oxfordjournals.org/content/28/2/184.full</a>.</p>
</div>
<div id="ref-Lapedes1999">
<p>46. Lapedes, A., Giraud, B., Liu, L., and Stormo, G. (1999). Correlated mutations in models of protein sequences: phylogenetic and structural effects. <em>33</em>, 236–256. Available at: <a href="http://www.citeulike.org/user/qluo/article/5092214" class="uri">http://www.citeulike.org/user/qluo/article/5092214</a>.</p>
</div>
<div id="ref-Burger2008">
<p>47. Burger, L., and Nimwegen, E. van (2008). Accurate prediction of protein-protein interactions from sequence alignments using a Bayesian method. Mol. Syst. Biol. <em>4</em>, 165. Available at: <a href="http://msb.embopress.org/content/4/1/165.abstract" class="uri">http://msb.embopress.org/content/4/1/165.abstract</a>.</p>
</div>
<div id="ref-Burger2010">
<p>48. Burger, L., and Nimwegen, E. van (2010). Disentangling direct from indirect co-evolution of residues in protein alignments. PLoS Comput. Biol. <em>6</em>, e1000633. Available at: <a href="http://dx.plos.org/10.1371/journal.pcbi.1000633" class="uri">http://dx.plos.org/10.1371/journal.pcbi.1000633</a>.</p>
</div>
<div id="ref-Monastyrskyy2015">
<p>49. Monastyrskyy, B., D’Andrea, D., Fidelis, K., Tramontano, A., and Kryshtafovych, A. (2015). New encouraging developments in contact prediction: Assessment of the CASP11 results. Proteins. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/26474083" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/26474083</a>.</p>
</div>
<div id="ref-Bettsa">
<p>50. Betts, M.J., and Russell, R.B. Amino Acid Properties and Consequences of Substitutions. In Bioinforma. genet. (Chichester, UK: John Wiley &amp; Sons, Ltd), pp. 289–316. Available at: <a href="http://doi.wiley.com/10.1002/0470867302.ch14" class="uri">http://doi.wiley.com/10.1002/0470867302.ch14</a>.</p>
</div>
<div id="ref-Duarte2010">
<p>51. Duarte, J.M., Sathyapriya, R., Stehr, H., Filippis, I., and Lappe, M. (2010). Optimal contact definition for reconstruction of contact maps. BMC Bioinformatics <em>11</em>, 283. Available at: <a href="http://www.biomedcentral.com/1471-2105/11/283" class="uri">http://www.biomedcentral.com/1471-2105/11/283</a>.</p>
</div>
<div id="ref-Anishchenko2017">
<p>52. Anishchenko, I., Ovchinnikov, S., Kamisetty, H., and Baker, D. (2017). Origins of coevolution between residues distant in protein 3D structures. Proc. Natl. Acad. Sci., 201702664. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/28784799 http://www.pnas.org/lookup/doi/10.1073/pnas.1702664114" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/28784799 http://www.pnas.org/lookup/doi/10.1073/pnas.1702664114</a>.</p>
</div>
<div id="ref-Jaynes1957a">
<p>53. Jaynes, E.T. (1957). Information Theory and Statistical Mechanics I. Phys. Rev. <em>106</em>, 620–630. Available at: <a href="https://link.aps.org/doi/10.1103/PhysRev.106.620" class="uri">https://link.aps.org/doi/10.1103/PhysRev.106.620</a>.</p>
</div>
<div id="ref-Jaynes1957b">
<p>54. Jaynes, E.T. (1957). Information Theory and Statistical Mechanics. II. Phys. Rev. <em>108</em>, 171–190. Available at: <a href="https://link.aps.org/doi/10.1103/PhysRev.108.171" class="uri">https://link.aps.org/doi/10.1103/PhysRev.108.171</a>.</p>
</div>
<div id="ref-Wainwright2007">
<p>55. Wainwright, M.J., and Jordan, M.I. (2007). Graphical Models, Exponential Families, and Variational Inference. Found. Trends Mach. Learn. <em>1</em>, 1–305. Available at: <a href="http://www.nowpublishers.com/article/Details/MAL-001" class="uri">http://www.nowpublishers.com/article/Details/MAL-001</a>.</p>
</div>
<div id="ref-Murphy2012">
<p>56. Murphy, K.P. (2012). Machine Learning: A Probabilistic Perspective (MIT Press).</p>
</div>
<div id="ref-Morcos2011">
<p>57. Morcos, F., Pagnani, A., Lunt, B., Bertolino, A., Marks, D.S., Sander, C., Zecchina, R., Onuchic, J.N., Hwa, T., and Weigt, M. (2011). Direct-coupling analysis of residue coevolution captures native contacts across many protein families. Proc. Natl. Acad. Sci. U. S. A. <em>108</em>, E1293–301. Available at: <a href="http://www.pnas.org/content/108/49/E1293.full" class="uri">http://www.pnas.org/content/108/49/E1293.full</a>.</p>
</div>
<div id="ref-Marks2011">
<p>58. Marks, D.S., Colwell, L.J., Sheridan, R., Hopf, T.A., Pagnani, A., Zecchina, R., and Sander, C. (2011). Protein 3D structure computed from evolutionary sequence variation. PLoS One <em>6</em>, e28766. Available at: <a href="http://dx.plos.org/10.1371/journal.pone.0028766" class="uri">http://dx.plos.org/10.1371/journal.pone.0028766</a>.</p>
</div>
<div id="ref-Cocco2017">
<p>59. Cocco, S., Feinauer, C., Figliuzzi, M., Monasson, R., and Weigt, M. (2017). Inverse Statistical Physics of Protein Sequences: A Key Issues Review. arXiv. Available at: <a href="https://arxiv.org/pdf/1703.01222.pdf" class="uri">https://arxiv.org/pdf/1703.01222.pdf</a>.</p>
</div>
<div id="ref-Koller2009">
<p>60. Koller, D., and Friedman, N.I.R. (2009). Probabilistic graphical models: Principles and Techniques (MIT Press).</p>
</div>
<div id="ref-Ekeberg2013">
<p>61. Ekeberg, M., Lövkvist, C., Lan, Y., Weigt, M., and Aurell, E. (2013). Improved contact prediction in proteins: Using pseudolikelihoods to infer Potts models. Phys. Rev. E <em>87</em>, 012707. Available at: <a href="http://link.aps.org/doi/10.1103/PhysRevE.87.012707" class="uri">http://link.aps.org/doi/10.1103/PhysRevE.87.012707</a>.</p>
</div>
<div id="ref-Stein2015a">
<p>62. Stein, R.R., Marks, D.S., and Sander, C. (2015). Inferring Pairwise Interactions from Biological Data Using Maximum-Entropy Probability Models. PLOS Comput. Biol. <em>11</em>, e1004182. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4520494{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract" class="uri">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4520494{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract</a>.</p>
</div>
<div id="ref-Seemayer2014">
<p>63. Seemayer, S., Gruber, M., and Söding, J. (2014). CCMpred-fast and precise prediction of protein residue-residue contacts from correlated mutations. Bioinformatics, btu500. Available at: <a href="http://bioinformatics.oxfordjournals.org/content/early/2014/08/12/bioinformatics.btu500" class="uri">http://bioinformatics.oxfordjournals.org/content/early/2014/08/12/bioinformatics.btu500</a>.</p>
</div>
<div id="ref-Ekeberg2014">
<p>64. Ekeberg, M., Hartonen, T., and Aurell, E. (2014). Fast pseudolikelihood maximization for direct-coupling analysis of protein structure from many homologous amino-acid sequences. J. Comput. Phys. <em>276</em>, 341–356. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S0021999114005178" class="uri">http://www.sciencedirect.com/science/article/pii/S0021999114005178</a>.</p>
</div>
<div id="ref-Kamisetty2013">
<p>65. Kamisetty, H., Ovchinnikov, S., and Baker, D. (2013). Assessing the utility of coevolution-based residue-residue contact predictions in a sequence- and structure-rich era. Proc. Natl. Acad. Sci. U. S. A. <em>110</em>, 15674–9. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3785744{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract" class="uri">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3785744{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract</a>.</p>
</div>
<div id="ref-Balakrishnan2011">
<p>66. Balakrishnan, S., Kamisetty, H., Carbonell, J.G., Lee, S.-I., and Langmead, C.J. (2011). Learning generative models for protein fold families. Proteins <em>79</em>, 1061–78. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/21268112" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/21268112</a>.</p>
</div>
<div id="ref-Friedman2008">
<p>67. Friedman, J., Hastie, T., and Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics <em>9</em>, 432–41. Available at: <a href="http://biostatistics.oxfordjournals.org/content/9/3/432.abstract" class="uri">http://biostatistics.oxfordjournals.org/content/9/3/432.abstract</a>.</p>
</div>
<div id="ref-Baldassi2014">
<p>68. Baldassi, C., Zamparo, M., Feinauer, C., Procaccini, A., Zecchina, R., Weigt, M., and Pagnani, A. (2014). Fast and accurate multivariate gaussian modeling of protein families: predicting residue contacts and protein-interaction partners. PLoS One <em>9</em>, e92721. Available at: <a href="http://dx.plos.org/10.1371/journal.pone.0092721" class="uri">http://dx.plos.org/10.1371/journal.pone.0092721</a>.</p>
</div>
<div id="ref-Besag1975">
<p>69. Besag, J. (1975). Statistical Analysis of Non-Lattice Data. Source Stat. <em>24</em>, 179–195. Available at: <a href="http://www.jstor.org http://www.jstor.org/stable/2987782" class="uri">http://www.jstor.org http://www.jstor.org/stable/2987782</a>.</p>
</div>
<div id="ref-Gidas1988">
<p>70. Gidas, B. (1988). Consistency of maximum likelihood and pseudo-likelihood estimators for Gibbs Distributions. Stoch. Differ. Syst. Stoch. Control Theory Appl. Available at: <a href="http://www.researchgate.net/publication/244456377{\_}Consistency{\_}of{\_}maximum{\_}likelihood{\_}and{\_}pseudo-likelihood{\_}estimators{\_}for{\_}Gibbs{\_}Distributions" class="uri">http://www.researchgate.net/publication/244456377{\_}Consistency{\_}of{\_}maximum{\_}likelihood{\_}and{\_}pseudo-likelihood{\_}estimators{\_}for{\_}Gibbs{\_}Distributions</a>.</p>
</div>
<div id="ref-Feinauer2014">
<p>71. Feinauer, C., Skwark, M.J., Pagnani, A., and Aurell, E. (2014). Improving contact prediction along three dimensions. 19. Available at: <a href="http://arxiv.org/abs/1403.0379" class="uri">http://arxiv.org/abs/1403.0379</a>.</p>
</div>
<div id="ref-Zhang2016">
<p>72. Zhang, H., Gao, Y., Deng, M., Wang, C., Zhu, J., Li, S.C., Zheng, W.-M., and Bu, D. (2016). Improving residue-residue contact prediction via low-rank and sparse decomposition of residue correlation matrix. Biochem. Biophys. Res. Commun. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S0006291X16301838" class="uri">http://www.sciencedirect.com/science/article/pii/S0006291X16301838</a>.</p>
</div>
<div id="ref-Marks2012">
<p>73. Marks, D.S., Hopf, T.A., and Sander, C. (2012). Protein structure prediction from sequence variation. Nat. Biotechnol. <em>30</em>, 1072–1080. Available at: <a href="http://www.nature.com/nbt/journal/v30/n11/full/nbt.2419.html" class="uri">http://www.nature.com/nbt/journal/v30/n11/full/nbt.2419.html</a>.</p>
</div>
<div id="ref-Buslje2009">
<p>74. Buslje, C.M., Santos, J., Delfino, J.M., and Nielsen, M. (2009). Correction for phylogeny, small number of observations and data redundancy improves the identification of coevolving amino acid pairs using mutual information. Bioinformatics <em>25</em>, 1125–31. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/19276150 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2672635" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/19276150 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2672635</a>.</p>
</div>
<div id="ref-Finn2016">
<p>75. Finn, R.D., Coggill, P., Eberhardt, R.Y., Eddy, S.R., Mistry, J., Mitchell, A.L., Potter, S.C., Punta, M., Qureshi, M., and Sangrador-Vegas, A. <em>et al.</em> (2016). The Pfam protein families database: towards a more sustainable future. Nucleic Acids Res. <em>44</em>, D279–D285. Available at: <a href="https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkv1344" class="uri">https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkv1344</a>.</p>
</div>
<div id="ref-Remmert2012">
<p>76. Remmert, M., Biegert, A., Hauser, A., and Söding, J. (2012). HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment. Nat. Methods <em>9</em>, 173–5. Available at: <a href="http://dx.doi.org/10.1038/nmeth.1818" class="uri">http://dx.doi.org/10.1038/nmeth.1818</a>.</p>
</div>
<div id="ref-Espada2014">
<p>77. Espada, R., Parra, R.G., Mora, T., Walczak, A.M., and Ferreiro, D. (2015). Capturing coevolutionary signals in repeat proteins. BMC Bioinformatics <em>16</em>, 207. Available at: <a href="http://arxiv.org/abs/1407.6903" class="uri">http://arxiv.org/abs/1407.6903</a>.</p>
</div>
<div id="ref-Toth-Petroczy2016">
<p>78. Toth-Petroczy, A., Palmedo, P., Ingraham, J., Hopf, T.A., Berger, B., Sander, C., Marks, D.S., Alexander, P., He, Y., and Chen, Y. <em>et al.</em> (2016). Structured States of Disordered Proteins from Genomic Sequences. Cell <em>167</em>, 158–170.e12. Available at: <a href="http://linkinghub.elsevier.com/retrieve/pii/S0092867416312430" class="uri">http://linkinghub.elsevier.com/retrieve/pii/S0092867416312430</a>.</p>
</div>
<div id="ref-Hopf2012">
<p>79. Hopf, T.A., Colwell, L.J., Sheridan, R., Rost, B., Sander, C., and Marks, D.S. (2012). Three-dimensional structures of membrane proteins from genomic sequencing. Cell <em>149</em>, 1607–21. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3641781{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract" class="uri">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3641781{\&amp;}tool=pmcentrez{\&amp;}rendertype=abstract</a>.</p>
</div>
<div id="ref-Lee2009">
<p>80. Lee, B.-C., and Kim, D. (2009). A new method for revealing correlated mutations under the structural and functional constraints in proteins. Bioinformatics <em>25</em>, 2506–13. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/19628501" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/19628501</a>.</p>
</div>
<div id="ref-Wang2015">
<p>81. Wang, Y., and Barth, P. (2015). Evolutionary-guided de novo structure prediction of self-associated transmembrane helical proteins with near-atomic accuracy. Nat. Commun. <em>6</em>, 7196. Available at: <a href="http://www.nature.com/ncomms/2015/150521/ncomms8196/abs/ncomms8196.html" class="uri">http://www.nature.com/ncomms/2015/150521/ncomms8196/abs/ncomms8196.html</a>.</p>
</div>
<div id="ref-Sutto2015">
<p>82. Sutto, L., Marsili, S., Valencia, A., and Gervasio, F.L. (2015). From residue coevolution to protein conformational ensembles and functional dynamics. Proc. Natl. Acad. Sci. U. S. A., 1508584112. Available at: <a href="http://www.pnas.org/content/early/2015/10/20/1508584112.abstract" class="uri">http://www.pnas.org/content/early/2015/10/20/1508584112.abstract</a>.</p>
</div>
<div id="ref-Jana2014">
<p>83. Jana, B., Morcos, F., and Onuchic, J.N. (2014). From structure to function: the convergence of structure based models and co-evolutionary information. Phys. Chem. Chem. Phys. <em>16</em>, 6496. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/24603809 http://xlink.rsc.org/?DOI=c3cp55275f" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/24603809 http://xlink.rsc.org/?DOI=c3cp55275f</a>.</p>
</div>
<div id="ref-DosSantos2015a">
<p>84. Dos Santos, R.N., Morcos, F., Jana, B., Andricopulo, A.D., and Onuchic, J.N. (2015). Dimeric interactions and complex formation using direct coevolutionary couplings. Sci. Rep. <em>5</em>, 13652. Available at: <a href="http://www.nature.com/srep/2015/150904/srep13652/full/srep13652.html" class="uri">http://www.nature.com/srep/2015/150904/srep13652/full/srep13652.html</a>.</p>
</div>
<div id="ref-Ovchinnikov2015b">
<p>85. Ovchinnikov, S., Kim, D.E., Wang, R.Y.-R., Liu, Y., DiMaio, F., and Baker, D. (2015). Improved de novo structure prediction in CASP11 by incorporating Co-evolution information into rosetta. Proteins. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/26677056" class="uri">http://www.ncbi.nlm.nih.gov/pubmed/26677056</a>.</p>
</div>
<div id="ref-Noel2016">
<p>86. Noel, J.K., Morcos, F., and Onuchic, J.N. (2016). Sequence co-evolutionary information is a natural partner to minimally-frustrated models of biomolecular dynamics. F1000Research <em>5</em>. Available at: <a href="http://f1000research.com/articles/5-106/v1" class="uri">http://f1000research.com/articles/5-106/v1</a>.</p>
</div>
<div id="ref-Sfriso2016">
<p>87. Sfriso, P., Duran-Frigola, M., Mosca, R., Emperador, A., Aloy, P., and Orozco, M. (2016). Residues Coevolution Guides the Systematic Identification of Alternative Functional Conformations in Proteins. Structure <em>24</em>, 116–126. Available at: <a href="http://linkinghub.elsevier.com/retrieve/pii/S0969212615004657" class="uri">http://linkinghub.elsevier.com/retrieve/pii/S0969212615004657</a>.</p>
</div>
<div id="ref-Coucke2016">
<p>88. Coucke, A., Uguzzoni, G., Oteri, F., Cocco, S., Monasson, R., and Weigt, M. (2016). Direct coevolutionary couplings reflect biophysical residue interactions in proteins. J. Chem. Phys. <em>145</em>, 174102. Available at: <a href="http://scitation.aip.org/content/aip/journal/jcp/145/17/10.1063/1.4966156" class="uri">http://scitation.aip.org/content/aip/journal/jcp/145/17/10.1063/1.4966156</a>.</p>
</div>
<div id="ref-Aurell2012">
<p>89. Aurell, E., and Ekeberg, M. (2012). Inverse Ising Inference Using All the Data. Phys. Rev. Lett. <em>108</em>, 090201. Available at: <a href="https://link.aps.org/doi/10.1103/PhysRevLett.108.090201" class="uri">https://link.aps.org/doi/10.1103/PhysRevLett.108.090201</a>.</p>
</div>
<div id="ref-Sillitoe2015">
<p>90. Sillitoe, I., Lewis, T.E., Cuff, A., Das, S., Ashford, P., Dawson, N.L., Furnham, N., Laskowski, R.A., Lee, D., and Lees, J.G. <em>et al.</em> (2015). CATH: comprehensive structural and functional annotations for genome sequences. Nucleic Acids Res. <em>43</em>, D376–D381. Available at: <a href="https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gku947" class="uri">https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gku947</a>.</p>
</div>
</div>
</div>
<!--bookdown:body:end-->
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://contactpredictionthesis.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
